{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "lY-_IOs7nNPp",
    "outputId": "fe697a04-f76b-402e-e128-3c01dfbc6f9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive \n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x\n",
    "from google.colab import drive \n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation,Dropout,Flatten,Reshape\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "38Ee1vFY1Iyn"
   },
   "source": [
    "# DCGANの基本構造の構築（Construction of basic structure of DCGAN）\n",
    "- Gモデルに、入力値を2次元に変換する処理を適用\n",
    "- Gモデルに、アップサンプリングを適用\n",
    "- Gモデルに、畳み込み層を適用\n",
    "\n",
    "- \n",
    "\n",
    "\n",
    "畳み込みの処理については[こちら](https://qiita.com/koshian2/items/885aba771190267a3a1c)\n",
    "\n",
    "アップサンプリングについては[こちら](https://postd.cc/generating-faces/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "feUPU871nQMS",
    "outputId": "3f5dc12d-7292-4c3d-826e-1ce9241ebe7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 5, 5, 128)         204928    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               819456    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,026,305\n",
      "Trainable params: 1,026,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 1024)              103424    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6272)              6428800   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 6272)              25088     \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 64)        204864    \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 1)         1601      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 6,767,873\n",
      "Trainable params: 6,753,281\n",
      "Non-trainable params: 14,592\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_2 (Sequential)    (None, 28, 28, 1)         6767873   \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 1)                 1026305   \n",
      "=================================================================\n",
      "Total params: 7,794,178\n",
      "Trainable params: 6,753,281\n",
      "Non-trainable params: 1,040,897\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_generator():\n",
    "    noise_shape = (z_dim, )\n",
    "    activation = Activation(\"relu\")\n",
    "\n",
    "    model = Sequential()\n",
    "　　　　\n",
    "    model.add(Dense(1024, input_shape=noise_shape))\n",
    "    model.add(activation)\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(7*7*128))\n",
    "    model.add(activation)\n",
    "    model.add(BatchNormalization())\n",
    "　　　　　　\n",
    "　　　　　　# 畳み込み層に7 * 7の画像を与える\n",
    "    model.add(Reshape((7,7,128),input_shape=(128*7*7,)))\n",
    "\n",
    "　　　　　　# UpSamplingをし、画像を14 * 14にする\n",
    "　　　　　　# 間の要素は０で埋める\n",
    "    model.add(UpSampling2D((2,2)))\n",
    "    model.add(Conv2D(64,5,padding=\"same\"))\n",
    "    model.add(activation)\n",
    "\n",
    "    # UpSamplingをし、画像を28 * 28にする\n",
    "    model.add(UpSampling2D((2,2)))\n",
    "\n",
    "    # 畳み込む（ゼロパディングをして、5*5のレイヤーで畳み込み）\n",
    "    model.add(Conv2D(1,5,padding=\"same\"))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def build_discriminator():\n",
    "    activation = LeakyReLU(alpha=0.2)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "　　　　　　# 畳み込む（ゼロパディングをし、5*5のフィルタを2マスごとに動かす）\n",
    "    model.add(Conv2D(64,5, strides=(2,2), padding=\"same\", input_shape=(img_shape)))\n",
    "    model.add(activation)\n",
    "\n",
    "　　　　　　# 畳み込み2回目\n",
    "    model.add(Conv2D(128,5, strides=(2,2)))\n",
    "    model.add(activation)\n",
    "\n",
    "    # 入力の平坦化（多分１次元化ってこと）\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(256))\n",
    "    model.add(activation)\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_combined():\n",
    "    discriminator.trainable = False\n",
    "    model = Sequential([generator, discriminator])\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "#入力画像の定義\n",
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols,channels)\n",
    "\n",
    "#潜在変数の次元数\n",
    "z_dim = 100\n",
    "\n",
    "# 最適化関数\n",
    "optimizer = Adam(lr = 0.0001, beta_1 = 0.5)\n",
    "\n",
    "#discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss = \"binary_crossentropy\", optimizer = optimizer, metrics = [\"accuracy\"])\n",
    "\n",
    "#generator\n",
    "generator = build_generator()\n",
    "\n",
    "combined_model = build_combined()\n",
    "combined_model.compile(loss = \"binary_crossentropy\", optimizer = optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RYYQiZ538M8r"
   },
   "source": [
    "# 学習部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9l3PlupsnYCA"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(epochs, batch_size=128, save_interval=50):\n",
    "        #mnist\n",
    "        (X_train, _),(_, _) = mnist.load_data()\n",
    "\n",
    "        #change -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
    "\n",
    "        #dim2 -> dim3\n",
    "        X_train = np.expand_dims(X_train, axis = 3)\n",
    "\n",
    "        half_batch = int(batch_size / 2)\n",
    "        # num_batches = int(X_train.shape[0] / half_batch)\n",
    "        num_batches = batch_size\n",
    "        print(\"Number of Batches : \", num_batches)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for iteration in range(num_batches):\n",
    "                #discriminator\n",
    "                \n",
    "                #バッチサイズの半分をgeneratorから作成\n",
    "                noise = np.random.normal(0, 1, (half_batch, z_dim))\n",
    "                gen_imgs = generator.predict(noise)\n",
    "\n",
    "                #バッチサイズの半分を教師データからピックアップ\n",
    "                idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "                imgs = X_train[idx]\n",
    "\n",
    "                #training\n",
    "                #本物と偽物は別々に\n",
    "                d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "                d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "\n",
    "                #それぞれの損失関数を平均\n",
    "                d_loss = np.add(d_loss_real, d_loss_fake) / 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #generator\n",
    "                \n",
    "                noise = np.random.normal(0, 1, (batch_size, z_dim))\n",
    "\n",
    "                #正解に近づけるようにする(本物ラベル、1)\n",
    "                valid_y = np.array([1] * batch_size)\n",
    "\n",
    "                #training\n",
    "                g_loss = combined_model.train_on_batch(noise, valid_y)\n",
    "\n",
    "                # 進捗の表示\n",
    "                print (\"epoch:%d, iter:%d,  [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, iteration, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # 指定した間隔で生成画像を保存\n",
    "            if epoch % save_interval == 0:\n",
    "                save_imgs(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yf7W2Y76dlkI"
   },
   "source": [
    "# 生成画像の保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2TGv_G3QnaZj"
   },
   "outputs": [],
   "source": [
    "def save_imgs(epoch):\n",
    "    # 生成画像を敷き詰めるときの行数、列数\n",
    "    r, c = 5, 5\n",
    "\n",
    "    noise = np.random.normal(0, 1, (r * c, z_dim))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    # 生成画像を0-1に再スケール\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"drive/My Drive/GAN_images/dcgan_mnist_%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Az_0tFIXdo7i"
   },
   "source": [
    "# 実行\n",
    "- 学習速度が早い（全結合層をなくし、畳み込み層で抽出した特徴を、特徴マップごとの平均を取り、そのまま分類器にかけるため、調整するパラメータ数が少なくなる。ちょっとよくわからない）\n",
    "- [ここ](https://elix-tech.github.io/ja/2017/02/06/gan.html)に説明あり\n",
    "- ノイズがなく綺麗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZICeK90KncvW",
    "outputId": "00680190-d0eb-4bb7-ae0c-5d9ea04c36c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n",
      "Number of Batches :  100\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, iter:0,  [D loss: 0.722480, acc.: 20.00%] [G loss: 0.699979]\n",
      "epoch:0, iter:1,  [D loss: 0.656297, acc.: 64.00%] [G loss: 0.685666]\n",
      "epoch:0, iter:2,  [D loss: 0.617196, acc.: 64.00%] [G loss: 0.691398]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, iter:3,  [D loss: 0.598628, acc.: 61.00%] [G loss: 0.675348]\n",
      "epoch:0, iter:4,  [D loss: 0.595533, acc.: 53.00%] [G loss: 0.643932]\n",
      "epoch:0, iter:5,  [D loss: 0.605774, acc.: 51.00%] [G loss: 0.594014]\n",
      "epoch:0, iter:6,  [D loss: 0.644328, acc.: 50.00%] [G loss: 0.572222]\n",
      "epoch:0, iter:7,  [D loss: 0.655207, acc.: 50.00%] [G loss: 0.580901]\n",
      "epoch:0, iter:8,  [D loss: 0.648471, acc.: 50.00%] [G loss: 0.629736]\n",
      "epoch:0, iter:9,  [D loss: 0.606433, acc.: 55.00%] [G loss: 0.730020]\n",
      "epoch:0, iter:10,  [D loss: 0.590684, acc.: 63.00%] [G loss: 0.829354]\n",
      "epoch:0, iter:11,  [D loss: 0.535994, acc.: 86.00%] [G loss: 0.935600]\n",
      "epoch:0, iter:12,  [D loss: 0.539911, acc.: 86.00%] [G loss: 0.983959]\n",
      "epoch:0, iter:13,  [D loss: 0.533309, acc.: 83.00%] [G loss: 0.949471]\n",
      "epoch:0, iter:14,  [D loss: 0.604121, acc.: 72.00%] [G loss: 0.864981]\n",
      "epoch:0, iter:15,  [D loss: 0.623863, acc.: 66.00%] [G loss: 0.721079]\n",
      "epoch:0, iter:16,  [D loss: 0.653685, acc.: 59.00%] [G loss: 0.651918]\n",
      "epoch:0, iter:17,  [D loss: 0.641985, acc.: 52.00%] [G loss: 0.653188]\n",
      "epoch:0, iter:18,  [D loss: 0.599612, acc.: 58.00%] [G loss: 0.657438]\n",
      "epoch:0, iter:19,  [D loss: 0.600668, acc.: 58.00%] [G loss: 0.693071]\n",
      "epoch:0, iter:20,  [D loss: 0.582226, acc.: 59.00%] [G loss: 0.732576]\n",
      "epoch:0, iter:21,  [D loss: 0.610467, acc.: 57.00%] [G loss: 0.743811]\n",
      "epoch:0, iter:22,  [D loss: 0.615845, acc.: 60.00%] [G loss: 0.747677]\n",
      "epoch:0, iter:23,  [D loss: 0.658526, acc.: 49.00%] [G loss: 0.744317]\n",
      "epoch:0, iter:24,  [D loss: 0.698432, acc.: 52.00%] [G loss: 0.725762]\n",
      "epoch:0, iter:25,  [D loss: 0.714727, acc.: 46.00%] [G loss: 0.728311]\n",
      "epoch:0, iter:26,  [D loss: 0.688660, acc.: 50.00%] [G loss: 0.750491]\n",
      "epoch:0, iter:27,  [D loss: 0.708007, acc.: 43.00%] [G loss: 0.758071]\n",
      "epoch:0, iter:28,  [D loss: 0.697995, acc.: 47.00%] [G loss: 0.757178]\n",
      "epoch:0, iter:29,  [D loss: 0.695666, acc.: 48.00%] [G loss: 0.748303]\n",
      "epoch:0, iter:30,  [D loss: 0.687552, acc.: 52.00%] [G loss: 0.736129]\n",
      "epoch:0, iter:31,  [D loss: 0.721366, acc.: 38.00%] [G loss: 0.709594]\n",
      "epoch:0, iter:32,  [D loss: 0.710367, acc.: 42.00%] [G loss: 0.675521]\n",
      "epoch:0, iter:33,  [D loss: 0.706663, acc.: 45.00%] [G loss: 0.685409]\n",
      "epoch:0, iter:34,  [D loss: 0.693051, acc.: 46.00%] [G loss: 0.693027]\n",
      "epoch:0, iter:35,  [D loss: 0.687010, acc.: 53.00%] [G loss: 0.702914]\n",
      "epoch:0, iter:36,  [D loss: 0.664666, acc.: 59.00%] [G loss: 0.714475]\n",
      "epoch:0, iter:37,  [D loss: 0.684367, acc.: 53.00%] [G loss: 0.718382]\n",
      "epoch:0, iter:38,  [D loss: 0.668769, acc.: 54.00%] [G loss: 0.719492]\n",
      "epoch:0, iter:39,  [D loss: 0.674471, acc.: 54.00%] [G loss: 0.713064]\n",
      "epoch:0, iter:40,  [D loss: 0.697092, acc.: 48.00%] [G loss: 0.701503]\n",
      "epoch:0, iter:41,  [D loss: 0.691514, acc.: 46.00%] [G loss: 0.694577]\n",
      "epoch:0, iter:42,  [D loss: 0.688526, acc.: 45.00%] [G loss: 0.709146]\n",
      "epoch:0, iter:43,  [D loss: 0.711332, acc.: 42.00%] [G loss: 0.696714]\n",
      "epoch:0, iter:44,  [D loss: 0.698236, acc.: 48.00%] [G loss: 0.722163]\n",
      "epoch:0, iter:45,  [D loss: 0.690529, acc.: 52.00%] [G loss: 0.724954]\n",
      "epoch:0, iter:46,  [D loss: 0.673069, acc.: 54.00%] [G loss: 0.753680]\n",
      "epoch:0, iter:47,  [D loss: 0.652856, acc.: 64.00%] [G loss: 0.744958]\n",
      "epoch:0, iter:48,  [D loss: 0.669969, acc.: 60.00%] [G loss: 0.723390]\n",
      "epoch:0, iter:49,  [D loss: 0.680896, acc.: 52.00%] [G loss: 0.731441]\n",
      "epoch:0, iter:50,  [D loss: 0.697897, acc.: 50.00%] [G loss: 0.682968]\n",
      "epoch:0, iter:51,  [D loss: 0.713417, acc.: 43.00%] [G loss: 0.674266]\n",
      "epoch:0, iter:52,  [D loss: 0.711961, acc.: 43.00%] [G loss: 0.653089]\n",
      "epoch:0, iter:53,  [D loss: 0.699892, acc.: 51.00%] [G loss: 0.651284]\n",
      "epoch:0, iter:54,  [D loss: 0.713429, acc.: 42.00%] [G loss: 0.650624]\n",
      "epoch:0, iter:55,  [D loss: 0.712820, acc.: 48.00%] [G loss: 0.645739]\n",
      "epoch:0, iter:56,  [D loss: 0.691551, acc.: 48.00%] [G loss: 0.648131]\n",
      "epoch:0, iter:57,  [D loss: 0.680704, acc.: 56.00%] [G loss: 0.669001]\n",
      "epoch:0, iter:58,  [D loss: 0.686832, acc.: 50.00%] [G loss: 0.696391]\n",
      "epoch:0, iter:59,  [D loss: 0.690787, acc.: 51.00%] [G loss: 0.696222]\n",
      "epoch:0, iter:60,  [D loss: 0.682141, acc.: 54.00%] [G loss: 0.688541]\n",
      "epoch:0, iter:61,  [D loss: 0.693565, acc.: 47.00%] [G loss: 0.668422]\n",
      "epoch:0, iter:62,  [D loss: 0.692215, acc.: 51.00%] [G loss: 0.669259]\n",
      "epoch:0, iter:63,  [D loss: 0.697649, acc.: 49.00%] [G loss: 0.661206]\n",
      "epoch:0, iter:64,  [D loss: 0.701836, acc.: 44.00%] [G loss: 0.662827]\n",
      "epoch:0, iter:65,  [D loss: 0.705444, acc.: 42.00%] [G loss: 0.662048]\n",
      "epoch:0, iter:66,  [D loss: 0.709062, acc.: 44.00%] [G loss: 0.661444]\n",
      "epoch:0, iter:67,  [D loss: 0.693703, acc.: 46.00%] [G loss: 0.676342]\n",
      "epoch:0, iter:68,  [D loss: 0.723591, acc.: 38.00%] [G loss: 0.678514]\n",
      "epoch:0, iter:69,  [D loss: 0.684596, acc.: 51.00%] [G loss: 0.691133]\n",
      "epoch:0, iter:70,  [D loss: 0.700732, acc.: 43.00%] [G loss: 0.704452]\n",
      "epoch:0, iter:71,  [D loss: 0.689931, acc.: 48.00%] [G loss: 0.715755]\n",
      "epoch:0, iter:72,  [D loss: 0.701941, acc.: 48.00%] [G loss: 0.718109]\n",
      "epoch:0, iter:73,  [D loss: 0.666043, acc.: 58.00%] [G loss: 0.715505]\n",
      "epoch:0, iter:74,  [D loss: 0.684114, acc.: 55.00%] [G loss: 0.691732]\n",
      "epoch:0, iter:75,  [D loss: 0.704518, acc.: 49.00%] [G loss: 0.692134]\n",
      "epoch:0, iter:76,  [D loss: 0.682007, acc.: 49.00%] [G loss: 0.698597]\n",
      "epoch:0, iter:77,  [D loss: 0.685824, acc.: 50.00%] [G loss: 0.695619]\n",
      "epoch:0, iter:78,  [D loss: 0.704008, acc.: 43.00%] [G loss: 0.678269]\n",
      "epoch:0, iter:79,  [D loss: 0.694547, acc.: 50.00%] [G loss: 0.681476]\n",
      "epoch:0, iter:80,  [D loss: 0.694692, acc.: 47.00%] [G loss: 0.687146]\n",
      "epoch:0, iter:81,  [D loss: 0.701403, acc.: 46.00%] [G loss: 0.678957]\n",
      "epoch:0, iter:82,  [D loss: 0.681119, acc.: 54.00%] [G loss: 0.689864]\n",
      "epoch:0, iter:83,  [D loss: 0.680634, acc.: 57.00%] [G loss: 0.693954]\n",
      "epoch:0, iter:84,  [D loss: 0.698550, acc.: 50.00%] [G loss: 0.697750]\n",
      "epoch:0, iter:85,  [D loss: 0.697861, acc.: 53.00%] [G loss: 0.708850]\n",
      "epoch:0, iter:86,  [D loss: 0.692118, acc.: 51.00%] [G loss: 0.707938]\n",
      "epoch:0, iter:87,  [D loss: 0.689211, acc.: 55.00%] [G loss: 0.709288]\n",
      "epoch:0, iter:88,  [D loss: 0.691438, acc.: 47.00%] [G loss: 0.713487]\n",
      "epoch:0, iter:89,  [D loss: 0.686864, acc.: 55.00%] [G loss: 0.699727]\n",
      "epoch:0, iter:90,  [D loss: 0.679468, acc.: 49.00%] [G loss: 0.703592]\n",
      "epoch:0, iter:91,  [D loss: 0.687169, acc.: 48.00%] [G loss: 0.719623]\n",
      "epoch:0, iter:92,  [D loss: 0.693369, acc.: 50.00%] [G loss: 0.708994]\n",
      "epoch:0, iter:93,  [D loss: 0.697957, acc.: 46.00%] [G loss: 0.714514]\n",
      "epoch:0, iter:94,  [D loss: 0.696862, acc.: 49.00%] [G loss: 0.720907]\n",
      "epoch:0, iter:95,  [D loss: 0.689892, acc.: 56.00%] [G loss: 0.712505]\n",
      "epoch:0, iter:96,  [D loss: 0.703332, acc.: 44.00%] [G loss: 0.717675]\n",
      "epoch:0, iter:97,  [D loss: 0.695760, acc.: 47.00%] [G loss: 0.728021]\n",
      "epoch:0, iter:98,  [D loss: 0.696281, acc.: 48.00%] [G loss: 0.717161]\n",
      "epoch:0, iter:99,  [D loss: 0.674968, acc.: 55.00%] [G loss: 0.723161]\n",
      "epoch:1, iter:0,  [D loss: 0.689008, acc.: 57.00%] [G loss: 0.719509]\n",
      "epoch:1, iter:1,  [D loss: 0.683302, acc.: 57.00%] [G loss: 0.721742]\n",
      "epoch:1, iter:2,  [D loss: 0.681817, acc.: 53.00%] [G loss: 0.728725]\n",
      "epoch:1, iter:3,  [D loss: 0.704556, acc.: 43.00%] [G loss: 0.721672]\n",
      "epoch:1, iter:4,  [D loss: 0.684059, acc.: 57.00%] [G loss: 0.730543]\n",
      "epoch:1, iter:5,  [D loss: 0.686234, acc.: 54.00%] [G loss: 0.734194]\n",
      "epoch:1, iter:6,  [D loss: 0.690681, acc.: 49.00%] [G loss: 0.723525]\n",
      "epoch:1, iter:7,  [D loss: 0.699302, acc.: 49.00%] [G loss: 0.723262]\n",
      "epoch:1, iter:8,  [D loss: 0.704567, acc.: 41.00%] [G loss: 0.704660]\n",
      "epoch:1, iter:9,  [D loss: 0.691586, acc.: 47.00%] [G loss: 0.708433]\n",
      "epoch:1, iter:10,  [D loss: 0.696733, acc.: 49.00%] [G loss: 0.712259]\n",
      "epoch:1, iter:11,  [D loss: 0.690074, acc.: 44.00%] [G loss: 0.707959]\n",
      "epoch:1, iter:12,  [D loss: 0.708459, acc.: 42.00%] [G loss: 0.699796]\n",
      "epoch:1, iter:13,  [D loss: 0.710351, acc.: 45.00%] [G loss: 0.688206]\n",
      "epoch:1, iter:14,  [D loss: 0.701172, acc.: 48.00%] [G loss: 0.688015]\n",
      "epoch:1, iter:15,  [D loss: 0.688148, acc.: 55.00%] [G loss: 0.699305]\n",
      "epoch:1, iter:16,  [D loss: 0.699766, acc.: 46.00%] [G loss: 0.693881]\n",
      "epoch:1, iter:17,  [D loss: 0.683678, acc.: 51.00%] [G loss: 0.698429]\n",
      "epoch:1, iter:18,  [D loss: 0.696426, acc.: 48.00%] [G loss: 0.703301]\n",
      "epoch:1, iter:19,  [D loss: 0.699331, acc.: 47.00%] [G loss: 0.695212]\n",
      "epoch:1, iter:20,  [D loss: 0.695715, acc.: 45.00%] [G loss: 0.701455]\n",
      "epoch:1, iter:21,  [D loss: 0.706037, acc.: 43.00%] [G loss: 0.691627]\n",
      "epoch:1, iter:22,  [D loss: 0.692110, acc.: 51.00%] [G loss: 0.690257]\n",
      "epoch:1, iter:23,  [D loss: 0.683720, acc.: 51.00%] [G loss: 0.686359]\n",
      "epoch:1, iter:24,  [D loss: 0.693550, acc.: 48.00%] [G loss: 0.700589]\n",
      "epoch:1, iter:25,  [D loss: 0.696594, acc.: 47.00%] [G loss: 0.689534]\n",
      "epoch:1, iter:26,  [D loss: 0.695753, acc.: 47.00%] [G loss: 0.692384]\n",
      "epoch:1, iter:27,  [D loss: 0.703469, acc.: 44.00%] [G loss: 0.692577]\n",
      "epoch:1, iter:28,  [D loss: 0.694079, acc.: 52.00%] [G loss: 0.693816]\n",
      "epoch:1, iter:29,  [D loss: 0.696405, acc.: 42.00%] [G loss: 0.682508]\n",
      "epoch:1, iter:30,  [D loss: 0.678992, acc.: 56.00%] [G loss: 0.686739]\n",
      "epoch:1, iter:31,  [D loss: 0.692581, acc.: 54.00%] [G loss: 0.695285]\n",
      "epoch:1, iter:32,  [D loss: 0.691217, acc.: 48.00%] [G loss: 0.690127]\n",
      "epoch:1, iter:33,  [D loss: 0.682984, acc.: 49.00%] [G loss: 0.689755]\n",
      "epoch:1, iter:34,  [D loss: 0.688894, acc.: 55.00%] [G loss: 0.701517]\n",
      "epoch:1, iter:35,  [D loss: 0.694226, acc.: 43.00%] [G loss: 0.696882]\n",
      "epoch:1, iter:36,  [D loss: 0.687108, acc.: 52.00%] [G loss: 0.700363]\n",
      "epoch:1, iter:37,  [D loss: 0.701688, acc.: 56.00%] [G loss: 0.703690]\n",
      "epoch:1, iter:38,  [D loss: 0.689032, acc.: 55.00%] [G loss: 0.704412]\n",
      "epoch:1, iter:39,  [D loss: 0.689857, acc.: 50.00%] [G loss: 0.698778]\n",
      "epoch:1, iter:40,  [D loss: 0.697424, acc.: 47.00%] [G loss: 0.703846]\n",
      "epoch:1, iter:41,  [D loss: 0.687677, acc.: 55.00%] [G loss: 0.698320]\n",
      "epoch:1, iter:42,  [D loss: 0.704539, acc.: 39.00%] [G loss: 0.707608]\n",
      "epoch:1, iter:43,  [D loss: 0.687274, acc.: 53.00%] [G loss: 0.701405]\n",
      "epoch:1, iter:44,  [D loss: 0.690652, acc.: 50.00%] [G loss: 0.691395]\n",
      "epoch:1, iter:45,  [D loss: 0.685198, acc.: 53.00%] [G loss: 0.705076]\n",
      "epoch:1, iter:46,  [D loss: 0.694003, acc.: 55.00%] [G loss: 0.705514]\n",
      "epoch:1, iter:47,  [D loss: 0.694152, acc.: 51.00%] [G loss: 0.700500]\n",
      "epoch:1, iter:48,  [D loss: 0.685663, acc.: 55.00%] [G loss: 0.711254]\n",
      "epoch:1, iter:49,  [D loss: 0.691424, acc.: 50.00%] [G loss: 0.701463]\n",
      "epoch:1, iter:50,  [D loss: 0.682657, acc.: 59.00%] [G loss: 0.714180]\n",
      "epoch:1, iter:51,  [D loss: 0.682111, acc.: 58.00%] [G loss: 0.706716]\n",
      "epoch:1, iter:52,  [D loss: 0.680770, acc.: 62.00%] [G loss: 0.707992]\n",
      "epoch:1, iter:53,  [D loss: 0.688381, acc.: 53.00%] [G loss: 0.703913]\n",
      "epoch:1, iter:54,  [D loss: 0.686221, acc.: 53.00%] [G loss: 0.714384]\n",
      "epoch:1, iter:55,  [D loss: 0.689475, acc.: 57.00%] [G loss: 0.720159]\n",
      "epoch:1, iter:56,  [D loss: 0.685061, acc.: 53.00%] [G loss: 0.713745]\n",
      "epoch:1, iter:57,  [D loss: 0.686030, acc.: 50.00%] [G loss: 0.719027]\n",
      "epoch:1, iter:58,  [D loss: 0.681157, acc.: 55.00%] [G loss: 0.711950]\n",
      "epoch:1, iter:59,  [D loss: 0.670238, acc.: 62.00%] [G loss: 0.724146]\n",
      "epoch:1, iter:60,  [D loss: 0.686959, acc.: 54.00%] [G loss: 0.724495]\n",
      "epoch:1, iter:61,  [D loss: 0.686256, acc.: 54.00%] [G loss: 0.718797]\n",
      "epoch:1, iter:62,  [D loss: 0.693266, acc.: 48.00%] [G loss: 0.707772]\n",
      "epoch:1, iter:63,  [D loss: 0.686927, acc.: 56.00%] [G loss: 0.701429]\n",
      "epoch:1, iter:64,  [D loss: 0.680593, acc.: 57.00%] [G loss: 0.712040]\n",
      "epoch:1, iter:65,  [D loss: 0.690949, acc.: 50.00%] [G loss: 0.716032]\n",
      "epoch:1, iter:66,  [D loss: 0.693751, acc.: 45.00%] [G loss: 0.706801]\n",
      "epoch:1, iter:67,  [D loss: 0.692086, acc.: 44.00%] [G loss: 0.708336]\n",
      "epoch:1, iter:68,  [D loss: 0.684894, acc.: 59.00%] [G loss: 0.711464]\n",
      "epoch:1, iter:69,  [D loss: 0.678920, acc.: 61.00%] [G loss: 0.693926]\n",
      "epoch:1, iter:70,  [D loss: 0.687424, acc.: 56.00%] [G loss: 0.706502]\n",
      "epoch:1, iter:71,  [D loss: 0.682515, acc.: 57.00%] [G loss: 0.712518]\n",
      "epoch:1, iter:72,  [D loss: 0.690456, acc.: 54.00%] [G loss: 0.705372]\n",
      "epoch:1, iter:73,  [D loss: 0.693096, acc.: 53.00%] [G loss: 0.690925]\n",
      "epoch:1, iter:74,  [D loss: 0.681055, acc.: 56.00%] [G loss: 0.701296]\n",
      "epoch:1, iter:75,  [D loss: 0.693541, acc.: 47.00%] [G loss: 0.693220]\n",
      "epoch:1, iter:76,  [D loss: 0.686574, acc.: 54.00%] [G loss: 0.706479]\n",
      "epoch:1, iter:77,  [D loss: 0.691068, acc.: 49.00%] [G loss: 0.702331]\n",
      "epoch:1, iter:78,  [D loss: 0.682735, acc.: 57.00%] [G loss: 0.705800]\n",
      "epoch:1, iter:79,  [D loss: 0.686996, acc.: 60.00%] [G loss: 0.696631]\n",
      "epoch:1, iter:80,  [D loss: 0.687934, acc.: 59.00%] [G loss: 0.700539]\n",
      "epoch:1, iter:81,  [D loss: 0.690804, acc.: 52.00%] [G loss: 0.702285]\n",
      "epoch:1, iter:82,  [D loss: 0.680919, acc.: 59.00%] [G loss: 0.706068]\n",
      "epoch:1, iter:83,  [D loss: 0.694934, acc.: 45.00%] [G loss: 0.703540]\n",
      "epoch:1, iter:84,  [D loss: 0.689762, acc.: 58.00%] [G loss: 0.707203]\n",
      "epoch:1, iter:85,  [D loss: 0.696980, acc.: 51.00%] [G loss: 0.713398]\n",
      "epoch:1, iter:86,  [D loss: 0.692415, acc.: 50.00%] [G loss: 0.705713]\n",
      "epoch:1, iter:87,  [D loss: 0.695970, acc.: 49.00%] [G loss: 0.705331]\n",
      "epoch:1, iter:88,  [D loss: 0.681100, acc.: 59.00%] [G loss: 0.710620]\n",
      "epoch:1, iter:89,  [D loss: 0.687327, acc.: 55.00%] [G loss: 0.704017]\n",
      "epoch:1, iter:90,  [D loss: 0.691824, acc.: 46.00%] [G loss: 0.705309]\n",
      "epoch:1, iter:91,  [D loss: 0.683339, acc.: 63.00%] [G loss: 0.706033]\n",
      "epoch:1, iter:92,  [D loss: 0.681563, acc.: 59.00%] [G loss: 0.705049]\n",
      "epoch:1, iter:93,  [D loss: 0.683414, acc.: 57.00%] [G loss: 0.703223]\n",
      "epoch:1, iter:94,  [D loss: 0.682836, acc.: 60.00%] [G loss: 0.700169]\n",
      "epoch:1, iter:95,  [D loss: 0.689622, acc.: 46.00%] [G loss: 0.709225]\n",
      "epoch:1, iter:96,  [D loss: 0.691041, acc.: 54.00%] [G loss: 0.706023]\n",
      "epoch:1, iter:97,  [D loss: 0.678936, acc.: 62.00%] [G loss: 0.721837]\n",
      "epoch:1, iter:98,  [D loss: 0.683041, acc.: 56.00%] [G loss: 0.713932]\n",
      "epoch:1, iter:99,  [D loss: 0.692287, acc.: 54.00%] [G loss: 0.709753]\n",
      "epoch:2, iter:0,  [D loss: 0.684751, acc.: 58.00%] [G loss: 0.718522]\n",
      "epoch:2, iter:1,  [D loss: 0.691964, acc.: 53.00%] [G loss: 0.712050]\n",
      "epoch:2, iter:2,  [D loss: 0.687295, acc.: 55.00%] [G loss: 0.712531]\n",
      "epoch:2, iter:3,  [D loss: 0.687476, acc.: 56.00%] [G loss: 0.711648]\n",
      "epoch:2, iter:4,  [D loss: 0.677160, acc.: 60.00%] [G loss: 0.713351]\n",
      "epoch:2, iter:5,  [D loss: 0.683436, acc.: 58.00%] [G loss: 0.712679]\n",
      "epoch:2, iter:6,  [D loss: 0.688840, acc.: 51.00%] [G loss: 0.709956]\n",
      "epoch:2, iter:7,  [D loss: 0.700720, acc.: 53.00%] [G loss: 0.713570]\n",
      "epoch:2, iter:8,  [D loss: 0.681827, acc.: 56.00%] [G loss: 0.719028]\n",
      "epoch:2, iter:9,  [D loss: 0.701938, acc.: 47.00%] [G loss: 0.715937]\n",
      "epoch:2, iter:10,  [D loss: 0.682142, acc.: 59.00%] [G loss: 0.715061]\n",
      "epoch:2, iter:11,  [D loss: 0.692812, acc.: 51.00%] [G loss: 0.714504]\n",
      "epoch:2, iter:12,  [D loss: 0.687074, acc.: 55.00%] [G loss: 0.716576]\n",
      "epoch:2, iter:13,  [D loss: 0.686026, acc.: 59.00%] [G loss: 0.719830]\n",
      "epoch:2, iter:14,  [D loss: 0.686627, acc.: 57.00%] [G loss: 0.722714]\n",
      "epoch:2, iter:15,  [D loss: 0.682792, acc.: 60.00%] [G loss: 0.712850]\n",
      "epoch:2, iter:16,  [D loss: 0.684326, acc.: 62.00%] [G loss: 0.712412]\n",
      "epoch:2, iter:17,  [D loss: 0.686833, acc.: 56.00%] [G loss: 0.710628]\n",
      "epoch:2, iter:18,  [D loss: 0.684519, acc.: 61.00%] [G loss: 0.716469]\n",
      "epoch:2, iter:19,  [D loss: 0.695791, acc.: 44.00%] [G loss: 0.722907]\n",
      "epoch:2, iter:20,  [D loss: 0.683618, acc.: 64.00%] [G loss: 0.727782]\n",
      "epoch:2, iter:21,  [D loss: 0.682076, acc.: 58.00%] [G loss: 0.736084]\n",
      "epoch:2, iter:22,  [D loss: 0.682615, acc.: 55.00%] [G loss: 0.732558]\n",
      "epoch:2, iter:23,  [D loss: 0.686347, acc.: 55.00%] [G loss: 0.728907]\n",
      "epoch:2, iter:24,  [D loss: 0.689912, acc.: 52.00%] [G loss: 0.730297]\n",
      "epoch:2, iter:25,  [D loss: 0.686908, acc.: 57.00%] [G loss: 0.728227]\n",
      "epoch:2, iter:26,  [D loss: 0.683322, acc.: 56.00%] [G loss: 0.727950]\n",
      "epoch:2, iter:27,  [D loss: 0.680093, acc.: 56.00%] [G loss: 0.734405]\n",
      "epoch:2, iter:28,  [D loss: 0.694356, acc.: 48.00%] [G loss: 0.729084]\n",
      "epoch:2, iter:29,  [D loss: 0.685811, acc.: 50.00%] [G loss: 0.721103]\n",
      "epoch:2, iter:30,  [D loss: 0.677480, acc.: 59.00%] [G loss: 0.716982]\n",
      "epoch:2, iter:31,  [D loss: 0.672048, acc.: 61.00%] [G loss: 0.725351]\n",
      "epoch:2, iter:32,  [D loss: 0.689069, acc.: 50.00%] [G loss: 0.728743]\n",
      "epoch:2, iter:33,  [D loss: 0.684925, acc.: 59.00%] [G loss: 0.723225]\n",
      "epoch:2, iter:34,  [D loss: 0.693439, acc.: 51.00%] [G loss: 0.716807]\n",
      "epoch:2, iter:35,  [D loss: 0.687159, acc.: 55.00%] [G loss: 0.718207]\n",
      "epoch:2, iter:36,  [D loss: 0.680974, acc.: 58.00%] [G loss: 0.719707]\n",
      "epoch:2, iter:37,  [D loss: 0.702974, acc.: 41.00%] [G loss: 0.723508]\n",
      "epoch:2, iter:38,  [D loss: 0.684674, acc.: 59.00%] [G loss: 0.718925]\n",
      "epoch:2, iter:39,  [D loss: 0.694390, acc.: 51.00%] [G loss: 0.715872]\n",
      "epoch:2, iter:40,  [D loss: 0.677616, acc.: 64.00%] [G loss: 0.720292]\n",
      "epoch:2, iter:41,  [D loss: 0.707843, acc.: 38.00%] [G loss: 0.724836]\n",
      "epoch:2, iter:42,  [D loss: 0.678647, acc.: 57.00%] [G loss: 0.725804]\n",
      "epoch:2, iter:43,  [D loss: 0.680121, acc.: 60.00%] [G loss: 0.727830]\n",
      "epoch:2, iter:44,  [D loss: 0.687559, acc.: 51.00%] [G loss: 0.714384]\n",
      "epoch:2, iter:45,  [D loss: 0.695369, acc.: 50.00%] [G loss: 0.708725]\n",
      "epoch:2, iter:46,  [D loss: 0.684175, acc.: 56.00%] [G loss: 0.710238]\n",
      "epoch:2, iter:47,  [D loss: 0.685343, acc.: 53.00%] [G loss: 0.703525]\n",
      "epoch:2, iter:48,  [D loss: 0.693550, acc.: 52.00%] [G loss: 0.720909]\n",
      "epoch:2, iter:49,  [D loss: 0.692408, acc.: 49.00%] [G loss: 0.713875]\n",
      "epoch:2, iter:50,  [D loss: 0.685961, acc.: 54.00%] [G loss: 0.714557]\n",
      "epoch:2, iter:51,  [D loss: 0.686721, acc.: 52.00%] [G loss: 0.714974]\n",
      "epoch:2, iter:52,  [D loss: 0.682423, acc.: 56.00%] [G loss: 0.724722]\n",
      "epoch:2, iter:53,  [D loss: 0.682886, acc.: 52.00%] [G loss: 0.719803]\n",
      "epoch:2, iter:54,  [D loss: 0.678418, acc.: 55.00%] [G loss: 0.727674]\n",
      "epoch:2, iter:55,  [D loss: 0.686802, acc.: 58.00%] [G loss: 0.725700]\n",
      "epoch:2, iter:56,  [D loss: 0.684738, acc.: 58.00%] [G loss: 0.721254]\n",
      "epoch:2, iter:57,  [D loss: 0.684939, acc.: 52.00%] [G loss: 0.727916]\n",
      "epoch:2, iter:58,  [D loss: 0.683612, acc.: 59.00%] [G loss: 0.728136]\n",
      "epoch:2, iter:59,  [D loss: 0.671908, acc.: 67.00%] [G loss: 0.723713]\n",
      "epoch:2, iter:60,  [D loss: 0.676593, acc.: 55.00%] [G loss: 0.731370]\n",
      "epoch:2, iter:61,  [D loss: 0.678985, acc.: 60.00%] [G loss: 0.721700]\n",
      "epoch:2, iter:62,  [D loss: 0.677055, acc.: 58.00%] [G loss: 0.723154]\n",
      "epoch:2, iter:63,  [D loss: 0.689148, acc.: 53.00%] [G loss: 0.723288]\n",
      "epoch:2, iter:64,  [D loss: 0.682055, acc.: 52.00%] [G loss: 0.718338]\n",
      "epoch:2, iter:65,  [D loss: 0.694981, acc.: 47.00%] [G loss: 0.717911]\n",
      "epoch:2, iter:66,  [D loss: 0.693100, acc.: 46.00%] [G loss: 0.717833]\n",
      "epoch:2, iter:67,  [D loss: 0.684493, acc.: 54.00%] [G loss: 0.714890]\n",
      "epoch:2, iter:68,  [D loss: 0.683415, acc.: 57.00%] [G loss: 0.705771]\n",
      "epoch:2, iter:69,  [D loss: 0.691637, acc.: 47.00%] [G loss: 0.710278]\n",
      "epoch:2, iter:70,  [D loss: 0.683543, acc.: 58.00%] [G loss: 0.715611]\n",
      "epoch:2, iter:71,  [D loss: 0.688382, acc.: 55.00%] [G loss: 0.709375]\n",
      "epoch:2, iter:72,  [D loss: 0.683493, acc.: 49.00%] [G loss: 0.709974]\n",
      "epoch:2, iter:73,  [D loss: 0.690444, acc.: 51.00%] [G loss: 0.724018]\n",
      "epoch:2, iter:74,  [D loss: 0.679698, acc.: 62.00%] [G loss: 0.708254]\n",
      "epoch:2, iter:75,  [D loss: 0.684174, acc.: 54.00%] [G loss: 0.726014]\n",
      "epoch:2, iter:76,  [D loss: 0.687259, acc.: 59.00%] [G loss: 0.729431]\n",
      "epoch:2, iter:77,  [D loss: 0.706178, acc.: 45.00%] [G loss: 0.719274]\n",
      "epoch:2, iter:78,  [D loss: 0.683078, acc.: 62.00%] [G loss: 0.726786]\n",
      "epoch:2, iter:79,  [D loss: 0.692216, acc.: 52.00%] [G loss: 0.727160]\n",
      "epoch:2, iter:80,  [D loss: 0.681861, acc.: 57.00%] [G loss: 0.713481]\n",
      "epoch:2, iter:81,  [D loss: 0.685366, acc.: 56.00%] [G loss: 0.736761]\n",
      "epoch:2, iter:82,  [D loss: 0.687960, acc.: 55.00%] [G loss: 0.722174]\n",
      "epoch:2, iter:83,  [D loss: 0.692211, acc.: 53.00%] [G loss: 0.727478]\n",
      "epoch:2, iter:84,  [D loss: 0.683816, acc.: 62.00%] [G loss: 0.734388]\n",
      "epoch:2, iter:85,  [D loss: 0.690130, acc.: 52.00%] [G loss: 0.739430]\n",
      "epoch:2, iter:86,  [D loss: 0.682557, acc.: 56.00%] [G loss: 0.735554]\n",
      "epoch:2, iter:87,  [D loss: 0.694737, acc.: 50.00%] [G loss: 0.738370]\n",
      "epoch:2, iter:88,  [D loss: 0.695157, acc.: 50.00%] [G loss: 0.737070]\n",
      "epoch:2, iter:89,  [D loss: 0.684466, acc.: 54.00%] [G loss: 0.729787]\n",
      "epoch:2, iter:90,  [D loss: 0.692892, acc.: 53.00%] [G loss: 0.723871]\n",
      "epoch:2, iter:91,  [D loss: 0.692924, acc.: 50.00%] [G loss: 0.731885]\n",
      "epoch:2, iter:92,  [D loss: 0.682187, acc.: 54.00%] [G loss: 0.732915]\n",
      "epoch:2, iter:93,  [D loss: 0.688959, acc.: 53.00%] [G loss: 0.728575]\n",
      "epoch:2, iter:94,  [D loss: 0.694916, acc.: 46.00%] [G loss: 0.733382]\n",
      "epoch:2, iter:95,  [D loss: 0.699975, acc.: 47.00%] [G loss: 0.725151]\n",
      "epoch:2, iter:96,  [D loss: 0.672987, acc.: 64.00%] [G loss: 0.726066]\n",
      "epoch:2, iter:97,  [D loss: 0.692763, acc.: 48.00%] [G loss: 0.722834]\n",
      "epoch:2, iter:98,  [D loss: 0.690304, acc.: 53.00%] [G loss: 0.725555]\n",
      "epoch:2, iter:99,  [D loss: 0.681760, acc.: 62.00%] [G loss: 0.731838]\n",
      "epoch:3, iter:0,  [D loss: 0.683964, acc.: 58.00%] [G loss: 0.720885]\n",
      "epoch:3, iter:1,  [D loss: 0.686487, acc.: 59.00%] [G loss: 0.732253]\n",
      "epoch:3, iter:2,  [D loss: 0.680578, acc.: 70.00%] [G loss: 0.731133]\n",
      "epoch:3, iter:3,  [D loss: 0.679364, acc.: 64.00%] [G loss: 0.727036]\n",
      "epoch:3, iter:4,  [D loss: 0.689449, acc.: 52.00%] [G loss: 0.737320]\n",
      "epoch:3, iter:5,  [D loss: 0.674132, acc.: 66.00%] [G loss: 0.724204]\n",
      "epoch:3, iter:6,  [D loss: 0.677741, acc.: 64.00%] [G loss: 0.743504]\n",
      "epoch:3, iter:7,  [D loss: 0.675860, acc.: 66.00%] [G loss: 0.716881]\n",
      "epoch:3, iter:8,  [D loss: 0.684273, acc.: 57.00%] [G loss: 0.721498]\n",
      "epoch:3, iter:9,  [D loss: 0.681664, acc.: 58.00%] [G loss: 0.727224]\n",
      "epoch:3, iter:10,  [D loss: 0.685740, acc.: 57.00%] [G loss: 0.725651]\n",
      "epoch:3, iter:11,  [D loss: 0.676149, acc.: 59.00%] [G loss: 0.718119]\n",
      "epoch:3, iter:12,  [D loss: 0.684793, acc.: 54.00%] [G loss: 0.715056]\n",
      "epoch:3, iter:13,  [D loss: 0.689207, acc.: 50.00%] [G loss: 0.715210]\n",
      "epoch:3, iter:14,  [D loss: 0.686108, acc.: 57.00%] [G loss: 0.724187]\n",
      "epoch:3, iter:15,  [D loss: 0.694636, acc.: 50.00%] [G loss: 0.726440]\n",
      "epoch:3, iter:16,  [D loss: 0.693672, acc.: 55.00%] [G loss: 0.723577]\n",
      "epoch:3, iter:17,  [D loss: 0.684314, acc.: 56.00%] [G loss: 0.724714]\n",
      "epoch:3, iter:18,  [D loss: 0.684873, acc.: 63.00%] [G loss: 0.727359]\n",
      "epoch:3, iter:19,  [D loss: 0.682223, acc.: 58.00%] [G loss: 0.733455]\n",
      "epoch:3, iter:20,  [D loss: 0.678041, acc.: 66.00%] [G loss: 0.744340]\n",
      "epoch:3, iter:21,  [D loss: 0.676740, acc.: 62.00%] [G loss: 0.741908]\n",
      "epoch:3, iter:22,  [D loss: 0.669157, acc.: 64.00%] [G loss: 0.737810]\n",
      "epoch:3, iter:23,  [D loss: 0.674068, acc.: 65.00%] [G loss: 0.749386]\n",
      "epoch:3, iter:24,  [D loss: 0.678460, acc.: 63.00%] [G loss: 0.746618]\n",
      "epoch:3, iter:25,  [D loss: 0.666726, acc.: 64.00%] [G loss: 0.744588]\n",
      "epoch:3, iter:26,  [D loss: 0.679626, acc.: 58.00%] [G loss: 0.736743]\n",
      "epoch:3, iter:27,  [D loss: 0.684545, acc.: 59.00%] [G loss: 0.720301]\n",
      "epoch:3, iter:28,  [D loss: 0.698693, acc.: 45.00%] [G loss: 0.715606]\n",
      "epoch:3, iter:29,  [D loss: 0.698442, acc.: 44.00%] [G loss: 0.721875]\n",
      "epoch:3, iter:30,  [D loss: 0.695333, acc.: 51.00%] [G loss: 0.705915]\n",
      "epoch:3, iter:31,  [D loss: 0.686293, acc.: 53.00%] [G loss: 0.705703]\n",
      "epoch:3, iter:32,  [D loss: 0.682746, acc.: 56.00%] [G loss: 0.712071]\n",
      "epoch:3, iter:33,  [D loss: 0.684005, acc.: 62.00%] [G loss: 0.714359]\n",
      "epoch:3, iter:34,  [D loss: 0.678748, acc.: 56.00%] [G loss: 0.725621]\n",
      "epoch:3, iter:35,  [D loss: 0.675674, acc.: 62.00%] [G loss: 0.728776]\n",
      "epoch:3, iter:36,  [D loss: 0.670147, acc.: 67.00%] [G loss: 0.729190]\n",
      "epoch:3, iter:37,  [D loss: 0.674632, acc.: 61.00%] [G loss: 0.736700]\n",
      "epoch:3, iter:38,  [D loss: 0.680235, acc.: 62.00%] [G loss: 0.734683]\n",
      "epoch:3, iter:39,  [D loss: 0.690738, acc.: 53.00%] [G loss: 0.732614]\n",
      "epoch:3, iter:40,  [D loss: 0.683504, acc.: 59.00%] [G loss: 0.731288]\n",
      "epoch:3, iter:41,  [D loss: 0.691489, acc.: 54.00%] [G loss: 0.732633]\n",
      "epoch:3, iter:42,  [D loss: 0.692276, acc.: 47.00%] [G loss: 0.713995]\n",
      "epoch:3, iter:43,  [D loss: 0.691315, acc.: 56.00%] [G loss: 0.713792]\n",
      "epoch:3, iter:44,  [D loss: 0.682636, acc.: 56.00%] [G loss: 0.716994]\n",
      "epoch:3, iter:45,  [D loss: 0.688678, acc.: 57.00%] [G loss: 0.724429]\n",
      "epoch:3, iter:46,  [D loss: 0.682640, acc.: 60.00%] [G loss: 0.735630]\n",
      "epoch:3, iter:47,  [D loss: 0.690127, acc.: 51.00%] [G loss: 0.732720]\n",
      "epoch:3, iter:48,  [D loss: 0.682363, acc.: 51.00%] [G loss: 0.732861]\n",
      "epoch:3, iter:49,  [D loss: 0.686504, acc.: 58.00%] [G loss: 0.745053]\n",
      "epoch:3, iter:50,  [D loss: 0.678306, acc.: 65.00%] [G loss: 0.744838]\n",
      "epoch:3, iter:51,  [D loss: 0.672216, acc.: 66.00%] [G loss: 0.748908]\n",
      "epoch:3, iter:52,  [D loss: 0.670449, acc.: 65.00%] [G loss: 0.757415]\n",
      "epoch:3, iter:53,  [D loss: 0.681804, acc.: 55.00%] [G loss: 0.749226]\n",
      "epoch:3, iter:54,  [D loss: 0.664567, acc.: 69.00%] [G loss: 0.753889]\n",
      "epoch:3, iter:55,  [D loss: 0.673029, acc.: 59.00%] [G loss: 0.756450]\n",
      "epoch:3, iter:56,  [D loss: 0.673143, acc.: 69.00%] [G loss: 0.738760]\n",
      "epoch:3, iter:57,  [D loss: 0.695405, acc.: 53.00%] [G loss: 0.732713]\n",
      "epoch:3, iter:58,  [D loss: 0.686116, acc.: 58.00%] [G loss: 0.721480]\n",
      "epoch:3, iter:59,  [D loss: 0.698070, acc.: 42.00%] [G loss: 0.709199]\n",
      "epoch:3, iter:60,  [D loss: 0.693970, acc.: 52.00%] [G loss: 0.714164]\n",
      "epoch:3, iter:61,  [D loss: 0.695124, acc.: 51.00%] [G loss: 0.715039]\n",
      "epoch:3, iter:62,  [D loss: 0.686751, acc.: 51.00%] [G loss: 0.727131]\n",
      "epoch:3, iter:63,  [D loss: 0.692494, acc.: 55.00%] [G loss: 0.730755]\n",
      "epoch:3, iter:64,  [D loss: 0.684380, acc.: 55.00%] [G loss: 0.733754]\n",
      "epoch:3, iter:65,  [D loss: 0.683734, acc.: 63.00%] [G loss: 0.736716]\n",
      "epoch:3, iter:66,  [D loss: 0.676507, acc.: 58.00%] [G loss: 0.737660]\n",
      "epoch:3, iter:67,  [D loss: 0.682199, acc.: 54.00%] [G loss: 0.726435]\n",
      "epoch:3, iter:68,  [D loss: 0.672511, acc.: 63.00%] [G loss: 0.735936]\n",
      "epoch:3, iter:69,  [D loss: 0.670985, acc.: 66.00%] [G loss: 0.726233]\n",
      "epoch:3, iter:70,  [D loss: 0.689636, acc.: 55.00%] [G loss: 0.728608]\n",
      "epoch:3, iter:71,  [D loss: 0.685043, acc.: 61.00%] [G loss: 0.725831]\n",
      "epoch:3, iter:72,  [D loss: 0.693406, acc.: 53.00%] [G loss: 0.725613]\n",
      "epoch:3, iter:73,  [D loss: 0.680619, acc.: 59.00%] [G loss: 0.719387]\n",
      "epoch:3, iter:74,  [D loss: 0.690386, acc.: 48.00%] [G loss: 0.726458]\n",
      "epoch:3, iter:75,  [D loss: 0.682334, acc.: 62.00%] [G loss: 0.729740]\n",
      "epoch:3, iter:76,  [D loss: 0.686633, acc.: 56.00%] [G loss: 0.734019]\n",
      "epoch:3, iter:77,  [D loss: 0.680166, acc.: 63.00%] [G loss: 0.723949]\n",
      "epoch:3, iter:78,  [D loss: 0.681566, acc.: 61.00%] [G loss: 0.738886]\n",
      "epoch:3, iter:79,  [D loss: 0.684834, acc.: 54.00%] [G loss: 0.736725]\n",
      "epoch:3, iter:80,  [D loss: 0.690284, acc.: 46.00%] [G loss: 0.734941]\n",
      "epoch:3, iter:81,  [D loss: 0.675250, acc.: 57.00%] [G loss: 0.737101]\n",
      "epoch:3, iter:82,  [D loss: 0.685909, acc.: 55.00%] [G loss: 0.737528]\n",
      "epoch:3, iter:83,  [D loss: 0.680767, acc.: 60.00%] [G loss: 0.731226]\n",
      "epoch:3, iter:84,  [D loss: 0.665887, acc.: 63.00%] [G loss: 0.737652]\n",
      "epoch:3, iter:85,  [D loss: 0.673021, acc.: 67.00%] [G loss: 0.747706]\n",
      "epoch:3, iter:86,  [D loss: 0.683744, acc.: 58.00%] [G loss: 0.743139]\n",
      "epoch:3, iter:87,  [D loss: 0.681027, acc.: 60.00%] [G loss: 0.739690]\n",
      "epoch:3, iter:88,  [D loss: 0.677705, acc.: 60.00%] [G loss: 0.740184]\n",
      "epoch:3, iter:89,  [D loss: 0.685482, acc.: 63.00%] [G loss: 0.732149]\n",
      "epoch:3, iter:90,  [D loss: 0.684265, acc.: 58.00%] [G loss: 0.747066]\n",
      "epoch:3, iter:91,  [D loss: 0.686196, acc.: 58.00%] [G loss: 0.737193]\n",
      "epoch:3, iter:92,  [D loss: 0.680357, acc.: 55.00%] [G loss: 0.739890]\n",
      "epoch:3, iter:93,  [D loss: 0.687495, acc.: 61.00%] [G loss: 0.736436]\n",
      "epoch:3, iter:94,  [D loss: 0.683961, acc.: 58.00%] [G loss: 0.742175]\n",
      "epoch:3, iter:95,  [D loss: 0.683051, acc.: 60.00%] [G loss: 0.733986]\n",
      "epoch:3, iter:96,  [D loss: 0.666509, acc.: 68.00%] [G loss: 0.728805]\n",
      "epoch:3, iter:97,  [D loss: 0.687128, acc.: 59.00%] [G loss: 0.740624]\n",
      "epoch:3, iter:98,  [D loss: 0.686140, acc.: 57.00%] [G loss: 0.739331]\n",
      "epoch:3, iter:99,  [D loss: 0.670721, acc.: 65.00%] [G loss: 0.743923]\n",
      "epoch:4, iter:0,  [D loss: 0.682064, acc.: 59.00%] [G loss: 0.749078]\n",
      "epoch:4, iter:1,  [D loss: 0.666633, acc.: 68.00%] [G loss: 0.749511]\n",
      "epoch:4, iter:2,  [D loss: 0.680991, acc.: 58.00%] [G loss: 0.736238]\n",
      "epoch:4, iter:3,  [D loss: 0.675145, acc.: 63.00%] [G loss: 0.738183]\n",
      "epoch:4, iter:4,  [D loss: 0.679557, acc.: 63.00%] [G loss: 0.731677]\n",
      "epoch:4, iter:5,  [D loss: 0.678291, acc.: 56.00%] [G loss: 0.736349]\n",
      "epoch:4, iter:6,  [D loss: 0.693381, acc.: 52.00%] [G loss: 0.734409]\n",
      "epoch:4, iter:7,  [D loss: 0.687969, acc.: 52.00%] [G loss: 0.747919]\n",
      "epoch:4, iter:8,  [D loss: 0.689894, acc.: 56.00%] [G loss: 0.732884]\n",
      "epoch:4, iter:9,  [D loss: 0.681886, acc.: 64.00%] [G loss: 0.740228]\n",
      "epoch:4, iter:10,  [D loss: 0.690076, acc.: 48.00%] [G loss: 0.735850]\n",
      "epoch:4, iter:11,  [D loss: 0.679241, acc.: 63.00%] [G loss: 0.741309]\n",
      "epoch:4, iter:12,  [D loss: 0.682338, acc.: 59.00%] [G loss: 0.741179]\n",
      "epoch:4, iter:13,  [D loss: 0.677423, acc.: 55.00%] [G loss: 0.733191]\n",
      "epoch:4, iter:14,  [D loss: 0.679567, acc.: 57.00%] [G loss: 0.750176]\n",
      "epoch:4, iter:15,  [D loss: 0.679788, acc.: 61.00%] [G loss: 0.743515]\n",
      "epoch:4, iter:16,  [D loss: 0.683036, acc.: 61.00%] [G loss: 0.729005]\n",
      "epoch:4, iter:17,  [D loss: 0.688459, acc.: 52.00%] [G loss: 0.738191]\n",
      "epoch:4, iter:18,  [D loss: 0.692700, acc.: 49.00%] [G loss: 0.740355]\n",
      "epoch:4, iter:19,  [D loss: 0.678242, acc.: 63.00%] [G loss: 0.731081]\n",
      "epoch:4, iter:20,  [D loss: 0.684089, acc.: 56.00%] [G loss: 0.733473]\n",
      "epoch:4, iter:21,  [D loss: 0.684601, acc.: 57.00%] [G loss: 0.737475]\n",
      "epoch:4, iter:22,  [D loss: 0.678582, acc.: 63.00%] [G loss: 0.721871]\n",
      "epoch:4, iter:23,  [D loss: 0.675167, acc.: 64.00%] [G loss: 0.740080]\n",
      "epoch:4, iter:24,  [D loss: 0.671669, acc.: 65.00%] [G loss: 0.730107]\n",
      "epoch:4, iter:25,  [D loss: 0.672581, acc.: 66.00%] [G loss: 0.730366]\n",
      "epoch:4, iter:26,  [D loss: 0.680790, acc.: 58.00%] [G loss: 0.730690]\n",
      "epoch:4, iter:27,  [D loss: 0.678564, acc.: 55.00%] [G loss: 0.729323]\n",
      "epoch:4, iter:28,  [D loss: 0.683511, acc.: 55.00%] [G loss: 0.733342]\n",
      "epoch:4, iter:29,  [D loss: 0.685360, acc.: 53.00%] [G loss: 0.729689]\n",
      "epoch:4, iter:30,  [D loss: 0.678030, acc.: 61.00%] [G loss: 0.727089]\n",
      "epoch:4, iter:31,  [D loss: 0.670394, acc.: 63.00%] [G loss: 0.737401]\n",
      "epoch:4, iter:32,  [D loss: 0.683563, acc.: 57.00%] [G loss: 0.729970]\n",
      "epoch:4, iter:33,  [D loss: 0.678110, acc.: 62.00%] [G loss: 0.744630]\n",
      "epoch:4, iter:34,  [D loss: 0.674657, acc.: 60.00%] [G loss: 0.737952]\n",
      "epoch:4, iter:35,  [D loss: 0.688989, acc.: 53.00%] [G loss: 0.737726]\n",
      "epoch:4, iter:36,  [D loss: 0.679043, acc.: 59.00%] [G loss: 0.738048]\n",
      "epoch:4, iter:37,  [D loss: 0.681339, acc.: 55.00%] [G loss: 0.737737]\n",
      "epoch:4, iter:38,  [D loss: 0.682964, acc.: 60.00%] [G loss: 0.733929]\n",
      "epoch:4, iter:39,  [D loss: 0.684549, acc.: 53.00%] [G loss: 0.720528]\n",
      "epoch:4, iter:40,  [D loss: 0.681157, acc.: 56.00%] [G loss: 0.732290]\n",
      "epoch:4, iter:41,  [D loss: 0.673588, acc.: 64.00%] [G loss: 0.732346]\n",
      "epoch:4, iter:42,  [D loss: 0.680292, acc.: 62.00%] [G loss: 0.727075]\n",
      "epoch:4, iter:43,  [D loss: 0.675106, acc.: 65.00%] [G loss: 0.734070]\n",
      "epoch:4, iter:44,  [D loss: 0.672547, acc.: 62.00%] [G loss: 0.728188]\n",
      "epoch:4, iter:45,  [D loss: 0.685054, acc.: 54.00%] [G loss: 0.726654]\n",
      "epoch:4, iter:46,  [D loss: 0.680251, acc.: 57.00%] [G loss: 0.725535]\n",
      "epoch:4, iter:47,  [D loss: 0.687827, acc.: 51.00%] [G loss: 0.726301]\n",
      "epoch:4, iter:48,  [D loss: 0.686885, acc.: 53.00%] [G loss: 0.721373]\n",
      "epoch:4, iter:49,  [D loss: 0.683508, acc.: 56.00%] [G loss: 0.749700]\n",
      "epoch:4, iter:50,  [D loss: 0.674903, acc.: 65.00%] [G loss: 0.736042]\n",
      "epoch:4, iter:51,  [D loss: 0.690850, acc.: 54.00%] [G loss: 0.748164]\n",
      "epoch:4, iter:52,  [D loss: 0.679482, acc.: 64.00%] [G loss: 0.737797]\n",
      "epoch:4, iter:53,  [D loss: 0.684682, acc.: 52.00%] [G loss: 0.746229]\n",
      "epoch:4, iter:54,  [D loss: 0.678439, acc.: 61.00%] [G loss: 0.746534]\n",
      "epoch:4, iter:55,  [D loss: 0.692481, acc.: 53.00%] [G loss: 0.733338]\n",
      "epoch:4, iter:56,  [D loss: 0.689614, acc.: 56.00%] [G loss: 0.730737]\n",
      "epoch:4, iter:57,  [D loss: 0.678013, acc.: 68.00%] [G loss: 0.734342]\n",
      "epoch:4, iter:58,  [D loss: 0.692948, acc.: 48.00%] [G loss: 0.734468]\n",
      "epoch:4, iter:59,  [D loss: 0.684612, acc.: 60.00%] [G loss: 0.725645]\n",
      "epoch:4, iter:60,  [D loss: 0.670520, acc.: 59.00%] [G loss: 0.743943]\n",
      "epoch:4, iter:61,  [D loss: 0.676529, acc.: 56.00%] [G loss: 0.743113]\n",
      "epoch:4, iter:62,  [D loss: 0.677083, acc.: 64.00%] [G loss: 0.749650]\n",
      "epoch:4, iter:63,  [D loss: 0.682431, acc.: 61.00%] [G loss: 0.729149]\n",
      "epoch:4, iter:64,  [D loss: 0.686710, acc.: 51.00%] [G loss: 0.747457]\n",
      "epoch:4, iter:65,  [D loss: 0.678761, acc.: 57.00%] [G loss: 0.739269]\n",
      "epoch:4, iter:66,  [D loss: 0.678783, acc.: 64.00%] [G loss: 0.744148]\n",
      "epoch:4, iter:67,  [D loss: 0.686256, acc.: 58.00%] [G loss: 0.736842]\n",
      "epoch:4, iter:68,  [D loss: 0.681458, acc.: 59.00%] [G loss: 0.751985]\n",
      "epoch:4, iter:69,  [D loss: 0.678961, acc.: 61.00%] [G loss: 0.732543]\n",
      "epoch:4, iter:70,  [D loss: 0.674891, acc.: 62.00%] [G loss: 0.748972]\n",
      "epoch:4, iter:71,  [D loss: 0.673923, acc.: 59.00%] [G loss: 0.737506]\n",
      "epoch:4, iter:72,  [D loss: 0.679010, acc.: 60.00%] [G loss: 0.732318]\n",
      "epoch:4, iter:73,  [D loss: 0.677173, acc.: 61.00%] [G loss: 0.736347]\n",
      "epoch:4, iter:74,  [D loss: 0.674998, acc.: 58.00%] [G loss: 0.742851]\n",
      "epoch:4, iter:75,  [D loss: 0.666328, acc.: 71.00%] [G loss: 0.735854]\n",
      "epoch:4, iter:76,  [D loss: 0.690553, acc.: 49.00%] [G loss: 0.735616]\n",
      "epoch:4, iter:77,  [D loss: 0.685428, acc.: 64.00%] [G loss: 0.725212]\n",
      "epoch:4, iter:78,  [D loss: 0.689614, acc.: 51.00%] [G loss: 0.731995]\n",
      "epoch:4, iter:79,  [D loss: 0.694153, acc.: 50.00%] [G loss: 0.728125]\n",
      "epoch:4, iter:80,  [D loss: 0.685956, acc.: 59.00%] [G loss: 0.729399]\n",
      "epoch:4, iter:81,  [D loss: 0.689970, acc.: 48.00%] [G loss: 0.735653]\n",
      "epoch:4, iter:82,  [D loss: 0.680751, acc.: 60.00%] [G loss: 0.733449]\n",
      "epoch:4, iter:83,  [D loss: 0.697972, acc.: 45.00%] [G loss: 0.732444]\n",
      "epoch:4, iter:84,  [D loss: 0.692672, acc.: 50.00%] [G loss: 0.735061]\n",
      "epoch:4, iter:85,  [D loss: 0.676842, acc.: 67.00%] [G loss: 0.740451]\n",
      "epoch:4, iter:86,  [D loss: 0.678188, acc.: 60.00%] [G loss: 0.733009]\n",
      "epoch:4, iter:87,  [D loss: 0.684903, acc.: 55.00%] [G loss: 0.737105]\n",
      "epoch:4, iter:88,  [D loss: 0.670846, acc.: 53.00%] [G loss: 0.727970]\n",
      "epoch:4, iter:89,  [D loss: 0.666396, acc.: 69.00%] [G loss: 0.749087]\n",
      "epoch:4, iter:90,  [D loss: 0.685479, acc.: 56.00%] [G loss: 0.736845]\n",
      "epoch:4, iter:91,  [D loss: 0.677308, acc.: 62.00%] [G loss: 0.733272]\n",
      "epoch:4, iter:92,  [D loss: 0.681516, acc.: 60.00%] [G loss: 0.726777]\n",
      "epoch:4, iter:93,  [D loss: 0.678692, acc.: 64.00%] [G loss: 0.734224]\n",
      "epoch:4, iter:94,  [D loss: 0.686834, acc.: 58.00%] [G loss: 0.725646]\n",
      "epoch:4, iter:95,  [D loss: 0.685414, acc.: 54.00%] [G loss: 0.715350]\n",
      "epoch:4, iter:96,  [D loss: 0.677832, acc.: 66.00%] [G loss: 0.744634]\n",
      "epoch:4, iter:97,  [D loss: 0.679908, acc.: 60.00%] [G loss: 0.740381]\n",
      "epoch:4, iter:98,  [D loss: 0.686108, acc.: 56.00%] [G loss: 0.738528]\n",
      "epoch:4, iter:99,  [D loss: 0.678708, acc.: 62.00%] [G loss: 0.746251]\n",
      "epoch:5, iter:0,  [D loss: 0.677796, acc.: 59.00%] [G loss: 0.747199]\n",
      "epoch:5, iter:1,  [D loss: 0.686590, acc.: 57.00%] [G loss: 0.733893]\n",
      "epoch:5, iter:2,  [D loss: 0.666607, acc.: 71.00%] [G loss: 0.744982]\n",
      "epoch:5, iter:3,  [D loss: 0.666260, acc.: 69.00%] [G loss: 0.752587]\n",
      "epoch:5, iter:4,  [D loss: 0.689106, acc.: 58.00%] [G loss: 0.744979]\n",
      "epoch:5, iter:5,  [D loss: 0.685089, acc.: 54.00%] [G loss: 0.736855]\n",
      "epoch:5, iter:6,  [D loss: 0.670290, acc.: 61.00%] [G loss: 0.738079]\n",
      "epoch:5, iter:7,  [D loss: 0.687367, acc.: 55.00%] [G loss: 0.733477]\n",
      "epoch:5, iter:8,  [D loss: 0.676494, acc.: 64.00%] [G loss: 0.741703]\n",
      "epoch:5, iter:9,  [D loss: 0.680229, acc.: 57.00%] [G loss: 0.732808]\n",
      "epoch:5, iter:10,  [D loss: 0.678537, acc.: 64.00%] [G loss: 0.734523]\n",
      "epoch:5, iter:11,  [D loss: 0.675563, acc.: 58.00%] [G loss: 0.721525]\n",
      "epoch:5, iter:12,  [D loss: 0.696455, acc.: 50.00%] [G loss: 0.729771]\n",
      "epoch:5, iter:13,  [D loss: 0.690466, acc.: 53.00%] [G loss: 0.720766]\n",
      "epoch:5, iter:14,  [D loss: 0.688157, acc.: 57.00%] [G loss: 0.718340]\n",
      "epoch:5, iter:15,  [D loss: 0.695524, acc.: 55.00%] [G loss: 0.717171]\n",
      "epoch:5, iter:16,  [D loss: 0.680312, acc.: 53.00%] [G loss: 0.723585]\n",
      "epoch:5, iter:17,  [D loss: 0.674812, acc.: 60.00%] [G loss: 0.724710]\n",
      "epoch:5, iter:18,  [D loss: 0.691360, acc.: 60.00%] [G loss: 0.729958]\n",
      "epoch:5, iter:19,  [D loss: 0.691166, acc.: 55.00%] [G loss: 0.722983]\n",
      "epoch:5, iter:20,  [D loss: 0.686826, acc.: 53.00%] [G loss: 0.742225]\n",
      "epoch:5, iter:21,  [D loss: 0.687360, acc.: 58.00%] [G loss: 0.737228]\n",
      "epoch:5, iter:22,  [D loss: 0.683530, acc.: 55.00%] [G loss: 0.744201]\n",
      "epoch:5, iter:23,  [D loss: 0.693612, acc.: 57.00%] [G loss: 0.742918]\n",
      "epoch:5, iter:24,  [D loss: 0.682204, acc.: 57.00%] [G loss: 0.732896]\n",
      "epoch:5, iter:25,  [D loss: 0.679349, acc.: 60.00%] [G loss: 0.750407]\n",
      "epoch:5, iter:26,  [D loss: 0.674995, acc.: 66.00%] [G loss: 0.746567]\n",
      "epoch:5, iter:27,  [D loss: 0.682238, acc.: 60.00%] [G loss: 0.751534]\n",
      "epoch:5, iter:28,  [D loss: 0.689209, acc.: 49.00%] [G loss: 0.746060]\n",
      "epoch:5, iter:29,  [D loss: 0.685065, acc.: 57.00%] [G loss: 0.749108]\n",
      "epoch:5, iter:30,  [D loss: 0.677924, acc.: 62.00%] [G loss: 0.760999]\n",
      "epoch:5, iter:31,  [D loss: 0.694225, acc.: 48.00%] [G loss: 0.739958]\n",
      "epoch:5, iter:32,  [D loss: 0.681541, acc.: 59.00%] [G loss: 0.751957]\n",
      "epoch:5, iter:33,  [D loss: 0.671430, acc.: 64.00%] [G loss: 0.756209]\n",
      "epoch:5, iter:34,  [D loss: 0.690123, acc.: 58.00%] [G loss: 0.737607]\n",
      "epoch:5, iter:35,  [D loss: 0.677213, acc.: 60.00%] [G loss: 0.754062]\n",
      "epoch:5, iter:36,  [D loss: 0.679548, acc.: 60.00%] [G loss: 0.743016]\n",
      "epoch:5, iter:37,  [D loss: 0.695230, acc.: 55.00%] [G loss: 0.737207]\n",
      "epoch:5, iter:38,  [D loss: 0.687627, acc.: 55.00%] [G loss: 0.734927]\n",
      "epoch:5, iter:39,  [D loss: 0.682928, acc.: 61.00%] [G loss: 0.743362]\n",
      "epoch:5, iter:40,  [D loss: 0.684558, acc.: 56.00%] [G loss: 0.728977]\n",
      "epoch:5, iter:41,  [D loss: 0.701431, acc.: 46.00%] [G loss: 0.725712]\n",
      "epoch:5, iter:42,  [D loss: 0.674832, acc.: 60.00%] [G loss: 0.727711]\n",
      "epoch:5, iter:43,  [D loss: 0.682143, acc.: 62.00%] [G loss: 0.736159]\n",
      "epoch:5, iter:44,  [D loss: 0.689131, acc.: 53.00%] [G loss: 0.722983]\n",
      "epoch:5, iter:45,  [D loss: 0.688225, acc.: 51.00%] [G loss: 0.720616]\n",
      "epoch:5, iter:46,  [D loss: 0.694164, acc.: 55.00%] [G loss: 0.724594]\n",
      "epoch:5, iter:47,  [D loss: 0.681204, acc.: 61.00%] [G loss: 0.733698]\n",
      "epoch:5, iter:48,  [D loss: 0.686978, acc.: 51.00%] [G loss: 0.731969]\n",
      "epoch:5, iter:49,  [D loss: 0.690335, acc.: 59.00%] [G loss: 0.735042]\n",
      "epoch:5, iter:50,  [D loss: 0.681605, acc.: 54.00%] [G loss: 0.738165]\n",
      "epoch:5, iter:51,  [D loss: 0.685574, acc.: 57.00%] [G loss: 0.734118]\n",
      "epoch:5, iter:52,  [D loss: 0.686392, acc.: 57.00%] [G loss: 0.744027]\n",
      "epoch:5, iter:53,  [D loss: 0.683633, acc.: 60.00%] [G loss: 0.749148]\n",
      "epoch:5, iter:54,  [D loss: 0.676243, acc.: 62.00%] [G loss: 0.735358]\n",
      "epoch:5, iter:55,  [D loss: 0.679804, acc.: 56.00%] [G loss: 0.726907]\n",
      "epoch:5, iter:56,  [D loss: 0.697375, acc.: 50.00%] [G loss: 0.744806]\n",
      "epoch:5, iter:57,  [D loss: 0.685777, acc.: 53.00%] [G loss: 0.729128]\n",
      "epoch:5, iter:58,  [D loss: 0.678100, acc.: 55.00%] [G loss: 0.739104]\n",
      "epoch:5, iter:59,  [D loss: 0.686644, acc.: 56.00%] [G loss: 0.747298]\n",
      "epoch:5, iter:60,  [D loss: 0.688446, acc.: 53.00%] [G loss: 0.743900]\n",
      "epoch:5, iter:61,  [D loss: 0.670586, acc.: 66.00%] [G loss: 0.753198]\n",
      "epoch:5, iter:62,  [D loss: 0.681843, acc.: 60.00%] [G loss: 0.750136]\n",
      "epoch:5, iter:63,  [D loss: 0.669387, acc.: 64.00%] [G loss: 0.744015]\n",
      "epoch:5, iter:64,  [D loss: 0.678029, acc.: 60.00%] [G loss: 0.742808]\n",
      "epoch:5, iter:65,  [D loss: 0.696322, acc.: 51.00%] [G loss: 0.736484]\n",
      "epoch:5, iter:66,  [D loss: 0.697577, acc.: 52.00%] [G loss: 0.739683]\n",
      "epoch:5, iter:67,  [D loss: 0.672369, acc.: 63.00%] [G loss: 0.740350]\n",
      "epoch:5, iter:68,  [D loss: 0.684431, acc.: 56.00%] [G loss: 0.730696]\n",
      "epoch:5, iter:69,  [D loss: 0.693112, acc.: 48.00%] [G loss: 0.742443]\n",
      "epoch:5, iter:70,  [D loss: 0.686146, acc.: 49.00%] [G loss: 0.748252]\n",
      "epoch:5, iter:71,  [D loss: 0.682386, acc.: 56.00%] [G loss: 0.745471]\n",
      "epoch:5, iter:72,  [D loss: 0.688048, acc.: 57.00%] [G loss: 0.715392]\n",
      "epoch:5, iter:73,  [D loss: 0.678560, acc.: 58.00%] [G loss: 0.722923]\n",
      "epoch:5, iter:74,  [D loss: 0.681509, acc.: 53.00%] [G loss: 0.737322]\n",
      "epoch:5, iter:75,  [D loss: 0.677394, acc.: 59.00%] [G loss: 0.733660]\n",
      "epoch:5, iter:76,  [D loss: 0.681838, acc.: 53.00%] [G loss: 0.730306]\n",
      "epoch:5, iter:77,  [D loss: 0.685079, acc.: 55.00%] [G loss: 0.732858]\n",
      "epoch:5, iter:78,  [D loss: 0.686330, acc.: 59.00%] [G loss: 0.724817]\n",
      "epoch:5, iter:79,  [D loss: 0.697823, acc.: 49.00%] [G loss: 0.731406]\n",
      "epoch:5, iter:80,  [D loss: 0.688399, acc.: 55.00%] [G loss: 0.727392]\n",
      "epoch:5, iter:81,  [D loss: 0.677952, acc.: 60.00%] [G loss: 0.732994]\n",
      "epoch:5, iter:82,  [D loss: 0.693700, acc.: 55.00%] [G loss: 0.743604]\n",
      "epoch:5, iter:83,  [D loss: 0.683864, acc.: 64.00%] [G loss: 0.736670]\n",
      "epoch:5, iter:84,  [D loss: 0.687376, acc.: 56.00%] [G loss: 0.734583]\n",
      "epoch:5, iter:85,  [D loss: 0.683706, acc.: 59.00%] [G loss: 0.738291]\n",
      "epoch:5, iter:86,  [D loss: 0.679038, acc.: 64.00%] [G loss: 0.729542]\n",
      "epoch:5, iter:87,  [D loss: 0.699656, acc.: 44.00%] [G loss: 0.742790]\n",
      "epoch:5, iter:88,  [D loss: 0.676797, acc.: 60.00%] [G loss: 0.731340]\n",
      "epoch:5, iter:89,  [D loss: 0.687644, acc.: 56.00%] [G loss: 0.737456]\n",
      "epoch:5, iter:90,  [D loss: 0.685863, acc.: 55.00%] [G loss: 0.734364]\n",
      "epoch:5, iter:91,  [D loss: 0.674161, acc.: 65.00%] [G loss: 0.737710]\n",
      "epoch:5, iter:92,  [D loss: 0.672447, acc.: 64.00%] [G loss: 0.749047]\n",
      "epoch:5, iter:93,  [D loss: 0.679584, acc.: 66.00%] [G loss: 0.735219]\n",
      "epoch:5, iter:94,  [D loss: 0.686907, acc.: 52.00%] [G loss: 0.739986]\n",
      "epoch:5, iter:95,  [D loss: 0.681608, acc.: 60.00%] [G loss: 0.742202]\n",
      "epoch:5, iter:96,  [D loss: 0.689481, acc.: 53.00%] [G loss: 0.733315]\n",
      "epoch:5, iter:97,  [D loss: 0.683749, acc.: 57.00%] [G loss: 0.732127]\n",
      "epoch:5, iter:98,  [D loss: 0.683640, acc.: 54.00%] [G loss: 0.738688]\n",
      "epoch:5, iter:99,  [D loss: 0.677043, acc.: 61.00%] [G loss: 0.740678]\n",
      "epoch:6, iter:0,  [D loss: 0.677309, acc.: 62.00%] [G loss: 0.731776]\n",
      "epoch:6, iter:1,  [D loss: 0.678558, acc.: 59.00%] [G loss: 0.734734]\n",
      "epoch:6, iter:2,  [D loss: 0.680279, acc.: 56.00%] [G loss: 0.745182]\n",
      "epoch:6, iter:3,  [D loss: 0.680388, acc.: 60.00%] [G loss: 0.738961]\n",
      "epoch:6, iter:4,  [D loss: 0.708894, acc.: 41.00%] [G loss: 0.735259]\n",
      "epoch:6, iter:5,  [D loss: 0.670457, acc.: 68.00%] [G loss: 0.740156]\n",
      "epoch:6, iter:6,  [D loss: 0.690267, acc.: 57.00%] [G loss: 0.751872]\n",
      "epoch:6, iter:7,  [D loss: 0.681266, acc.: 61.00%] [G loss: 0.747051]\n",
      "epoch:6, iter:8,  [D loss: 0.679964, acc.: 55.00%] [G loss: 0.740623]\n",
      "epoch:6, iter:9,  [D loss: 0.685137, acc.: 57.00%] [G loss: 0.729637]\n",
      "epoch:6, iter:10,  [D loss: 0.679139, acc.: 61.00%] [G loss: 0.745692]\n",
      "epoch:6, iter:11,  [D loss: 0.676498, acc.: 57.00%] [G loss: 0.745891]\n",
      "epoch:6, iter:12,  [D loss: 0.689014, acc.: 49.00%] [G loss: 0.740256]\n",
      "epoch:6, iter:13,  [D loss: 0.688281, acc.: 56.00%] [G loss: 0.734716]\n",
      "epoch:6, iter:14,  [D loss: 0.664139, acc.: 73.00%] [G loss: 0.741934]\n",
      "epoch:6, iter:15,  [D loss: 0.681560, acc.: 56.00%] [G loss: 0.743657]\n",
      "epoch:6, iter:16,  [D loss: 0.685532, acc.: 55.00%] [G loss: 0.745210]\n",
      "epoch:6, iter:17,  [D loss: 0.676832, acc.: 60.00%] [G loss: 0.740912]\n",
      "epoch:6, iter:18,  [D loss: 0.688479, acc.: 54.00%] [G loss: 0.729436]\n",
      "epoch:6, iter:19,  [D loss: 0.686634, acc.: 52.00%] [G loss: 0.731122]\n",
      "epoch:6, iter:20,  [D loss: 0.677654, acc.: 63.00%] [G loss: 0.733439]\n",
      "epoch:6, iter:21,  [D loss: 0.685530, acc.: 55.00%] [G loss: 0.745345]\n",
      "epoch:6, iter:22,  [D loss: 0.678158, acc.: 58.00%] [G loss: 0.723056]\n",
      "epoch:6, iter:23,  [D loss: 0.686177, acc.: 55.00%] [G loss: 0.729117]\n",
      "epoch:6, iter:24,  [D loss: 0.688833, acc.: 56.00%] [G loss: 0.729830]\n",
      "epoch:6, iter:25,  [D loss: 0.678527, acc.: 52.00%] [G loss: 0.717055]\n",
      "epoch:6, iter:26,  [D loss: 0.690521, acc.: 56.00%] [G loss: 0.717897]\n",
      "epoch:6, iter:27,  [D loss: 0.680944, acc.: 57.00%] [G loss: 0.718173]\n",
      "epoch:6, iter:28,  [D loss: 0.678707, acc.: 56.00%] [G loss: 0.715972]\n",
      "epoch:6, iter:29,  [D loss: 0.687805, acc.: 50.00%] [G loss: 0.721438]\n",
      "epoch:6, iter:30,  [D loss: 0.672202, acc.: 58.00%] [G loss: 0.736488]\n",
      "epoch:6, iter:31,  [D loss: 0.678928, acc.: 62.00%] [G loss: 0.726797]\n",
      "epoch:6, iter:32,  [D loss: 0.679530, acc.: 56.00%] [G loss: 0.733079]\n",
      "epoch:6, iter:33,  [D loss: 0.685073, acc.: 56.00%] [G loss: 0.724244]\n",
      "epoch:6, iter:34,  [D loss: 0.672244, acc.: 64.00%] [G loss: 0.734716]\n",
      "epoch:6, iter:35,  [D loss: 0.674934, acc.: 60.00%] [G loss: 0.735381]\n",
      "epoch:6, iter:36,  [D loss: 0.674000, acc.: 63.00%] [G loss: 0.730482]\n",
      "epoch:6, iter:37,  [D loss: 0.672855, acc.: 67.00%] [G loss: 0.737097]\n",
      "epoch:6, iter:38,  [D loss: 0.678355, acc.: 62.00%] [G loss: 0.734302]\n",
      "epoch:6, iter:39,  [D loss: 0.679721, acc.: 59.00%] [G loss: 0.736874]\n",
      "epoch:6, iter:40,  [D loss: 0.683250, acc.: 55.00%] [G loss: 0.736361]\n",
      "epoch:6, iter:41,  [D loss: 0.673536, acc.: 57.00%] [G loss: 0.730371]\n",
      "epoch:6, iter:42,  [D loss: 0.686456, acc.: 56.00%] [G loss: 0.730290]\n",
      "epoch:6, iter:43,  [D loss: 0.667382, acc.: 64.00%] [G loss: 0.751710]\n",
      "epoch:6, iter:44,  [D loss: 0.688998, acc.: 50.00%] [G loss: 0.732201]\n",
      "epoch:6, iter:45,  [D loss: 0.681547, acc.: 56.00%] [G loss: 0.729974]\n",
      "epoch:6, iter:46,  [D loss: 0.687629, acc.: 50.00%] [G loss: 0.721576]\n",
      "epoch:6, iter:47,  [D loss: 0.688002, acc.: 55.00%] [G loss: 0.734318]\n",
      "epoch:6, iter:48,  [D loss: 0.697155, acc.: 53.00%] [G loss: 0.728164]\n",
      "epoch:6, iter:49,  [D loss: 0.678781, acc.: 60.00%] [G loss: 0.721888]\n",
      "epoch:6, iter:50,  [D loss: 0.676673, acc.: 56.00%] [G loss: 0.719645]\n",
      "epoch:6, iter:51,  [D loss: 0.675339, acc.: 62.00%] [G loss: 0.720606]\n",
      "epoch:6, iter:52,  [D loss: 0.696373, acc.: 49.00%] [G loss: 0.719097]\n",
      "epoch:6, iter:53,  [D loss: 0.685298, acc.: 57.00%] [G loss: 0.730240]\n",
      "epoch:6, iter:54,  [D loss: 0.691763, acc.: 52.00%] [G loss: 0.741766]\n",
      "epoch:6, iter:55,  [D loss: 0.698234, acc.: 50.00%] [G loss: 0.732343]\n",
      "epoch:6, iter:56,  [D loss: 0.695114, acc.: 51.00%] [G loss: 0.730532]\n",
      "epoch:6, iter:57,  [D loss: 0.688586, acc.: 53.00%] [G loss: 0.754295]\n",
      "epoch:6, iter:58,  [D loss: 0.688485, acc.: 57.00%] [G loss: 0.758983]\n",
      "epoch:6, iter:59,  [D loss: 0.688594, acc.: 61.00%] [G loss: 0.751047]\n",
      "epoch:6, iter:60,  [D loss: 0.679186, acc.: 68.00%] [G loss: 0.756818]\n",
      "epoch:6, iter:61,  [D loss: 0.682565, acc.: 60.00%] [G loss: 0.758237]\n",
      "epoch:6, iter:62,  [D loss: 0.678002, acc.: 63.00%] [G loss: 0.752821]\n",
      "epoch:6, iter:63,  [D loss: 0.687912, acc.: 53.00%] [G loss: 0.744683]\n",
      "epoch:6, iter:64,  [D loss: 0.676935, acc.: 56.00%] [G loss: 0.745428]\n",
      "epoch:6, iter:65,  [D loss: 0.680084, acc.: 58.00%] [G loss: 0.755939]\n",
      "epoch:6, iter:66,  [D loss: 0.672862, acc.: 61.00%] [G loss: 0.739741]\n",
      "epoch:6, iter:67,  [D loss: 0.674010, acc.: 60.00%] [G loss: 0.752751]\n",
      "epoch:6, iter:68,  [D loss: 0.678078, acc.: 61.00%] [G loss: 0.746913]\n",
      "epoch:6, iter:69,  [D loss: 0.682083, acc.: 55.00%] [G loss: 0.733040]\n",
      "epoch:6, iter:70,  [D loss: 0.685372, acc.: 59.00%] [G loss: 0.730521]\n",
      "epoch:6, iter:71,  [D loss: 0.686796, acc.: 51.00%] [G loss: 0.728422]\n",
      "epoch:6, iter:72,  [D loss: 0.686467, acc.: 58.00%] [G loss: 0.731481]\n",
      "epoch:6, iter:73,  [D loss: 0.685056, acc.: 56.00%] [G loss: 0.728341]\n",
      "epoch:6, iter:74,  [D loss: 0.694508, acc.: 53.00%] [G loss: 0.717675]\n",
      "epoch:6, iter:75,  [D loss: 0.681270, acc.: 59.00%] [G loss: 0.713740]\n",
      "epoch:6, iter:76,  [D loss: 0.679193, acc.: 63.00%] [G loss: 0.716655]\n",
      "epoch:6, iter:77,  [D loss: 0.672699, acc.: 65.00%] [G loss: 0.721353]\n",
      "epoch:6, iter:78,  [D loss: 0.668066, acc.: 60.00%] [G loss: 0.732934]\n",
      "epoch:6, iter:79,  [D loss: 0.715711, acc.: 44.00%] [G loss: 0.723995]\n",
      "epoch:6, iter:80,  [D loss: 0.689351, acc.: 50.00%] [G loss: 0.723006]\n",
      "epoch:6, iter:81,  [D loss: 0.694846, acc.: 56.00%] [G loss: 0.739315]\n",
      "epoch:6, iter:82,  [D loss: 0.691077, acc.: 57.00%] [G loss: 0.724857]\n",
      "epoch:6, iter:83,  [D loss: 0.698796, acc.: 46.00%] [G loss: 0.739887]\n",
      "epoch:6, iter:84,  [D loss: 0.686356, acc.: 56.00%] [G loss: 0.736167]\n",
      "epoch:6, iter:85,  [D loss: 0.671881, acc.: 63.00%] [G loss: 0.738154]\n",
      "epoch:6, iter:86,  [D loss: 0.681028, acc.: 56.00%] [G loss: 0.731822]\n",
      "epoch:6, iter:87,  [D loss: 0.683865, acc.: 61.00%] [G loss: 0.745912]\n",
      "epoch:6, iter:88,  [D loss: 0.676232, acc.: 61.00%] [G loss: 0.719690]\n",
      "epoch:6, iter:89,  [D loss: 0.684433, acc.: 57.00%] [G loss: 0.722407]\n",
      "epoch:6, iter:90,  [D loss: 0.690332, acc.: 48.00%] [G loss: 0.708844]\n",
      "epoch:6, iter:91,  [D loss: 0.684781, acc.: 55.00%] [G loss: 0.723198]\n",
      "epoch:6, iter:92,  [D loss: 0.680407, acc.: 62.00%] [G loss: 0.717720]\n",
      "epoch:6, iter:93,  [D loss: 0.680434, acc.: 54.00%] [G loss: 0.718522]\n",
      "epoch:6, iter:94,  [D loss: 0.672610, acc.: 63.00%] [G loss: 0.730820]\n",
      "epoch:6, iter:95,  [D loss: 0.687969, acc.: 50.00%] [G loss: 0.724856]\n",
      "epoch:6, iter:96,  [D loss: 0.678328, acc.: 58.00%] [G loss: 0.732608]\n",
      "epoch:6, iter:97,  [D loss: 0.678222, acc.: 63.00%] [G loss: 0.736384]\n",
      "epoch:6, iter:98,  [D loss: 0.678038, acc.: 62.00%] [G loss: 0.753523]\n",
      "epoch:6, iter:99,  [D loss: 0.681794, acc.: 57.00%] [G loss: 0.734197]\n",
      "epoch:7, iter:0,  [D loss: 0.694225, acc.: 50.00%] [G loss: 0.729422]\n",
      "epoch:7, iter:1,  [D loss: 0.706438, acc.: 46.00%] [G loss: 0.734596]\n",
      "epoch:7, iter:2,  [D loss: 0.678997, acc.: 59.00%] [G loss: 0.738717]\n",
      "epoch:7, iter:3,  [D loss: 0.689701, acc.: 55.00%] [G loss: 0.719527]\n",
      "epoch:7, iter:4,  [D loss: 0.671430, acc.: 62.00%] [G loss: 0.730741]\n",
      "epoch:7, iter:5,  [D loss: 0.695536, acc.: 54.00%] [G loss: 0.728702]\n",
      "epoch:7, iter:6,  [D loss: 0.688769, acc.: 54.00%] [G loss: 0.728161]\n",
      "epoch:7, iter:7,  [D loss: 0.668967, acc.: 61.00%] [G loss: 0.733666]\n",
      "epoch:7, iter:8,  [D loss: 0.683437, acc.: 60.00%] [G loss: 0.721833]\n",
      "epoch:7, iter:9,  [D loss: 0.688148, acc.: 51.00%] [G loss: 0.733040]\n",
      "epoch:7, iter:10,  [D loss: 0.689054, acc.: 51.00%] [G loss: 0.724932]\n",
      "epoch:7, iter:11,  [D loss: 0.684338, acc.: 55.00%] [G loss: 0.724694]\n",
      "epoch:7, iter:12,  [D loss: 0.691305, acc.: 55.00%] [G loss: 0.729053]\n",
      "epoch:7, iter:13,  [D loss: 0.673123, acc.: 62.00%] [G loss: 0.729627]\n",
      "epoch:7, iter:14,  [D loss: 0.673596, acc.: 60.00%] [G loss: 0.735897]\n",
      "epoch:7, iter:15,  [D loss: 0.673842, acc.: 64.00%] [G loss: 0.738140]\n",
      "epoch:7, iter:16,  [D loss: 0.675989, acc.: 62.00%] [G loss: 0.730109]\n",
      "epoch:7, iter:17,  [D loss: 0.692313, acc.: 52.00%] [G loss: 0.726552]\n",
      "epoch:7, iter:18,  [D loss: 0.682981, acc.: 59.00%] [G loss: 0.733011]\n",
      "epoch:7, iter:19,  [D loss: 0.683295, acc.: 63.00%] [G loss: 0.726274]\n",
      "epoch:7, iter:20,  [D loss: 0.687566, acc.: 55.00%] [G loss: 0.718787]\n",
      "epoch:7, iter:21,  [D loss: 0.698674, acc.: 44.00%] [G loss: 0.732067]\n",
      "epoch:7, iter:22,  [D loss: 0.692616, acc.: 53.00%] [G loss: 0.726972]\n",
      "epoch:7, iter:23,  [D loss: 0.694261, acc.: 51.00%] [G loss: 0.720606]\n",
      "epoch:7, iter:24,  [D loss: 0.684853, acc.: 60.00%] [G loss: 0.718645]\n",
      "epoch:7, iter:25,  [D loss: 0.685546, acc.: 58.00%] [G loss: 0.723637]\n",
      "epoch:7, iter:26,  [D loss: 0.682111, acc.: 63.00%] [G loss: 0.727421]\n",
      "epoch:7, iter:27,  [D loss: 0.677510, acc.: 55.00%] [G loss: 0.730556]\n",
      "epoch:7, iter:28,  [D loss: 0.684846, acc.: 56.00%] [G loss: 0.731726]\n",
      "epoch:7, iter:29,  [D loss: 0.683605, acc.: 57.00%] [G loss: 0.731202]\n",
      "epoch:7, iter:30,  [D loss: 0.675577, acc.: 63.00%] [G loss: 0.721632]\n",
      "epoch:7, iter:31,  [D loss: 0.679753, acc.: 53.00%] [G loss: 0.715316]\n",
      "epoch:7, iter:32,  [D loss: 0.677858, acc.: 65.00%] [G loss: 0.722871]\n",
      "epoch:7, iter:33,  [D loss: 0.681273, acc.: 58.00%] [G loss: 0.722052]\n",
      "epoch:7, iter:34,  [D loss: 0.678754, acc.: 55.00%] [G loss: 0.733947]\n",
      "epoch:7, iter:35,  [D loss: 0.684908, acc.: 57.00%] [G loss: 0.730446]\n",
      "epoch:7, iter:36,  [D loss: 0.672829, acc.: 58.00%] [G loss: 0.737886]\n",
      "epoch:7, iter:37,  [D loss: 0.688270, acc.: 48.00%] [G loss: 0.734430]\n",
      "epoch:7, iter:38,  [D loss: 0.682095, acc.: 56.00%] [G loss: 0.747471]\n",
      "epoch:7, iter:39,  [D loss: 0.680387, acc.: 61.00%] [G loss: 0.747956]\n",
      "epoch:7, iter:40,  [D loss: 0.685650, acc.: 58.00%] [G loss: 0.733398]\n",
      "epoch:7, iter:41,  [D loss: 0.688842, acc.: 52.00%] [G loss: 0.722572]\n",
      "epoch:7, iter:42,  [D loss: 0.670264, acc.: 65.00%] [G loss: 0.740321]\n",
      "epoch:7, iter:43,  [D loss: 0.696667, acc.: 56.00%] [G loss: 0.726045]\n",
      "epoch:7, iter:44,  [D loss: 0.689978, acc.: 52.00%] [G loss: 0.729203]\n",
      "epoch:7, iter:45,  [D loss: 0.695493, acc.: 52.00%] [G loss: 0.722133]\n",
      "epoch:7, iter:46,  [D loss: 0.685182, acc.: 60.00%] [G loss: 0.710829]\n",
      "epoch:7, iter:47,  [D loss: 0.685419, acc.: 55.00%] [G loss: 0.736735]\n",
      "epoch:7, iter:48,  [D loss: 0.686511, acc.: 52.00%] [G loss: 0.725920]\n",
      "epoch:7, iter:49,  [D loss: 0.687332, acc.: 53.00%] [G loss: 0.720227]\n",
      "epoch:7, iter:50,  [D loss: 0.679102, acc.: 57.00%] [G loss: 0.725860]\n",
      "epoch:7, iter:51,  [D loss: 0.687733, acc.: 55.00%] [G loss: 0.722207]\n",
      "epoch:7, iter:52,  [D loss: 0.679441, acc.: 54.00%] [G loss: 0.727041]\n",
      "epoch:7, iter:53,  [D loss: 0.690008, acc.: 53.00%] [G loss: 0.735346]\n",
      "epoch:7, iter:54,  [D loss: 0.686908, acc.: 53.00%] [G loss: 0.740136]\n",
      "epoch:7, iter:55,  [D loss: 0.686600, acc.: 55.00%] [G loss: 0.746508]\n",
      "epoch:7, iter:56,  [D loss: 0.699116, acc.: 43.00%] [G loss: 0.730093]\n",
      "epoch:7, iter:57,  [D loss: 0.686466, acc.: 52.00%] [G loss: 0.730857]\n",
      "epoch:7, iter:58,  [D loss: 0.690838, acc.: 52.00%] [G loss: 0.732097]\n",
      "epoch:7, iter:59,  [D loss: 0.678537, acc.: 61.00%] [G loss: 0.739177]\n",
      "epoch:7, iter:60,  [D loss: 0.694872, acc.: 49.00%] [G loss: 0.734231]\n",
      "epoch:7, iter:61,  [D loss: 0.688894, acc.: 58.00%] [G loss: 0.738614]\n",
      "epoch:7, iter:62,  [D loss: 0.681188, acc.: 63.00%] [G loss: 0.743773]\n",
      "epoch:7, iter:63,  [D loss: 0.696302, acc.: 47.00%] [G loss: 0.753807]\n",
      "epoch:7, iter:64,  [D loss: 0.683607, acc.: 58.00%] [G loss: 0.745265]\n",
      "epoch:7, iter:65,  [D loss: 0.671903, acc.: 64.00%] [G loss: 0.732951]\n",
      "epoch:7, iter:66,  [D loss: 0.688491, acc.: 51.00%] [G loss: 0.741622]\n",
      "epoch:7, iter:67,  [D loss: 0.687014, acc.: 58.00%] [G loss: 0.742377]\n",
      "epoch:7, iter:68,  [D loss: 0.686088, acc.: 54.00%] [G loss: 0.739979]\n",
      "epoch:7, iter:69,  [D loss: 0.693299, acc.: 52.00%] [G loss: 0.734405]\n",
      "epoch:7, iter:70,  [D loss: 0.682120, acc.: 55.00%] [G loss: 0.730948]\n",
      "epoch:7, iter:71,  [D loss: 0.674196, acc.: 68.00%] [G loss: 0.729104]\n",
      "epoch:7, iter:72,  [D loss: 0.680097, acc.: 56.00%] [G loss: 0.726041]\n",
      "epoch:7, iter:73,  [D loss: 0.682372, acc.: 56.00%] [G loss: 0.717561]\n",
      "epoch:7, iter:74,  [D loss: 0.678365, acc.: 64.00%] [G loss: 0.713329]\n",
      "epoch:7, iter:75,  [D loss: 0.691397, acc.: 53.00%] [G loss: 0.715656]\n",
      "epoch:7, iter:76,  [D loss: 0.684626, acc.: 57.00%] [G loss: 0.717800]\n",
      "epoch:7, iter:77,  [D loss: 0.688963, acc.: 55.00%] [G loss: 0.731632]\n",
      "epoch:7, iter:78,  [D loss: 0.682250, acc.: 56.00%] [G loss: 0.738482]\n",
      "epoch:7, iter:79,  [D loss: 0.680907, acc.: 58.00%] [G loss: 0.740516]\n",
      "epoch:7, iter:80,  [D loss: 0.676926, acc.: 63.00%] [G loss: 0.743637]\n",
      "epoch:7, iter:81,  [D loss: 0.682500, acc.: 56.00%] [G loss: 0.746424]\n",
      "epoch:7, iter:82,  [D loss: 0.685402, acc.: 53.00%] [G loss: 0.734084]\n",
      "epoch:7, iter:83,  [D loss: 0.692284, acc.: 51.00%] [G loss: 0.742526]\n",
      "epoch:7, iter:84,  [D loss: 0.682831, acc.: 58.00%] [G loss: 0.747925]\n",
      "epoch:7, iter:85,  [D loss: 0.672475, acc.: 64.00%] [G loss: 0.750901]\n",
      "epoch:7, iter:86,  [D loss: 0.690960, acc.: 52.00%] [G loss: 0.738307]\n",
      "epoch:7, iter:87,  [D loss: 0.670350, acc.: 72.00%] [G loss: 0.745165]\n",
      "epoch:7, iter:88,  [D loss: 0.683505, acc.: 53.00%] [G loss: 0.735238]\n",
      "epoch:7, iter:89,  [D loss: 0.672217, acc.: 58.00%] [G loss: 0.720691]\n",
      "epoch:7, iter:90,  [D loss: 0.687093, acc.: 57.00%] [G loss: 0.724509]\n",
      "epoch:7, iter:91,  [D loss: 0.690651, acc.: 53.00%] [G loss: 0.725362]\n",
      "epoch:7, iter:92,  [D loss: 0.687727, acc.: 54.00%] [G loss: 0.711033]\n",
      "epoch:7, iter:93,  [D loss: 0.681953, acc.: 58.00%] [G loss: 0.710586]\n",
      "epoch:7, iter:94,  [D loss: 0.689158, acc.: 54.00%] [G loss: 0.724135]\n",
      "epoch:7, iter:95,  [D loss: 0.679626, acc.: 58.00%] [G loss: 0.712972]\n",
      "epoch:7, iter:96,  [D loss: 0.683367, acc.: 60.00%] [G loss: 0.723307]\n",
      "epoch:7, iter:97,  [D loss: 0.705243, acc.: 52.00%] [G loss: 0.726379]\n",
      "epoch:7, iter:98,  [D loss: 0.684131, acc.: 59.00%] [G loss: 0.730643]\n",
      "epoch:7, iter:99,  [D loss: 0.686987, acc.: 56.00%] [G loss: 0.740813]\n",
      "epoch:8, iter:0,  [D loss: 0.689502, acc.: 52.00%] [G loss: 0.724003]\n",
      "epoch:8, iter:1,  [D loss: 0.681017, acc.: 54.00%] [G loss: 0.728737]\n",
      "epoch:8, iter:2,  [D loss: 0.696640, acc.: 47.00%] [G loss: 0.707511]\n",
      "epoch:8, iter:3,  [D loss: 0.680791, acc.: 63.00%] [G loss: 0.713914]\n",
      "epoch:8, iter:4,  [D loss: 0.694429, acc.: 53.00%] [G loss: 0.727484]\n",
      "epoch:8, iter:5,  [D loss: 0.682220, acc.: 57.00%] [G loss: 0.733659]\n",
      "epoch:8, iter:6,  [D loss: 0.685336, acc.: 52.00%] [G loss: 0.714563]\n",
      "epoch:8, iter:7,  [D loss: 0.680876, acc.: 59.00%] [G loss: 0.707020]\n",
      "epoch:8, iter:8,  [D loss: 0.690668, acc.: 52.00%] [G loss: 0.713593]\n",
      "epoch:8, iter:9,  [D loss: 0.687382, acc.: 52.00%] [G loss: 0.727218]\n",
      "epoch:8, iter:10,  [D loss: 0.688374, acc.: 48.00%] [G loss: 0.717617]\n",
      "epoch:8, iter:11,  [D loss: 0.681914, acc.: 56.00%] [G loss: 0.708611]\n",
      "epoch:8, iter:12,  [D loss: 0.688473, acc.: 56.00%] [G loss: 0.730291]\n",
      "epoch:8, iter:13,  [D loss: 0.697099, acc.: 50.00%] [G loss: 0.723026]\n",
      "epoch:8, iter:14,  [D loss: 0.684085, acc.: 54.00%] [G loss: 0.726549]\n",
      "epoch:8, iter:15,  [D loss: 0.679223, acc.: 57.00%] [G loss: 0.737748]\n",
      "epoch:8, iter:16,  [D loss: 0.677508, acc.: 64.00%] [G loss: 0.743798]\n",
      "epoch:8, iter:17,  [D loss: 0.682680, acc.: 56.00%] [G loss: 0.751809]\n",
      "epoch:8, iter:18,  [D loss: 0.690172, acc.: 59.00%] [G loss: 0.749540]\n",
      "epoch:8, iter:19,  [D loss: 0.691081, acc.: 52.00%] [G loss: 0.738679]\n",
      "epoch:8, iter:20,  [D loss: 0.687822, acc.: 57.00%] [G loss: 0.742902]\n",
      "epoch:8, iter:21,  [D loss: 0.678535, acc.: 62.00%] [G loss: 0.736836]\n",
      "epoch:8, iter:22,  [D loss: 0.687695, acc.: 48.00%] [G loss: 0.747211]\n",
      "epoch:8, iter:23,  [D loss: 0.687238, acc.: 54.00%] [G loss: 0.735952]\n",
      "epoch:8, iter:24,  [D loss: 0.691635, acc.: 51.00%] [G loss: 0.735961]\n",
      "epoch:8, iter:25,  [D loss: 0.689263, acc.: 54.00%] [G loss: 0.727636]\n",
      "epoch:8, iter:26,  [D loss: 0.684229, acc.: 57.00%] [G loss: 0.723000]\n",
      "epoch:8, iter:27,  [D loss: 0.686148, acc.: 54.00%] [G loss: 0.729926]\n",
      "epoch:8, iter:28,  [D loss: 0.690149, acc.: 55.00%] [G loss: 0.719555]\n",
      "epoch:8, iter:29,  [D loss: 0.685670, acc.: 56.00%] [G loss: 0.721925]\n",
      "epoch:8, iter:30,  [D loss: 0.688120, acc.: 56.00%] [G loss: 0.721897]\n",
      "epoch:8, iter:31,  [D loss: 0.675918, acc.: 59.00%] [G loss: 0.724813]\n",
      "epoch:8, iter:32,  [D loss: 0.691235, acc.: 51.00%] [G loss: 0.719746]\n",
      "epoch:8, iter:33,  [D loss: 0.676704, acc.: 58.00%] [G loss: 0.722379]\n",
      "epoch:8, iter:34,  [D loss: 0.690384, acc.: 57.00%] [G loss: 0.736103]\n",
      "epoch:8, iter:35,  [D loss: 0.699784, acc.: 45.00%] [G loss: 0.720086]\n",
      "epoch:8, iter:36,  [D loss: 0.689126, acc.: 54.00%] [G loss: 0.718186]\n",
      "epoch:8, iter:37,  [D loss: 0.691669, acc.: 55.00%] [G loss: 0.724875]\n",
      "epoch:8, iter:38,  [D loss: 0.687142, acc.: 46.00%] [G loss: 0.721555]\n",
      "epoch:8, iter:39,  [D loss: 0.675243, acc.: 67.00%] [G loss: 0.729455]\n",
      "epoch:8, iter:40,  [D loss: 0.686426, acc.: 53.00%] [G loss: 0.720367]\n",
      "epoch:8, iter:41,  [D loss: 0.684168, acc.: 58.00%] [G loss: 0.731018]\n",
      "epoch:8, iter:42,  [D loss: 0.685705, acc.: 58.00%] [G loss: 0.725247]\n",
      "epoch:8, iter:43,  [D loss: 0.687769, acc.: 55.00%] [G loss: 0.724162]\n",
      "epoch:8, iter:44,  [D loss: 0.695563, acc.: 49.00%] [G loss: 0.718598]\n",
      "epoch:8, iter:45,  [D loss: 0.684147, acc.: 60.00%] [G loss: 0.724994]\n",
      "epoch:8, iter:46,  [D loss: 0.686858, acc.: 55.00%] [G loss: 0.730814]\n",
      "epoch:8, iter:47,  [D loss: 0.676039, acc.: 56.00%] [G loss: 0.715608]\n",
      "epoch:8, iter:48,  [D loss: 0.678526, acc.: 62.00%] [G loss: 0.704407]\n",
      "epoch:8, iter:49,  [D loss: 0.710348, acc.: 43.00%] [G loss: 0.700483]\n",
      "epoch:8, iter:50,  [D loss: 0.679602, acc.: 58.00%] [G loss: 0.706743]\n",
      "epoch:8, iter:51,  [D loss: 0.674986, acc.: 63.00%] [G loss: 0.714446]\n",
      "epoch:8, iter:52,  [D loss: 0.677225, acc.: 60.00%] [G loss: 0.722487]\n",
      "epoch:8, iter:53,  [D loss: 0.690391, acc.: 50.00%] [G loss: 0.730964]\n",
      "epoch:8, iter:54,  [D loss: 0.680767, acc.: 58.00%] [G loss: 0.719809]\n",
      "epoch:8, iter:55,  [D loss: 0.679438, acc.: 57.00%] [G loss: 0.733771]\n",
      "epoch:8, iter:56,  [D loss: 0.690214, acc.: 54.00%] [G loss: 0.729246]\n",
      "epoch:8, iter:57,  [D loss: 0.683120, acc.: 58.00%] [G loss: 0.716589]\n",
      "epoch:8, iter:58,  [D loss: 0.703841, acc.: 43.00%] [G loss: 0.730298]\n",
      "epoch:8, iter:59,  [D loss: 0.681115, acc.: 58.00%] [G loss: 0.726597]\n",
      "epoch:8, iter:60,  [D loss: 0.691255, acc.: 55.00%] [G loss: 0.729171]\n",
      "epoch:8, iter:61,  [D loss: 0.673421, acc.: 60.00%] [G loss: 0.733182]\n",
      "epoch:8, iter:62,  [D loss: 0.687868, acc.: 56.00%] [G loss: 0.727939]\n",
      "epoch:8, iter:63,  [D loss: 0.679208, acc.: 56.00%] [G loss: 0.737971]\n",
      "epoch:8, iter:64,  [D loss: 0.677560, acc.: 60.00%] [G loss: 0.727436]\n",
      "epoch:8, iter:65,  [D loss: 0.683196, acc.: 55.00%] [G loss: 0.724416]\n",
      "epoch:8, iter:66,  [D loss: 0.702413, acc.: 46.00%] [G loss: 0.730042]\n",
      "epoch:8, iter:67,  [D loss: 0.685293, acc.: 56.00%] [G loss: 0.730863]\n",
      "epoch:8, iter:68,  [D loss: 0.688036, acc.: 56.00%] [G loss: 0.724284]\n",
      "epoch:8, iter:69,  [D loss: 0.690531, acc.: 51.00%] [G loss: 0.721007]\n",
      "epoch:8, iter:70,  [D loss: 0.686426, acc.: 54.00%] [G loss: 0.732630]\n",
      "epoch:8, iter:71,  [D loss: 0.677410, acc.: 65.00%] [G loss: 0.720239]\n",
      "epoch:8, iter:72,  [D loss: 0.685922, acc.: 56.00%] [G loss: 0.747747]\n",
      "epoch:8, iter:73,  [D loss: 0.679773, acc.: 59.00%] [G loss: 0.732166]\n",
      "epoch:8, iter:74,  [D loss: 0.689371, acc.: 47.00%] [G loss: 0.728617]\n",
      "epoch:8, iter:75,  [D loss: 0.679288, acc.: 61.00%] [G loss: 0.745942]\n",
      "epoch:8, iter:76,  [D loss: 0.693950, acc.: 51.00%] [G loss: 0.730844]\n",
      "epoch:8, iter:77,  [D loss: 0.690532, acc.: 50.00%] [G loss: 0.729846]\n",
      "epoch:8, iter:78,  [D loss: 0.684738, acc.: 58.00%] [G loss: 0.724217]\n",
      "epoch:8, iter:79,  [D loss: 0.681174, acc.: 56.00%] [G loss: 0.728164]\n",
      "epoch:8, iter:80,  [D loss: 0.677576, acc.: 59.00%] [G loss: 0.733846]\n",
      "epoch:8, iter:81,  [D loss: 0.689229, acc.: 49.00%] [G loss: 0.735045]\n",
      "epoch:8, iter:82,  [D loss: 0.682360, acc.: 58.00%] [G loss: 0.738446]\n",
      "epoch:8, iter:83,  [D loss: 0.689884, acc.: 56.00%] [G loss: 0.715230]\n",
      "epoch:8, iter:84,  [D loss: 0.691503, acc.: 55.00%] [G loss: 0.732200]\n",
      "epoch:8, iter:85,  [D loss: 0.692113, acc.: 50.00%] [G loss: 0.725835]\n",
      "epoch:8, iter:86,  [D loss: 0.669604, acc.: 68.00%] [G loss: 0.720163]\n",
      "epoch:8, iter:87,  [D loss: 0.677727, acc.: 61.00%] [G loss: 0.720719]\n",
      "epoch:8, iter:88,  [D loss: 0.688273, acc.: 51.00%] [G loss: 0.728286]\n",
      "epoch:8, iter:89,  [D loss: 0.691019, acc.: 50.00%] [G loss: 0.718443]\n",
      "epoch:8, iter:90,  [D loss: 0.684355, acc.: 56.00%] [G loss: 0.719645]\n",
      "epoch:8, iter:91,  [D loss: 0.675946, acc.: 59.00%] [G loss: 0.731769]\n",
      "epoch:8, iter:92,  [D loss: 0.681977, acc.: 54.00%] [G loss: 0.725007]\n",
      "epoch:8, iter:93,  [D loss: 0.697180, acc.: 46.00%] [G loss: 0.719369]\n",
      "epoch:8, iter:94,  [D loss: 0.686405, acc.: 51.00%] [G loss: 0.712902]\n",
      "epoch:8, iter:95,  [D loss: 0.675266, acc.: 67.00%] [G loss: 0.727783]\n",
      "epoch:8, iter:96,  [D loss: 0.679139, acc.: 65.00%] [G loss: 0.722082]\n",
      "epoch:8, iter:97,  [D loss: 0.683988, acc.: 55.00%] [G loss: 0.726788]\n",
      "epoch:8, iter:98,  [D loss: 0.674812, acc.: 68.00%] [G loss: 0.730321]\n",
      "epoch:8, iter:99,  [D loss: 0.680523, acc.: 62.00%] [G loss: 0.715432]\n",
      "epoch:9, iter:0,  [D loss: 0.696601, acc.: 49.00%] [G loss: 0.715044]\n",
      "epoch:9, iter:1,  [D loss: 0.672563, acc.: 63.00%] [G loss: 0.717955]\n",
      "epoch:9, iter:2,  [D loss: 0.685874, acc.: 56.00%] [G loss: 0.716960]\n",
      "epoch:9, iter:3,  [D loss: 0.690615, acc.: 52.00%] [G loss: 0.719840]\n",
      "epoch:9, iter:4,  [D loss: 0.682258, acc.: 58.00%] [G loss: 0.725055]\n",
      "epoch:9, iter:5,  [D loss: 0.682694, acc.: 57.00%] [G loss: 0.718162]\n",
      "epoch:9, iter:6,  [D loss: 0.692415, acc.: 57.00%] [G loss: 0.721135]\n",
      "epoch:9, iter:7,  [D loss: 0.687889, acc.: 50.00%] [G loss: 0.717896]\n",
      "epoch:9, iter:8,  [D loss: 0.682828, acc.: 55.00%] [G loss: 0.719568]\n",
      "epoch:9, iter:9,  [D loss: 0.689274, acc.: 57.00%] [G loss: 0.726237]\n",
      "epoch:9, iter:10,  [D loss: 0.683705, acc.: 59.00%] [G loss: 0.732983]\n",
      "epoch:9, iter:11,  [D loss: 0.682109, acc.: 58.00%] [G loss: 0.722575]\n",
      "epoch:9, iter:12,  [D loss: 0.687719, acc.: 50.00%] [G loss: 0.731556]\n",
      "epoch:9, iter:13,  [D loss: 0.679760, acc.: 61.00%] [G loss: 0.734110]\n",
      "epoch:9, iter:14,  [D loss: 0.684515, acc.: 57.00%] [G loss: 0.719065]\n",
      "epoch:9, iter:15,  [D loss: 0.686315, acc.: 54.00%] [G loss: 0.731765]\n",
      "epoch:9, iter:16,  [D loss: 0.693690, acc.: 51.00%] [G loss: 0.734527]\n",
      "epoch:9, iter:17,  [D loss: 0.693647, acc.: 55.00%] [G loss: 0.721949]\n",
      "epoch:9, iter:18,  [D loss: 0.678214, acc.: 56.00%] [G loss: 0.734410]\n",
      "epoch:9, iter:19,  [D loss: 0.684802, acc.: 66.00%] [G loss: 0.728393]\n",
      "epoch:9, iter:20,  [D loss: 0.684362, acc.: 59.00%] [G loss: 0.727264]\n",
      "epoch:9, iter:21,  [D loss: 0.681619, acc.: 55.00%] [G loss: 0.727963]\n",
      "epoch:9, iter:22,  [D loss: 0.673686, acc.: 64.00%] [G loss: 0.716739]\n",
      "epoch:9, iter:23,  [D loss: 0.675412, acc.: 63.00%] [G loss: 0.730727]\n",
      "epoch:9, iter:24,  [D loss: 0.681585, acc.: 59.00%] [G loss: 0.709008]\n",
      "epoch:9, iter:25,  [D loss: 0.685141, acc.: 52.00%] [G loss: 0.712497]\n",
      "epoch:9, iter:26,  [D loss: 0.684985, acc.: 56.00%] [G loss: 0.720459]\n",
      "epoch:9, iter:27,  [D loss: 0.674869, acc.: 62.00%] [G loss: 0.717290]\n",
      "epoch:9, iter:28,  [D loss: 0.688190, acc.: 53.00%] [G loss: 0.724726]\n",
      "epoch:9, iter:29,  [D loss: 0.689525, acc.: 59.00%] [G loss: 0.730841]\n",
      "epoch:9, iter:30,  [D loss: 0.691532, acc.: 55.00%] [G loss: 0.742705]\n",
      "epoch:9, iter:31,  [D loss: 0.679266, acc.: 54.00%] [G loss: 0.738211]\n",
      "epoch:9, iter:32,  [D loss: 0.693078, acc.: 56.00%] [G loss: 0.710614]\n",
      "epoch:9, iter:33,  [D loss: 0.680058, acc.: 60.00%] [G loss: 0.726827]\n",
      "epoch:9, iter:34,  [D loss: 0.686492, acc.: 54.00%] [G loss: 0.730203]\n",
      "epoch:9, iter:35,  [D loss: 0.683652, acc.: 56.00%] [G loss: 0.733678]\n",
      "epoch:9, iter:36,  [D loss: 0.698070, acc.: 50.00%] [G loss: 0.735573]\n",
      "epoch:9, iter:37,  [D loss: 0.699132, acc.: 48.00%] [G loss: 0.739691]\n",
      "epoch:9, iter:38,  [D loss: 0.681737, acc.: 58.00%] [G loss: 0.737101]\n",
      "epoch:9, iter:39,  [D loss: 0.688566, acc.: 57.00%] [G loss: 0.736876]\n",
      "epoch:9, iter:40,  [D loss: 0.680976, acc.: 54.00%] [G loss: 0.732233]\n",
      "epoch:9, iter:41,  [D loss: 0.685886, acc.: 55.00%] [G loss: 0.736316]\n",
      "epoch:9, iter:42,  [D loss: 0.677139, acc.: 62.00%] [G loss: 0.743699]\n",
      "epoch:9, iter:43,  [D loss: 0.683226, acc.: 59.00%] [G loss: 0.732802]\n",
      "epoch:9, iter:44,  [D loss: 0.683412, acc.: 60.00%] [G loss: 0.718889]\n",
      "epoch:9, iter:45,  [D loss: 0.700618, acc.: 50.00%] [G loss: 0.715477]\n",
      "epoch:9, iter:46,  [D loss: 0.687867, acc.: 49.00%] [G loss: 0.721214]\n",
      "epoch:9, iter:47,  [D loss: 0.676095, acc.: 56.00%] [G loss: 0.736207]\n",
      "epoch:9, iter:48,  [D loss: 0.687455, acc.: 57.00%] [G loss: 0.721551]\n",
      "epoch:9, iter:49,  [D loss: 0.693053, acc.: 51.00%] [G loss: 0.722564]\n",
      "epoch:9, iter:50,  [D loss: 0.687176, acc.: 52.00%] [G loss: 0.732356]\n",
      "epoch:9, iter:51,  [D loss: 0.686141, acc.: 57.00%] [G loss: 0.715835]\n",
      "epoch:9, iter:52,  [D loss: 0.691231, acc.: 54.00%] [G loss: 0.724341]\n",
      "epoch:9, iter:53,  [D loss: 0.679561, acc.: 55.00%] [G loss: 0.717162]\n",
      "epoch:9, iter:54,  [D loss: 0.684444, acc.: 55.00%] [G loss: 0.725822]\n",
      "epoch:9, iter:55,  [D loss: 0.687977, acc.: 53.00%] [G loss: 0.722989]\n",
      "epoch:9, iter:56,  [D loss: 0.697811, acc.: 50.00%] [G loss: 0.721352]\n",
      "epoch:9, iter:57,  [D loss: 0.696069, acc.: 51.00%] [G loss: 0.726459]\n",
      "epoch:9, iter:58,  [D loss: 0.671176, acc.: 67.00%] [G loss: 0.728460]\n",
      "epoch:9, iter:59,  [D loss: 0.686847, acc.: 56.00%] [G loss: 0.730816]\n",
      "epoch:9, iter:60,  [D loss: 0.690367, acc.: 51.00%] [G loss: 0.726331]\n",
      "epoch:9, iter:61,  [D loss: 0.681016, acc.: 57.00%] [G loss: 0.721844]\n",
      "epoch:9, iter:62,  [D loss: 0.692061, acc.: 53.00%] [G loss: 0.723329]\n",
      "epoch:9, iter:63,  [D loss: 0.675147, acc.: 58.00%] [G loss: 0.717707]\n",
      "epoch:9, iter:64,  [D loss: 0.684979, acc.: 65.00%] [G loss: 0.725093]\n",
      "epoch:9, iter:65,  [D loss: 0.682547, acc.: 52.00%] [G loss: 0.723913]\n",
      "epoch:9, iter:66,  [D loss: 0.667491, acc.: 66.00%] [G loss: 0.729775]\n",
      "epoch:9, iter:67,  [D loss: 0.683612, acc.: 63.00%] [G loss: 0.731648]\n",
      "epoch:9, iter:68,  [D loss: 0.688566, acc.: 57.00%] [G loss: 0.733953]\n",
      "epoch:9, iter:69,  [D loss: 0.710771, acc.: 45.00%] [G loss: 0.729034]\n",
      "epoch:9, iter:70,  [D loss: 0.676585, acc.: 61.00%] [G loss: 0.711252]\n",
      "epoch:9, iter:71,  [D loss: 0.674095, acc.: 60.00%] [G loss: 0.727224]\n",
      "epoch:9, iter:72,  [D loss: 0.691280, acc.: 49.00%] [G loss: 0.723806]\n",
      "epoch:9, iter:73,  [D loss: 0.686659, acc.: 55.00%] [G loss: 0.740042]\n",
      "epoch:9, iter:74,  [D loss: 0.685048, acc.: 54.00%] [G loss: 0.720621]\n",
      "epoch:9, iter:75,  [D loss: 0.680862, acc.: 59.00%] [G loss: 0.715430]\n",
      "epoch:9, iter:76,  [D loss: 0.678401, acc.: 65.00%] [G loss: 0.722818]\n",
      "epoch:9, iter:77,  [D loss: 0.689658, acc.: 51.00%] [G loss: 0.721233]\n",
      "epoch:9, iter:78,  [D loss: 0.670024, acc.: 65.00%] [G loss: 0.735150]\n",
      "epoch:9, iter:79,  [D loss: 0.668797, acc.: 66.00%] [G loss: 0.722781]\n",
      "epoch:9, iter:80,  [D loss: 0.675056, acc.: 65.00%] [G loss: 0.723678]\n",
      "epoch:9, iter:81,  [D loss: 0.684393, acc.: 60.00%] [G loss: 0.732675]\n",
      "epoch:9, iter:82,  [D loss: 0.688448, acc.: 57.00%] [G loss: 0.733171]\n",
      "epoch:9, iter:83,  [D loss: 0.685369, acc.: 62.00%] [G loss: 0.726847]\n",
      "epoch:9, iter:84,  [D loss: 0.694726, acc.: 53.00%] [G loss: 0.731500]\n",
      "epoch:9, iter:85,  [D loss: 0.670204, acc.: 66.00%] [G loss: 0.736315]\n",
      "epoch:9, iter:86,  [D loss: 0.682335, acc.: 60.00%] [G loss: 0.719505]\n",
      "epoch:9, iter:87,  [D loss: 0.681609, acc.: 60.00%] [G loss: 0.726583]\n",
      "epoch:9, iter:88,  [D loss: 0.680860, acc.: 56.00%] [G loss: 0.723481]\n",
      "epoch:9, iter:89,  [D loss: 0.695775, acc.: 53.00%] [G loss: 0.730304]\n",
      "epoch:9, iter:90,  [D loss: 0.674115, acc.: 62.00%] [G loss: 0.731333]\n",
      "epoch:9, iter:91,  [D loss: 0.682260, acc.: 58.00%] [G loss: 0.728144]\n",
      "epoch:9, iter:92,  [D loss: 0.677746, acc.: 57.00%] [G loss: 0.722114]\n",
      "epoch:9, iter:93,  [D loss: 0.686612, acc.: 56.00%] [G loss: 0.709488]\n",
      "epoch:9, iter:94,  [D loss: 0.675106, acc.: 58.00%] [G loss: 0.702856]\n",
      "epoch:9, iter:95,  [D loss: 0.692603, acc.: 57.00%] [G loss: 0.715405]\n",
      "epoch:9, iter:96,  [D loss: 0.698836, acc.: 48.00%] [G loss: 0.718014]\n",
      "epoch:9, iter:97,  [D loss: 0.688205, acc.: 50.00%] [G loss: 0.716941]\n",
      "epoch:9, iter:98,  [D loss: 0.687746, acc.: 51.00%] [G loss: 0.725964]\n",
      "epoch:9, iter:99,  [D loss: 0.679369, acc.: 60.00%] [G loss: 0.738731]\n",
      "経過時間：81.58539390563965\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t1 = time.time() \n",
    "train(epochs=10, batch_size=100, save_interval=1)\n",
    "t2 = time.time()\n",
    "\n",
    "# 経過時間を表示\n",
    "elapsed_time = t2-t1\n",
    "print(f\"経過時間：{elapsed_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0kvRNEFWneVf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DCGAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
