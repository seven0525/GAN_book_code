{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANの実装\n",
    "- Google Colabratory上での実装を想定している点にご注意ください "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Googleドライブをマウント（生成画像をドライブに保存するため）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive \n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls drive/My\\ Drive/GAN_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3mfOf7fNvqhF"
   },
   "source": [
    "## 必要なライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_6J_Mj5JvcXw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "38Ee1vFY1Iyn"
   },
   "source": [
    "# GANの基本構造の構築\n",
    "- 入力画像の型定義\n",
    "- Discriminatorのモデル定義（それぞれの層、活性化関数など）、コンパイル\n",
    "- Generatorのモデル定義（それぞれの層、活性化関数など）、コンパイル（コンビネーションさせること注意）\n",
    "\n",
    "参考\n",
    "- Gneratorの活性化関数LeakyRELUについては[こちら](http://www.thothchildren.com/chapter/59b93f7575704408bd4300f2)　　\n",
    "- 最適化手法の種類については[こちら](https://qiita.com/tkazusa/items/4562cc7080105d5c78a9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "id": "feUPU871nQMS",
    "outputId": "46a3476c-0fdc-4b9b-c707-24269365c629"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1264: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,486,352\n",
      "Trainable params: 1,486,352\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Gモデルの構築関数\n",
    "def build_generator():\n",
    "    noise_shape = (z_dim,)\n",
    "    model = Sequential()\n",
    "\n",
    "    #入力層（入力の型はランダムノイズz、出力の型は256ノード、活性化関数はReLU）\n",
    "    model.add(Dense(256, input_shape=noise_shape, activation='relu'))\n",
    "\n",
    "    #中間層（入力の型は256ノード、出力の型は512ノード、活性化関数はReLU）\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "\n",
    "    #中間層（入力の型は512ノード、出力の型は1024ノード、活性化関数はReLU）\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "\n",
    "    #中間層（入力の型は1024ノード、出力の型は784（28×28×1）ノード、活性化関数はtanh）\n",
    "    model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
    "\n",
    "    #出力層（28×28×1の画像として出力）\n",
    "    model.add(Reshape(img_shape))\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "#Dモデルの構築関数\n",
    "def build_discriminator():\n",
    "    img_shape = (img_rows, img_cols, channels)\n",
    "    model = Sequential()\n",
    "\n",
    "    #入力層（入力の型は画像、出力の型は512ノード、活性化関数はReLU）\n",
    "    model.add(Flatten(input_shape=img_shape))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "\n",
    "    #中間層（入力の型は512ノード、出力の型は256ノード、活性化関数はReLU）\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "\n",
    "    #出力層（シグモイド関数で0〜1の値として出力）\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "#GモデルとDモデルを合併させる関数（Dモデルの学習は停止）\n",
    "def build_combined1():\n",
    "    discriminator.trainable = False\n",
    "    model = Sequential([generator, discriminator])\n",
    "    return model\n",
    "\n",
    "\n",
    "#入力画像（mnistデータ）の型\n",
    "img_rows = 28 \n",
    "img_cols = 28\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "\n",
    "#潜在変数の次元数 \n",
    "z_dim = 100\n",
    "\n",
    "#最適化手法（Adamを使用（学習率、浮動小数点数を引数））\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "#Dモデル構築(欠損関数、最適化手法、評価関数)\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
    "\n",
    "#Gモデル構築(欠損関数、最適化手法、評価関数)\n",
    "generator = build_generator()\n",
    "combined = build_combined1() #Dモデルとのコンビネーションネットワークとして最適化させる\n",
    "combined.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RYYQiZ538M8r"
   },
   "source": [
    "# 学習部分\n",
    "- MNISTの画像データ読み込み\n",
    "- 訓練データの画像と、Gモデルの生成した画像を用意\n",
    "- Dモデル：訓練データは「１」と認識し、生成データは「０」と認識するように、損失関数を元にそれぞれ学習\n",
    "- Gモデル：生成データをDモデルが「１」と認識するように、損失関数を元に学習\n",
    "\n",
    "参考\n",
    "- [欠損値関数とは？](損失関数を元に学習)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9l3PlupsnYCA"
   },
   "outputs": [],
   "source": [
    "def train(epochs, batch_size=128, save_interval=50): \n",
    "    # 教師データ（mnist）の画像を読み込み\n",
    "    (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "    # 画像のそれぞれの値を-1〜1に規格化\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "    X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "    half_batch = int(batch_size / 2)\n",
    "\n",
    "    # batch_size × epochs回繰り返す\n",
    "    for epoch in range(epochs):\n",
    "        for iteration in range(batch_size):\n",
    "            # バッチサイズの半数をGeneratorから生成\n",
    "            noise = np.random.normal(0, 1, (half_batch, z_dim))\n",
    "            gen_imgs = generator.predict(noise)\n",
    "\n",
    "            # バッチサイズの半数を教師データからピックアップ\n",
    "            idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # ---------------------\n",
    "            #  Discriminatorの学習\n",
    "            # ---------------------\n",
    "            # 訓練データを「1」と認識するよう、欠損値関数を元に学習\n",
    "            d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "\n",
    "            # 生成データを「0」と認識するよう、欠損値関数を元に学習\n",
    "            d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "\n",
    "            # 欠損値の平均を算出\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Generatorの学習\n",
    "            # ---------------------\n",
    "            noise = np.random.normal(0, 1, (batch_size, z_dim))\n",
    "            valid_y = np.array([1] * batch_size)\n",
    "\n",
    "            # 生成データをDモデルが「１」と認識するように、損失関数を元に学習\n",
    "            g_loss = combined.train_on_batch(noise, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成画像の保存\n",
    "- 保存先の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2TGv_G3QnaZj"
   },
   "outputs": [],
   "source": [
    "def save_imgs(epoch):\n",
    "    # 生成画像を敷き詰めるときの行数、列数\n",
    "    r, c = 5, 5\n",
    "\n",
    "    noise = np.random.normal(0, 1, (r * c, z_dim))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    # 生成画像を0-1に再スケール\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"drive/My Drive/GAN_images/mnist_%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z-W8uEZcTqek"
   },
   "source": [
    "# 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZICeK90KncvW",
    "outputId": "6b949c01-30bc-44ad-e85d-54394f94d26f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 100\n",
      "epoch:0, iter:0,  [D loss: 0.413749, acc.: 72.00%] [G loss: 0.849203]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, iter:1,  [D loss: 0.333665, acc.: 81.00%] [G loss: 0.925321]\n",
      "epoch:0, iter:2,  [D loss: 0.311607, acc.: 83.00%] [G loss: 1.020845]\n",
      "epoch:0, iter:3,  [D loss: 0.247104, acc.: 96.00%] [G loss: 1.134141]\n",
      "epoch:0, iter:4,  [D loss: 0.231100, acc.: 94.00%] [G loss: 1.207508]\n",
      "epoch:0, iter:5,  [D loss: 0.216186, acc.: 98.00%] [G loss: 1.294431]\n",
      "epoch:0, iter:6,  [D loss: 0.188102, acc.: 100.00%] [G loss: 1.429263]\n",
      "epoch:0, iter:7,  [D loss: 0.172220, acc.: 100.00%] [G loss: 1.525172]\n",
      "epoch:0, iter:8,  [D loss: 0.161006, acc.: 100.00%] [G loss: 1.555645]\n",
      "epoch:0, iter:9,  [D loss: 0.147624, acc.: 99.00%] [G loss: 1.641025]\n",
      "epoch:0, iter:10,  [D loss: 0.125126, acc.: 100.00%] [G loss: 1.751947]\n",
      "epoch:0, iter:11,  [D loss: 0.119357, acc.: 100.00%] [G loss: 1.789836]\n",
      "epoch:0, iter:12,  [D loss: 0.105801, acc.: 100.00%] [G loss: 1.845316]\n",
      "epoch:0, iter:13,  [D loss: 0.104781, acc.: 100.00%] [G loss: 1.968103]\n",
      "epoch:0, iter:14,  [D loss: 0.098580, acc.: 100.00%] [G loss: 2.080174]\n",
      "epoch:0, iter:15,  [D loss: 0.087840, acc.: 100.00%] [G loss: 2.014374]\n",
      "epoch:0, iter:16,  [D loss: 0.074544, acc.: 100.00%] [G loss: 2.093660]\n",
      "epoch:0, iter:17,  [D loss: 0.080463, acc.: 100.00%] [G loss: 2.231422]\n",
      "epoch:0, iter:18,  [D loss: 0.079877, acc.: 100.00%] [G loss: 2.236709]\n",
      "epoch:0, iter:19,  [D loss: 0.066672, acc.: 100.00%] [G loss: 2.361988]\n",
      "epoch:0, iter:20,  [D loss: 0.070078, acc.: 100.00%] [G loss: 2.426710]\n",
      "epoch:0, iter:21,  [D loss: 0.063364, acc.: 100.00%] [G loss: 2.487321]\n",
      "epoch:0, iter:22,  [D loss: 0.056569, acc.: 100.00%] [G loss: 2.498466]\n",
      "epoch:0, iter:23,  [D loss: 0.056731, acc.: 100.00%] [G loss: 2.547936]\n",
      "epoch:0, iter:24,  [D loss: 0.048028, acc.: 100.00%] [G loss: 2.646847]\n",
      "epoch:0, iter:25,  [D loss: 0.041403, acc.: 100.00%] [G loss: 2.722309]\n",
      "epoch:0, iter:26,  [D loss: 0.044891, acc.: 100.00%] [G loss: 2.736586]\n",
      "epoch:0, iter:27,  [D loss: 0.048001, acc.: 100.00%] [G loss: 2.780366]\n",
      "epoch:0, iter:28,  [D loss: 0.039472, acc.: 100.00%] [G loss: 2.819751]\n",
      "epoch:0, iter:29,  [D loss: 0.044156, acc.: 100.00%] [G loss: 2.909777]\n",
      "epoch:0, iter:30,  [D loss: 0.040382, acc.: 100.00%] [G loss: 2.884855]\n",
      "epoch:0, iter:31,  [D loss: 0.035909, acc.: 100.00%] [G loss: 2.920952]\n",
      "epoch:0, iter:32,  [D loss: 0.036923, acc.: 100.00%] [G loss: 2.962798]\n",
      "epoch:0, iter:33,  [D loss: 0.036043, acc.: 100.00%] [G loss: 2.927529]\n",
      "epoch:0, iter:34,  [D loss: 0.038097, acc.: 100.00%] [G loss: 3.056593]\n",
      "epoch:0, iter:35,  [D loss: 0.036042, acc.: 100.00%] [G loss: 3.089710]\n",
      "epoch:0, iter:36,  [D loss: 0.028952, acc.: 100.00%] [G loss: 3.073929]\n",
      "epoch:0, iter:37,  [D loss: 0.039883, acc.: 100.00%] [G loss: 3.253905]\n",
      "epoch:0, iter:38,  [D loss: 0.034185, acc.: 100.00%] [G loss: 3.229972]\n",
      "epoch:0, iter:39,  [D loss: 0.032443, acc.: 100.00%] [G loss: 3.270024]\n",
      "epoch:0, iter:40,  [D loss: 0.032794, acc.: 100.00%] [G loss: 3.330534]\n",
      "epoch:0, iter:41,  [D loss: 0.039119, acc.: 100.00%] [G loss: 3.374950]\n",
      "epoch:0, iter:42,  [D loss: 0.033737, acc.: 100.00%] [G loss: 3.432462]\n",
      "epoch:0, iter:43,  [D loss: 0.041158, acc.: 100.00%] [G loss: 3.419065]\n",
      "epoch:0, iter:44,  [D loss: 0.031104, acc.: 100.00%] [G loss: 3.481009]\n",
      "epoch:0, iter:45,  [D loss: 0.021776, acc.: 100.00%] [G loss: 3.546502]\n",
      "epoch:0, iter:46,  [D loss: 0.028551, acc.: 100.00%] [G loss: 3.527597]\n",
      "epoch:0, iter:47,  [D loss: 0.033764, acc.: 100.00%] [G loss: 3.451155]\n",
      "epoch:0, iter:48,  [D loss: 0.032311, acc.: 100.00%] [G loss: 3.549784]\n",
      "epoch:0, iter:49,  [D loss: 0.021176, acc.: 100.00%] [G loss: 3.479272]\n",
      "epoch:0, iter:50,  [D loss: 0.026893, acc.: 100.00%] [G loss: 3.498051]\n",
      "epoch:0, iter:51,  [D loss: 0.029590, acc.: 100.00%] [G loss: 3.508510]\n",
      "epoch:0, iter:52,  [D loss: 0.027717, acc.: 100.00%] [G loss: 3.523847]\n",
      "epoch:0, iter:53,  [D loss: 0.036854, acc.: 100.00%] [G loss: 3.609051]\n",
      "epoch:0, iter:54,  [D loss: 0.023135, acc.: 100.00%] [G loss: 3.565961]\n",
      "epoch:0, iter:55,  [D loss: 0.028363, acc.: 100.00%] [G loss: 3.561388]\n",
      "epoch:0, iter:56,  [D loss: 0.029049, acc.: 100.00%] [G loss: 3.568850]\n",
      "epoch:0, iter:57,  [D loss: 0.035963, acc.: 100.00%] [G loss: 3.600891]\n",
      "epoch:0, iter:58,  [D loss: 0.027363, acc.: 100.00%] [G loss: 3.611355]\n",
      "epoch:0, iter:59,  [D loss: 0.030563, acc.: 100.00%] [G loss: 3.671292]\n",
      "epoch:0, iter:60,  [D loss: 0.037349, acc.: 100.00%] [G loss: 3.796301]\n",
      "epoch:0, iter:61,  [D loss: 0.025888, acc.: 100.00%] [G loss: 3.722700]\n",
      "epoch:0, iter:62,  [D loss: 0.044372, acc.: 100.00%] [G loss: 3.618490]\n",
      "epoch:0, iter:63,  [D loss: 0.028829, acc.: 100.00%] [G loss: 3.759200]\n",
      "epoch:0, iter:64,  [D loss: 0.028814, acc.: 100.00%] [G loss: 3.700627]\n",
      "epoch:0, iter:65,  [D loss: 0.032720, acc.: 100.00%] [G loss: 3.696056]\n",
      "epoch:0, iter:66,  [D loss: 0.032824, acc.: 100.00%] [G loss: 3.575193]\n",
      "epoch:0, iter:67,  [D loss: 0.037747, acc.: 100.00%] [G loss: 3.779977]\n",
      "epoch:0, iter:68,  [D loss: 0.037097, acc.: 100.00%] [G loss: 3.914514]\n",
      "epoch:0, iter:69,  [D loss: 0.061093, acc.: 100.00%] [G loss: 4.009934]\n",
      "epoch:0, iter:70,  [D loss: 0.038267, acc.: 100.00%] [G loss: 4.051774]\n",
      "epoch:0, iter:71,  [D loss: 0.035179, acc.: 99.00%] [G loss: 3.696653]\n",
      "epoch:0, iter:72,  [D loss: 0.053982, acc.: 100.00%] [G loss: 4.038913]\n",
      "epoch:0, iter:73,  [D loss: 0.041548, acc.: 100.00%] [G loss: 3.796232]\n",
      "epoch:0, iter:74,  [D loss: 0.047958, acc.: 98.00%] [G loss: 3.807769]\n",
      "epoch:0, iter:75,  [D loss: 0.035343, acc.: 100.00%] [G loss: 3.768201]\n",
      "epoch:0, iter:76,  [D loss: 0.041460, acc.: 100.00%] [G loss: 3.802607]\n",
      "epoch:0, iter:77,  [D loss: 0.077032, acc.: 99.00%] [G loss: 4.020088]\n",
      "epoch:0, iter:78,  [D loss: 0.063850, acc.: 99.00%] [G loss: 3.720556]\n",
      "epoch:0, iter:79,  [D loss: 0.035832, acc.: 100.00%] [G loss: 3.730328]\n",
      "epoch:0, iter:80,  [D loss: 0.059139, acc.: 99.00%] [G loss: 3.871224]\n",
      "epoch:0, iter:81,  [D loss: 0.060571, acc.: 99.00%] [G loss: 3.593192]\n",
      "epoch:0, iter:82,  [D loss: 0.031808, acc.: 100.00%] [G loss: 3.642363]\n",
      "epoch:0, iter:83,  [D loss: 0.056902, acc.: 99.00%] [G loss: 3.738926]\n",
      "epoch:0, iter:84,  [D loss: 0.086322, acc.: 99.00%] [G loss: 3.710361]\n",
      "epoch:0, iter:85,  [D loss: 0.097246, acc.: 99.00%] [G loss: 4.112805]\n",
      "epoch:0, iter:86,  [D loss: 0.565408, acc.: 82.00%] [G loss: 3.351876]\n",
      "epoch:0, iter:87,  [D loss: 0.061562, acc.: 98.00%] [G loss: 3.740181]\n",
      "epoch:0, iter:88,  [D loss: 0.042915, acc.: 100.00%] [G loss: 3.759609]\n",
      "epoch:0, iter:89,  [D loss: 0.051596, acc.: 99.00%] [G loss: 3.993828]\n",
      "epoch:0, iter:90,  [D loss: 0.070631, acc.: 100.00%] [G loss: 3.659614]\n",
      "epoch:0, iter:91,  [D loss: 0.101076, acc.: 98.00%] [G loss: 3.529827]\n",
      "epoch:0, iter:92,  [D loss: 0.108917, acc.: 97.00%] [G loss: 3.503793]\n",
      "epoch:0, iter:93,  [D loss: 0.070210, acc.: 100.00%] [G loss: 3.625245]\n",
      "epoch:0, iter:94,  [D loss: 0.307315, acc.: 85.00%] [G loss: 3.559923]\n",
      "epoch:0, iter:95,  [D loss: 0.111953, acc.: 97.00%] [G loss: 3.762551]\n",
      "epoch:0, iter:96,  [D loss: 0.183144, acc.: 92.00%] [G loss: 2.896547]\n",
      "epoch:0, iter:97,  [D loss: 0.146570, acc.: 94.00%] [G loss: 3.632978]\n",
      "epoch:0, iter:98,  [D loss: 0.049027, acc.: 100.00%] [G loss: 3.816114]\n",
      "epoch:0, iter:99,  [D loss: 0.146985, acc.: 95.00%] [G loss: 3.467589]\n",
      "epoch:1, iter:0,  [D loss: 0.146996, acc.: 93.00%] [G loss: 3.763227]\n",
      "epoch:1, iter:1,  [D loss: 0.179568, acc.: 93.00%] [G loss: 3.507922]\n",
      "epoch:1, iter:2,  [D loss: 0.056620, acc.: 100.00%] [G loss: 3.748666]\n",
      "epoch:1, iter:3,  [D loss: 0.183202, acc.: 93.00%] [G loss: 3.220869]\n",
      "epoch:1, iter:4,  [D loss: 0.120237, acc.: 95.00%] [G loss: 3.944863]\n",
      "epoch:1, iter:5,  [D loss: 1.368804, acc.: 51.00%] [G loss: 2.573660]\n",
      "epoch:1, iter:6,  [D loss: 0.555818, acc.: 75.00%] [G loss: 2.266327]\n",
      "epoch:1, iter:7,  [D loss: 0.114662, acc.: 97.00%] [G loss: 2.994474]\n",
      "epoch:1, iter:8,  [D loss: 0.086152, acc.: 98.00%] [G loss: 3.258642]\n",
      "epoch:1, iter:9,  [D loss: 0.108686, acc.: 94.00%] [G loss: 3.258247]\n",
      "epoch:1, iter:10,  [D loss: 0.068241, acc.: 99.00%] [G loss: 3.650211]\n",
      "epoch:1, iter:11,  [D loss: 0.104520, acc.: 98.00%] [G loss: 3.165112]\n",
      "epoch:1, iter:12,  [D loss: 0.247890, acc.: 87.00%] [G loss: 2.569627]\n",
      "epoch:1, iter:13,  [D loss: 0.109072, acc.: 97.00%] [G loss: 2.929860]\n",
      "epoch:1, iter:14,  [D loss: 0.106896, acc.: 96.00%] [G loss: 2.866807]\n",
      "epoch:1, iter:15,  [D loss: 0.158802, acc.: 92.00%] [G loss: 2.786535]\n",
      "epoch:1, iter:16,  [D loss: 0.224357, acc.: 92.00%] [G loss: 2.727252]\n",
      "epoch:1, iter:17,  [D loss: 0.198016, acc.: 91.00%] [G loss: 3.013542]\n",
      "epoch:1, iter:18,  [D loss: 0.277244, acc.: 87.00%] [G loss: 2.759187]\n",
      "epoch:1, iter:19,  [D loss: 0.198034, acc.: 90.00%] [G loss: 3.410976]\n",
      "epoch:1, iter:20,  [D loss: 0.533300, acc.: 74.00%] [G loss: 2.193171]\n",
      "epoch:1, iter:21,  [D loss: 0.149930, acc.: 94.00%] [G loss: 3.197827]\n",
      "epoch:1, iter:22,  [D loss: 0.244638, acc.: 94.00%] [G loss: 3.150518]\n",
      "epoch:1, iter:23,  [D loss: 0.186018, acc.: 92.00%] [G loss: 3.125971]\n",
      "epoch:1, iter:24,  [D loss: 0.262914, acc.: 89.00%] [G loss: 2.809755]\n",
      "epoch:1, iter:25,  [D loss: 0.214780, acc.: 90.00%] [G loss: 3.181345]\n",
      "epoch:1, iter:26,  [D loss: 0.441062, acc.: 83.00%] [G loss: 2.441113]\n",
      "epoch:1, iter:27,  [D loss: 0.139315, acc.: 95.00%] [G loss: 3.029318]\n",
      "epoch:1, iter:28,  [D loss: 0.136466, acc.: 100.00%] [G loss: 2.687165]\n",
      "epoch:1, iter:29,  [D loss: 0.225736, acc.: 91.00%] [G loss: 3.003963]\n",
      "epoch:1, iter:30,  [D loss: 0.408512, acc.: 80.00%] [G loss: 2.624859]\n",
      "epoch:1, iter:31,  [D loss: 0.222084, acc.: 88.00%] [G loss: 3.084810]\n",
      "epoch:1, iter:32,  [D loss: 0.571258, acc.: 78.00%] [G loss: 1.724051]\n",
      "epoch:1, iter:33,  [D loss: 0.318261, acc.: 84.00%] [G loss: 2.299844]\n",
      "epoch:1, iter:34,  [D loss: 0.074447, acc.: 99.00%] [G loss: 3.151316]\n",
      "epoch:1, iter:35,  [D loss: 0.173412, acc.: 94.00%] [G loss: 3.073481]\n",
      "epoch:1, iter:36,  [D loss: 0.392438, acc.: 79.00%] [G loss: 2.992516]\n",
      "epoch:1, iter:37,  [D loss: 0.209797, acc.: 93.00%] [G loss: 3.119530]\n",
      "epoch:1, iter:38,  [D loss: 0.611581, acc.: 71.00%] [G loss: 1.767817]\n",
      "epoch:1, iter:39,  [D loss: 0.347898, acc.: 79.00%] [G loss: 2.234883]\n",
      "epoch:1, iter:40,  [D loss: 0.142501, acc.: 94.00%] [G loss: 3.577501]\n",
      "epoch:1, iter:41,  [D loss: 0.156792, acc.: 98.00%] [G loss: 3.335927]\n",
      "epoch:1, iter:42,  [D loss: 0.241392, acc.: 89.00%] [G loss: 2.820779]\n",
      "epoch:1, iter:43,  [D loss: 0.309464, acc.: 81.00%] [G loss: 2.896591]\n",
      "epoch:1, iter:44,  [D loss: 0.501190, acc.: 77.00%] [G loss: 2.206098]\n",
      "epoch:1, iter:45,  [D loss: 0.170664, acc.: 96.00%] [G loss: 2.881276]\n",
      "epoch:1, iter:46,  [D loss: 0.504911, acc.: 79.00%] [G loss: 2.114961]\n",
      "epoch:1, iter:47,  [D loss: 0.178769, acc.: 92.00%] [G loss: 2.890968]\n",
      "epoch:1, iter:48,  [D loss: 0.176789, acc.: 95.00%] [G loss: 3.377632]\n",
      "epoch:1, iter:49,  [D loss: 0.544741, acc.: 71.00%] [G loss: 1.887158]\n",
      "epoch:1, iter:50,  [D loss: 0.252251, acc.: 86.00%] [G loss: 2.808734]\n",
      "epoch:1, iter:51,  [D loss: 0.114141, acc.: 100.00%] [G loss: 3.297089]\n",
      "epoch:1, iter:52,  [D loss: 0.471318, acc.: 80.00%] [G loss: 1.970023]\n",
      "epoch:1, iter:53,  [D loss: 0.216043, acc.: 88.00%] [G loss: 2.793859]\n",
      "epoch:1, iter:54,  [D loss: 0.197229, acc.: 96.00%] [G loss: 2.482516]\n",
      "epoch:1, iter:55,  [D loss: 0.227616, acc.: 91.00%] [G loss: 2.689654]\n",
      "epoch:1, iter:56,  [D loss: 0.269585, acc.: 89.00%] [G loss: 2.599225]\n",
      "epoch:1, iter:57,  [D loss: 0.308154, acc.: 86.00%] [G loss: 2.447958]\n",
      "epoch:1, iter:58,  [D loss: 0.418864, acc.: 81.00%] [G loss: 2.239739]\n",
      "epoch:1, iter:59,  [D loss: 0.223625, acc.: 91.00%] [G loss: 3.203846]\n",
      "epoch:1, iter:60,  [D loss: 0.595248, acc.: 78.00%] [G loss: 1.927200]\n",
      "epoch:1, iter:61,  [D loss: 0.350701, acc.: 79.00%] [G loss: 2.931188]\n",
      "epoch:1, iter:62,  [D loss: 0.148051, acc.: 97.00%] [G loss: 3.630226]\n",
      "epoch:1, iter:63,  [D loss: 0.457303, acc.: 80.00%] [G loss: 2.052177]\n",
      "epoch:1, iter:64,  [D loss: 0.254252, acc.: 85.00%] [G loss: 2.761821]\n",
      "epoch:1, iter:65,  [D loss: 0.321787, acc.: 89.00%] [G loss: 2.979329]\n",
      "epoch:1, iter:66,  [D loss: 0.360438, acc.: 83.00%] [G loss: 2.807063]\n",
      "epoch:1, iter:67,  [D loss: 0.319553, acc.: 87.00%] [G loss: 2.484935]\n",
      "epoch:1, iter:68,  [D loss: 0.294796, acc.: 83.00%] [G loss: 2.826896]\n",
      "epoch:1, iter:69,  [D loss: 0.469195, acc.: 79.00%] [G loss: 2.167578]\n",
      "epoch:1, iter:70,  [D loss: 0.302655, acc.: 82.00%] [G loss: 3.593582]\n",
      "epoch:1, iter:71,  [D loss: 1.017974, acc.: 44.00%] [G loss: 1.344621]\n",
      "epoch:1, iter:72,  [D loss: 0.531941, acc.: 70.00%] [G loss: 2.287213]\n",
      "epoch:1, iter:73,  [D loss: 0.159469, acc.: 96.00%] [G loss: 3.375477]\n",
      "epoch:1, iter:74,  [D loss: 0.546370, acc.: 70.00%] [G loss: 2.228359]\n",
      "epoch:1, iter:75,  [D loss: 0.239267, acc.: 88.00%] [G loss: 2.522156]\n",
      "epoch:1, iter:76,  [D loss: 0.243337, acc.: 92.00%] [G loss: 2.985557]\n",
      "epoch:1, iter:77,  [D loss: 0.511331, acc.: 71.00%] [G loss: 2.245988]\n",
      "epoch:1, iter:78,  [D loss: 0.318242, acc.: 80.00%] [G loss: 2.408553]\n",
      "epoch:1, iter:79,  [D loss: 0.478197, acc.: 76.00%] [G loss: 2.140408]\n",
      "epoch:1, iter:80,  [D loss: 0.287718, acc.: 84.00%] [G loss: 2.932932]\n",
      "epoch:1, iter:81,  [D loss: 0.435362, acc.: 79.00%] [G loss: 2.058923]\n",
      "epoch:1, iter:82,  [D loss: 0.285814, acc.: 86.00%] [G loss: 2.977380]\n",
      "epoch:1, iter:83,  [D loss: 0.656342, acc.: 71.00%] [G loss: 2.085944]\n",
      "epoch:1, iter:84,  [D loss: 0.234228, acc.: 87.00%] [G loss: 3.062101]\n",
      "epoch:1, iter:85,  [D loss: 0.575187, acc.: 67.00%] [G loss: 2.334174]\n",
      "epoch:1, iter:86,  [D loss: 0.303088, acc.: 87.00%] [G loss: 2.900150]\n",
      "epoch:1, iter:87,  [D loss: 0.840463, acc.: 48.00%] [G loss: 1.612164]\n",
      "epoch:1, iter:88,  [D loss: 0.301056, acc.: 84.00%] [G loss: 2.588788]\n",
      "epoch:1, iter:89,  [D loss: 0.484458, acc.: 73.00%] [G loss: 2.291126]\n",
      "epoch:1, iter:90,  [D loss: 0.305595, acc.: 89.00%] [G loss: 2.422220]\n",
      "epoch:1, iter:91,  [D loss: 0.460006, acc.: 78.00%] [G loss: 2.360381]\n",
      "epoch:1, iter:92,  [D loss: 0.393733, acc.: 81.00%] [G loss: 2.558859]\n",
      "epoch:1, iter:93,  [D loss: 0.527194, acc.: 78.00%] [G loss: 2.276172]\n",
      "epoch:1, iter:94,  [D loss: 0.443634, acc.: 74.00%] [G loss: 2.517842]\n",
      "epoch:1, iter:95,  [D loss: 0.763117, acc.: 60.00%] [G loss: 1.899309]\n",
      "epoch:1, iter:96,  [D loss: 0.347033, acc.: 83.00%] [G loss: 2.881076]\n",
      "epoch:1, iter:97,  [D loss: 0.712701, acc.: 59.00%] [G loss: 1.681520]\n",
      "epoch:1, iter:98,  [D loss: 0.309682, acc.: 83.00%] [G loss: 2.555813]\n",
      "epoch:1, iter:99,  [D loss: 0.597752, acc.: 67.00%] [G loss: 2.049062]\n",
      "epoch:2, iter:0,  [D loss: 0.364656, acc.: 83.00%] [G loss: 2.397757]\n",
      "epoch:2, iter:1,  [D loss: 0.444930, acc.: 75.00%] [G loss: 2.384044]\n",
      "epoch:2, iter:2,  [D loss: 0.439317, acc.: 79.00%] [G loss: 2.300722]\n",
      "epoch:2, iter:3,  [D loss: 0.458807, acc.: 75.00%] [G loss: 2.224332]\n",
      "epoch:2, iter:4,  [D loss: 0.370256, acc.: 83.00%] [G loss: 2.114972]\n",
      "epoch:2, iter:5,  [D loss: 0.450679, acc.: 75.00%] [G loss: 2.291279]\n",
      "epoch:2, iter:6,  [D loss: 0.539934, acc.: 72.00%] [G loss: 2.028915]\n",
      "epoch:2, iter:7,  [D loss: 0.496410, acc.: 72.00%] [G loss: 2.208242]\n",
      "epoch:2, iter:8,  [D loss: 0.508603, acc.: 74.00%] [G loss: 1.959614]\n",
      "epoch:2, iter:9,  [D loss: 0.393646, acc.: 79.00%] [G loss: 2.393559]\n",
      "epoch:2, iter:10,  [D loss: 0.589302, acc.: 67.00%] [G loss: 1.942981]\n",
      "epoch:2, iter:11,  [D loss: 0.459935, acc.: 77.00%] [G loss: 2.204087]\n",
      "epoch:2, iter:12,  [D loss: 0.569199, acc.: 68.00%] [G loss: 2.251130]\n",
      "epoch:2, iter:13,  [D loss: 0.484660, acc.: 76.00%] [G loss: 2.293768]\n",
      "epoch:2, iter:14,  [D loss: 0.519397, acc.: 76.00%] [G loss: 2.026397]\n",
      "epoch:2, iter:15,  [D loss: 0.432446, acc.: 82.00%] [G loss: 2.118562]\n",
      "epoch:2, iter:16,  [D loss: 0.452034, acc.: 77.00%] [G loss: 2.488226]\n",
      "epoch:2, iter:17,  [D loss: 0.425401, acc.: 79.00%] [G loss: 2.436988]\n",
      "epoch:2, iter:18,  [D loss: 0.521028, acc.: 73.00%] [G loss: 1.804990]\n",
      "epoch:2, iter:19,  [D loss: 0.428030, acc.: 76.00%] [G loss: 2.538635]\n",
      "epoch:2, iter:20,  [D loss: 0.720210, acc.: 57.00%] [G loss: 1.674340]\n",
      "epoch:2, iter:21,  [D loss: 0.415622, acc.: 78.00%] [G loss: 2.446383]\n",
      "epoch:2, iter:22,  [D loss: 0.672213, acc.: 61.00%] [G loss: 1.662429]\n",
      "epoch:2, iter:23,  [D loss: 0.433548, acc.: 74.00%] [G loss: 2.461967]\n",
      "epoch:2, iter:24,  [D loss: 0.949941, acc.: 42.00%] [G loss: 1.180748]\n",
      "epoch:2, iter:25,  [D loss: 0.429200, acc.: 75.00%] [G loss: 2.319447]\n",
      "epoch:2, iter:26,  [D loss: 0.558990, acc.: 68.00%] [G loss: 1.734656]\n",
      "epoch:2, iter:27,  [D loss: 0.551290, acc.: 72.00%] [G loss: 2.012699]\n",
      "epoch:2, iter:28,  [D loss: 0.477149, acc.: 77.00%] [G loss: 2.129714]\n",
      "epoch:2, iter:29,  [D loss: 0.527519, acc.: 74.00%] [G loss: 1.909627]\n",
      "epoch:2, iter:30,  [D loss: 0.533346, acc.: 74.00%] [G loss: 2.052888]\n",
      "epoch:2, iter:31,  [D loss: 0.507867, acc.: 76.00%] [G loss: 1.901154]\n",
      "epoch:2, iter:32,  [D loss: 0.594587, acc.: 67.00%] [G loss: 1.847372]\n",
      "epoch:2, iter:33,  [D loss: 0.461206, acc.: 78.00%] [G loss: 1.844999]\n",
      "epoch:2, iter:34,  [D loss: 0.580230, acc.: 72.00%] [G loss: 1.713851]\n",
      "epoch:2, iter:35,  [D loss: 0.552233, acc.: 71.00%] [G loss: 1.816767]\n",
      "epoch:2, iter:36,  [D loss: 0.592309, acc.: 66.00%] [G loss: 1.801912]\n",
      "epoch:2, iter:37,  [D loss: 0.486566, acc.: 71.00%] [G loss: 2.112404]\n",
      "epoch:2, iter:38,  [D loss: 0.828832, acc.: 45.00%] [G loss: 1.178295]\n",
      "epoch:2, iter:39,  [D loss: 0.533122, acc.: 63.00%] [G loss: 1.961238]\n",
      "epoch:2, iter:40,  [D loss: 0.697745, acc.: 54.00%] [G loss: 1.453642]\n",
      "epoch:2, iter:41,  [D loss: 0.557971, acc.: 67.00%] [G loss: 1.825675]\n",
      "epoch:2, iter:42,  [D loss: 0.694496, acc.: 61.00%] [G loss: 1.514452]\n",
      "epoch:2, iter:43,  [D loss: 0.570498, acc.: 66.00%] [G loss: 1.754361]\n",
      "epoch:2, iter:44,  [D loss: 0.510298, acc.: 75.00%] [G loss: 1.758485]\n",
      "epoch:2, iter:45,  [D loss: 0.694132, acc.: 60.00%] [G loss: 1.463975]\n",
      "epoch:2, iter:46,  [D loss: 0.588447, acc.: 70.00%] [G loss: 1.773262]\n",
      "epoch:2, iter:47,  [D loss: 0.673228, acc.: 56.00%] [G loss: 1.637068]\n",
      "epoch:2, iter:48,  [D loss: 0.677913, acc.: 62.00%] [G loss: 1.272244]\n",
      "epoch:2, iter:49,  [D loss: 0.566699, acc.: 70.00%] [G loss: 1.716164]\n",
      "epoch:2, iter:50,  [D loss: 0.747138, acc.: 57.00%] [G loss: 1.224753]\n",
      "epoch:2, iter:51,  [D loss: 0.546590, acc.: 70.00%] [G loss: 1.687291]\n",
      "epoch:2, iter:52,  [D loss: 0.687989, acc.: 58.00%] [G loss: 1.479267]\n",
      "epoch:2, iter:53,  [D loss: 0.628588, acc.: 62.00%] [G loss: 1.473448]\n",
      "epoch:2, iter:54,  [D loss: 0.663250, acc.: 60.00%] [G loss: 1.319053]\n",
      "epoch:2, iter:55,  [D loss: 0.646829, acc.: 61.00%] [G loss: 1.407412]\n",
      "epoch:2, iter:56,  [D loss: 0.539682, acc.: 72.00%] [G loss: 1.862828]\n",
      "epoch:2, iter:57,  [D loss: 0.735810, acc.: 53.00%] [G loss: 1.123217]\n",
      "epoch:2, iter:58,  [D loss: 0.526110, acc.: 72.00%] [G loss: 1.478543]\n",
      "epoch:2, iter:59,  [D loss: 0.662074, acc.: 62.00%] [G loss: 1.349666]\n",
      "epoch:2, iter:60,  [D loss: 0.593688, acc.: 68.00%] [G loss: 1.400998]\n",
      "epoch:2, iter:61,  [D loss: 0.661794, acc.: 63.00%] [G loss: 1.329875]\n",
      "epoch:2, iter:62,  [D loss: 0.714799, acc.: 52.00%] [G loss: 1.164202]\n",
      "epoch:2, iter:63,  [D loss: 0.671814, acc.: 61.00%] [G loss: 1.232503]\n",
      "epoch:2, iter:64,  [D loss: 0.656358, acc.: 57.00%] [G loss: 1.252809]\n",
      "epoch:2, iter:65,  [D loss: 0.783133, acc.: 42.00%] [G loss: 1.066271]\n",
      "epoch:2, iter:66,  [D loss: 0.660164, acc.: 55.00%] [G loss: 1.152838]\n",
      "epoch:2, iter:67,  [D loss: 0.745046, acc.: 42.00%] [G loss: 1.016458]\n",
      "epoch:2, iter:68,  [D loss: 0.705909, acc.: 47.00%] [G loss: 1.112381]\n",
      "epoch:2, iter:69,  [D loss: 0.738268, acc.: 48.00%] [G loss: 1.064613]\n",
      "epoch:2, iter:70,  [D loss: 0.735440, acc.: 49.00%] [G loss: 0.911028]\n",
      "epoch:2, iter:71,  [D loss: 0.723621, acc.: 48.00%] [G loss: 0.968896]\n",
      "epoch:2, iter:72,  [D loss: 0.811175, acc.: 41.00%] [G loss: 0.751464]\n",
      "epoch:2, iter:73,  [D loss: 0.710791, acc.: 46.00%] [G loss: 0.843013]\n",
      "epoch:2, iter:74,  [D loss: 0.760541, acc.: 41.00%] [G loss: 0.800333]\n",
      "epoch:2, iter:75,  [D loss: 0.703343, acc.: 45.00%] [G loss: 0.845151]\n",
      "epoch:2, iter:76,  [D loss: 0.785438, acc.: 38.00%] [G loss: 0.861702]\n",
      "epoch:2, iter:77,  [D loss: 0.729254, acc.: 43.00%] [G loss: 0.827821]\n",
      "epoch:2, iter:78,  [D loss: 0.720358, acc.: 40.00%] [G loss: 0.836435]\n",
      "epoch:2, iter:79,  [D loss: 0.713895, acc.: 47.00%] [G loss: 0.864338]\n",
      "epoch:2, iter:80,  [D loss: 0.695954, acc.: 49.00%] [G loss: 0.860950]\n",
      "epoch:2, iter:81,  [D loss: 0.678001, acc.: 55.00%] [G loss: 0.911546]\n",
      "epoch:2, iter:82,  [D loss: 0.715949, acc.: 51.00%] [G loss: 0.839060]\n",
      "epoch:2, iter:83,  [D loss: 0.681310, acc.: 54.00%] [G loss: 0.832379]\n",
      "epoch:2, iter:84,  [D loss: 0.733067, acc.: 42.00%] [G loss: 0.799799]\n",
      "epoch:2, iter:85,  [D loss: 0.695458, acc.: 49.00%] [G loss: 0.852693]\n",
      "epoch:2, iter:86,  [D loss: 0.722168, acc.: 43.00%] [G loss: 0.803256]\n",
      "epoch:2, iter:87,  [D loss: 0.700822, acc.: 43.00%] [G loss: 0.774642]\n",
      "epoch:2, iter:88,  [D loss: 0.745591, acc.: 40.00%] [G loss: 0.724633]\n",
      "epoch:2, iter:89,  [D loss: 0.725538, acc.: 38.00%] [G loss: 0.734761]\n",
      "epoch:2, iter:90,  [D loss: 0.675977, acc.: 55.00%] [G loss: 0.785364]\n",
      "epoch:2, iter:91,  [D loss: 0.776150, acc.: 42.00%] [G loss: 0.705415]\n",
      "epoch:2, iter:92,  [D loss: 0.748910, acc.: 38.00%] [G loss: 0.708745]\n",
      "epoch:2, iter:93,  [D loss: 0.702495, acc.: 45.00%] [G loss: 0.723732]\n",
      "epoch:2, iter:94,  [D loss: 0.734294, acc.: 41.00%] [G loss: 0.700490]\n",
      "epoch:2, iter:95,  [D loss: 0.694727, acc.: 45.00%] [G loss: 0.712016]\n",
      "epoch:2, iter:96,  [D loss: 0.695729, acc.: 47.00%] [G loss: 0.700643]\n",
      "epoch:2, iter:97,  [D loss: 0.699713, acc.: 43.00%] [G loss: 0.707514]\n",
      "epoch:2, iter:98,  [D loss: 0.727706, acc.: 43.00%] [G loss: 0.685736]\n",
      "epoch:2, iter:99,  [D loss: 0.680643, acc.: 46.00%] [G loss: 0.730197]\n",
      "epoch:3, iter:0,  [D loss: 0.702596, acc.: 40.00%] [G loss: 0.705808]\n",
      "epoch:3, iter:1,  [D loss: 0.688191, acc.: 51.00%] [G loss: 0.728444]\n",
      "epoch:3, iter:2,  [D loss: 0.719611, acc.: 43.00%] [G loss: 0.697809]\n",
      "epoch:3, iter:3,  [D loss: 0.689288, acc.: 44.00%] [G loss: 0.689898]\n",
      "epoch:3, iter:4,  [D loss: 0.738162, acc.: 41.00%] [G loss: 0.671254]\n",
      "epoch:3, iter:5,  [D loss: 0.689400, acc.: 45.00%] [G loss: 0.672837]\n",
      "epoch:3, iter:6,  [D loss: 0.686450, acc.: 48.00%] [G loss: 0.721641]\n",
      "epoch:3, iter:7,  [D loss: 0.710183, acc.: 43.00%] [G loss: 0.675298]\n",
      "epoch:3, iter:8,  [D loss: 0.719541, acc.: 41.00%] [G loss: 0.685131]\n",
      "epoch:3, iter:9,  [D loss: 0.698435, acc.: 46.00%] [G loss: 0.666586]\n",
      "epoch:3, iter:10,  [D loss: 0.690377, acc.: 48.00%] [G loss: 0.675281]\n",
      "epoch:3, iter:11,  [D loss: 0.683706, acc.: 46.00%] [G loss: 0.681953]\n",
      "epoch:3, iter:12,  [D loss: 0.676457, acc.: 48.00%] [G loss: 0.693304]\n",
      "epoch:3, iter:13,  [D loss: 0.664406, acc.: 48.00%] [G loss: 0.721229]\n",
      "epoch:3, iter:14,  [D loss: 0.718623, acc.: 37.00%] [G loss: 0.698955]\n",
      "epoch:3, iter:15,  [D loss: 0.693759, acc.: 44.00%] [G loss: 0.681596]\n",
      "epoch:3, iter:16,  [D loss: 0.692518, acc.: 47.00%] [G loss: 0.665328]\n",
      "epoch:3, iter:17,  [D loss: 0.705272, acc.: 43.00%] [G loss: 0.675306]\n",
      "epoch:3, iter:18,  [D loss: 0.666697, acc.: 45.00%] [G loss: 0.683197]\n",
      "epoch:3, iter:19,  [D loss: 0.687158, acc.: 46.00%] [G loss: 0.695331]\n",
      "epoch:3, iter:20,  [D loss: 0.696224, acc.: 45.00%] [G loss: 0.681353]\n",
      "epoch:3, iter:21,  [D loss: 0.666584, acc.: 50.00%] [G loss: 0.693913]\n",
      "epoch:3, iter:22,  [D loss: 0.700139, acc.: 43.00%] [G loss: 0.680871]\n",
      "epoch:3, iter:23,  [D loss: 0.682751, acc.: 50.00%] [G loss: 0.674206]\n",
      "epoch:3, iter:24,  [D loss: 0.709057, acc.: 43.00%] [G loss: 0.674274]\n",
      "epoch:3, iter:25,  [D loss: 0.688255, acc.: 46.00%] [G loss: 0.669412]\n",
      "epoch:3, iter:26,  [D loss: 0.700778, acc.: 43.00%] [G loss: 0.663387]\n",
      "epoch:3, iter:27,  [D loss: 0.706920, acc.: 41.00%] [G loss: 0.646903]\n",
      "epoch:3, iter:28,  [D loss: 0.687416, acc.: 43.00%] [G loss: 0.659649]\n",
      "epoch:3, iter:29,  [D loss: 0.676720, acc.: 49.00%] [G loss: 0.673638]\n",
      "epoch:3, iter:30,  [D loss: 0.694771, acc.: 43.00%] [G loss: 0.669828]\n",
      "epoch:3, iter:31,  [D loss: 0.662845, acc.: 50.00%] [G loss: 0.691392]\n",
      "epoch:3, iter:32,  [D loss: 0.690751, acc.: 44.00%] [G loss: 0.688267]\n",
      "epoch:3, iter:33,  [D loss: 0.714851, acc.: 39.00%] [G loss: 0.683263]\n",
      "epoch:3, iter:34,  [D loss: 0.671740, acc.: 50.00%] [G loss: 0.685708]\n",
      "epoch:3, iter:35,  [D loss: 0.675498, acc.: 46.00%] [G loss: 0.692254]\n",
      "epoch:3, iter:36,  [D loss: 0.698541, acc.: 43.00%] [G loss: 0.683467]\n",
      "epoch:3, iter:37,  [D loss: 0.712439, acc.: 46.00%] [G loss: 0.666758]\n",
      "epoch:3, iter:38,  [D loss: 0.681917, acc.: 45.00%] [G loss: 0.667242]\n",
      "epoch:3, iter:39,  [D loss: 0.728953, acc.: 39.00%] [G loss: 0.649979]\n",
      "epoch:3, iter:40,  [D loss: 0.703616, acc.: 44.00%] [G loss: 0.638374]\n",
      "epoch:3, iter:41,  [D loss: 0.694228, acc.: 45.00%] [G loss: 0.665596]\n",
      "epoch:3, iter:42,  [D loss: 0.677895, acc.: 48.00%] [G loss: 0.671819]\n",
      "epoch:3, iter:43,  [D loss: 0.689198, acc.: 45.00%] [G loss: 0.675189]\n",
      "epoch:3, iter:44,  [D loss: 0.673410, acc.: 49.00%] [G loss: 0.687627]\n",
      "epoch:3, iter:45,  [D loss: 0.683339, acc.: 44.00%] [G loss: 0.685473]\n",
      "epoch:3, iter:46,  [D loss: 0.673618, acc.: 50.00%] [G loss: 0.688780]\n",
      "epoch:3, iter:47,  [D loss: 0.653321, acc.: 56.00%] [G loss: 0.701561]\n",
      "epoch:3, iter:48,  [D loss: 0.706053, acc.: 46.00%] [G loss: 0.688674]\n",
      "epoch:3, iter:49,  [D loss: 0.663043, acc.: 49.00%] [G loss: 0.691548]\n",
      "epoch:3, iter:50,  [D loss: 0.669498, acc.: 54.00%] [G loss: 0.685220]\n",
      "epoch:3, iter:51,  [D loss: 0.678367, acc.: 45.00%] [G loss: 0.684312]\n",
      "epoch:3, iter:52,  [D loss: 0.680364, acc.: 47.00%] [G loss: 0.678505]\n",
      "epoch:3, iter:53,  [D loss: 0.672273, acc.: 48.00%] [G loss: 0.667127]\n",
      "epoch:3, iter:54,  [D loss: 0.685400, acc.: 43.00%] [G loss: 0.666499]\n",
      "epoch:3, iter:55,  [D loss: 0.665827, acc.: 48.00%] [G loss: 0.670938]\n",
      "epoch:3, iter:56,  [D loss: 0.661968, acc.: 49.00%] [G loss: 0.670843]\n",
      "epoch:3, iter:57,  [D loss: 0.660221, acc.: 48.00%] [G loss: 0.683633]\n",
      "epoch:3, iter:58,  [D loss: 0.666386, acc.: 49.00%] [G loss: 0.688671]\n",
      "epoch:3, iter:59,  [D loss: 0.664951, acc.: 50.00%] [G loss: 0.696275]\n",
      "epoch:3, iter:60,  [D loss: 0.686255, acc.: 46.00%] [G loss: 0.699389]\n",
      "epoch:3, iter:61,  [D loss: 0.650430, acc.: 47.00%] [G loss: 0.706154]\n",
      "epoch:3, iter:62,  [D loss: 0.655318, acc.: 47.00%] [G loss: 0.707556]\n",
      "epoch:3, iter:63,  [D loss: 0.660755, acc.: 55.00%] [G loss: 0.702413]\n",
      "epoch:3, iter:64,  [D loss: 0.649539, acc.: 54.00%] [G loss: 0.704548]\n",
      "epoch:3, iter:65,  [D loss: 0.686763, acc.: 46.00%] [G loss: 0.694964]\n",
      "epoch:3, iter:66,  [D loss: 0.667303, acc.: 43.00%] [G loss: 0.686432]\n",
      "epoch:3, iter:67,  [D loss: 0.672834, acc.: 47.00%] [G loss: 0.683438]\n",
      "epoch:3, iter:68,  [D loss: 0.654663, acc.: 50.00%] [G loss: 0.696727]\n",
      "epoch:3, iter:69,  [D loss: 0.660295, acc.: 49.00%] [G loss: 0.699070]\n",
      "epoch:3, iter:70,  [D loss: 0.665135, acc.: 50.00%] [G loss: 0.701736]\n",
      "epoch:3, iter:71,  [D loss: 0.672952, acc.: 49.00%] [G loss: 0.704385]\n",
      "epoch:3, iter:72,  [D loss: 0.664198, acc.: 50.00%] [G loss: 0.696733]\n",
      "epoch:3, iter:73,  [D loss: 0.666853, acc.: 46.00%] [G loss: 0.690139]\n",
      "epoch:3, iter:74,  [D loss: 0.668621, acc.: 54.00%] [G loss: 0.695272]\n",
      "epoch:3, iter:75,  [D loss: 0.654036, acc.: 51.00%] [G loss: 0.692919]\n",
      "epoch:3, iter:76,  [D loss: 0.652725, acc.: 52.00%] [G loss: 0.699206]\n",
      "epoch:3, iter:77,  [D loss: 0.678442, acc.: 53.00%] [G loss: 0.698544]\n",
      "epoch:3, iter:78,  [D loss: 0.665501, acc.: 47.00%] [G loss: 0.698975]\n",
      "epoch:3, iter:79,  [D loss: 0.663052, acc.: 49.00%] [G loss: 0.690005]\n",
      "epoch:3, iter:80,  [D loss: 0.658441, acc.: 51.00%] [G loss: 0.690387]\n",
      "epoch:3, iter:81,  [D loss: 0.662753, acc.: 48.00%] [G loss: 0.700806]\n",
      "epoch:3, iter:82,  [D loss: 0.675236, acc.: 46.00%] [G loss: 0.691906]\n",
      "epoch:3, iter:83,  [D loss: 0.642050, acc.: 49.00%] [G loss: 0.690987]\n",
      "epoch:3, iter:84,  [D loss: 0.669572, acc.: 46.00%] [G loss: 0.689328]\n",
      "epoch:3, iter:85,  [D loss: 0.675095, acc.: 48.00%] [G loss: 0.688006]\n",
      "epoch:3, iter:86,  [D loss: 0.678960, acc.: 43.00%] [G loss: 0.696917]\n",
      "epoch:3, iter:87,  [D loss: 0.675679, acc.: 46.00%] [G loss: 0.694767]\n",
      "epoch:3, iter:88,  [D loss: 0.681191, acc.: 46.00%] [G loss: 0.695695]\n",
      "epoch:3, iter:89,  [D loss: 0.673231, acc.: 51.00%] [G loss: 0.696798]\n",
      "epoch:3, iter:90,  [D loss: 0.658704, acc.: 50.00%] [G loss: 0.694102]\n",
      "epoch:3, iter:91,  [D loss: 0.677110, acc.: 45.00%] [G loss: 0.705722]\n",
      "epoch:3, iter:92,  [D loss: 0.680835, acc.: 45.00%] [G loss: 0.726285]\n",
      "epoch:3, iter:93,  [D loss: 0.662531, acc.: 45.00%] [G loss: 0.738418]\n",
      "epoch:3, iter:94,  [D loss: 0.671013, acc.: 47.00%] [G loss: 0.740589]\n",
      "epoch:3, iter:95,  [D loss: 0.689136, acc.: 43.00%] [G loss: 0.707737]\n",
      "epoch:3, iter:96,  [D loss: 0.669445, acc.: 50.00%] [G loss: 0.711837]\n",
      "epoch:3, iter:97,  [D loss: 0.682657, acc.: 47.00%] [G loss: 0.707695]\n",
      "epoch:3, iter:98,  [D loss: 0.663170, acc.: 49.00%] [G loss: 0.711188]\n",
      "epoch:3, iter:99,  [D loss: 0.680820, acc.: 45.00%] [G loss: 0.702403]\n",
      "epoch:4, iter:0,  [D loss: 0.674032, acc.: 43.00%] [G loss: 0.699360]\n",
      "epoch:4, iter:1,  [D loss: 0.678117, acc.: 47.00%] [G loss: 0.701876]\n",
      "epoch:4, iter:2,  [D loss: 0.673908, acc.: 45.00%] [G loss: 0.688097]\n",
      "epoch:4, iter:3,  [D loss: 0.675047, acc.: 49.00%] [G loss: 0.711397]\n",
      "epoch:4, iter:4,  [D loss: 0.662472, acc.: 47.00%] [G loss: 0.726220]\n",
      "epoch:4, iter:5,  [D loss: 0.663856, acc.: 48.00%] [G loss: 0.725974]\n",
      "epoch:4, iter:6,  [D loss: 0.681893, acc.: 44.00%] [G loss: 0.715630]\n",
      "epoch:4, iter:7,  [D loss: 0.679365, acc.: 40.00%] [G loss: 0.716775]\n",
      "epoch:4, iter:8,  [D loss: 0.664679, acc.: 48.00%] [G loss: 0.714019]\n",
      "epoch:4, iter:9,  [D loss: 0.686989, acc.: 49.00%] [G loss: 0.702496]\n",
      "epoch:4, iter:10,  [D loss: 0.667266, acc.: 45.00%] [G loss: 0.704236]\n",
      "epoch:4, iter:11,  [D loss: 0.673955, acc.: 48.00%] [G loss: 0.727730]\n",
      "epoch:4, iter:12,  [D loss: 0.657080, acc.: 46.00%] [G loss: 0.714131]\n",
      "epoch:4, iter:13,  [D loss: 0.660462, acc.: 48.00%] [G loss: 0.724831]\n",
      "epoch:4, iter:14,  [D loss: 0.666966, acc.: 54.00%] [G loss: 0.736263]\n",
      "epoch:4, iter:15,  [D loss: 0.674321, acc.: 58.00%] [G loss: 0.729754]\n",
      "epoch:4, iter:16,  [D loss: 0.686359, acc.: 46.00%] [G loss: 0.714125]\n",
      "epoch:4, iter:17,  [D loss: 0.645974, acc.: 51.00%] [G loss: 0.705474]\n",
      "epoch:4, iter:18,  [D loss: 0.675475, acc.: 48.00%] [G loss: 0.694292]\n",
      "epoch:4, iter:19,  [D loss: 0.660395, acc.: 49.00%] [G loss: 0.701055]\n",
      "epoch:4, iter:20,  [D loss: 0.669288, acc.: 46.00%] [G loss: 0.723340]\n",
      "epoch:4, iter:21,  [D loss: 0.657374, acc.: 46.00%] [G loss: 0.732810]\n",
      "epoch:4, iter:22,  [D loss: 0.656630, acc.: 52.00%] [G loss: 0.715069]\n",
      "epoch:4, iter:23,  [D loss: 0.667204, acc.: 52.00%] [G loss: 0.733591]\n",
      "epoch:4, iter:24,  [D loss: 0.667719, acc.: 47.00%] [G loss: 0.733370]\n",
      "epoch:4, iter:25,  [D loss: 0.662291, acc.: 54.00%] [G loss: 0.734270]\n",
      "epoch:4, iter:26,  [D loss: 0.641662, acc.: 52.00%] [G loss: 0.730791]\n",
      "epoch:4, iter:27,  [D loss: 0.649808, acc.: 52.00%] [G loss: 0.728627]\n",
      "epoch:4, iter:28,  [D loss: 0.659266, acc.: 48.00%] [G loss: 0.714898]\n",
      "epoch:4, iter:29,  [D loss: 0.663425, acc.: 49.00%] [G loss: 0.722749]\n",
      "epoch:4, iter:30,  [D loss: 0.659229, acc.: 50.00%] [G loss: 0.707912]\n",
      "epoch:4, iter:31,  [D loss: 0.657993, acc.: 51.00%] [G loss: 0.712253]\n",
      "epoch:4, iter:32,  [D loss: 0.645669, acc.: 53.00%] [G loss: 0.716395]\n",
      "epoch:4, iter:33,  [D loss: 0.665538, acc.: 51.00%] [G loss: 0.690924]\n",
      "epoch:4, iter:34,  [D loss: 0.627139, acc.: 60.00%] [G loss: 0.690911]\n",
      "epoch:4, iter:35,  [D loss: 0.654056, acc.: 58.00%] [G loss: 0.696507]\n",
      "epoch:4, iter:36,  [D loss: 0.662656, acc.: 55.00%] [G loss: 0.699838]\n",
      "epoch:4, iter:37,  [D loss: 0.632716, acc.: 65.00%] [G loss: 0.718361]\n",
      "epoch:4, iter:38,  [D loss: 0.654790, acc.: 60.00%] [G loss: 0.720741]\n",
      "epoch:4, iter:39,  [D loss: 0.639316, acc.: 65.00%] [G loss: 0.708733]\n",
      "epoch:4, iter:40,  [D loss: 0.661695, acc.: 57.00%] [G loss: 0.699811]\n",
      "epoch:4, iter:41,  [D loss: 0.661002, acc.: 58.00%] [G loss: 0.708623]\n",
      "epoch:4, iter:42,  [D loss: 0.632017, acc.: 62.00%] [G loss: 0.713072]\n",
      "epoch:4, iter:43,  [D loss: 0.647215, acc.: 66.00%] [G loss: 0.724152]\n",
      "epoch:4, iter:44,  [D loss: 0.642715, acc.: 62.00%] [G loss: 0.729819]\n",
      "epoch:4, iter:45,  [D loss: 0.640723, acc.: 71.00%] [G loss: 0.730627]\n",
      "epoch:4, iter:46,  [D loss: 0.670589, acc.: 63.00%] [G loss: 0.728204]\n",
      "epoch:4, iter:47,  [D loss: 0.656597, acc.: 64.00%] [G loss: 0.735611]\n",
      "epoch:4, iter:48,  [D loss: 0.655021, acc.: 65.00%] [G loss: 0.737547]\n",
      "epoch:4, iter:49,  [D loss: 0.655131, acc.: 56.00%] [G loss: 0.724644]\n",
      "epoch:4, iter:50,  [D loss: 0.662836, acc.: 57.00%] [G loss: 0.712623]\n",
      "epoch:4, iter:51,  [D loss: 0.645945, acc.: 60.00%] [G loss: 0.706738]\n",
      "epoch:4, iter:52,  [D loss: 0.646439, acc.: 65.00%] [G loss: 0.702731]\n",
      "epoch:4, iter:53,  [D loss: 0.657048, acc.: 64.00%] [G loss: 0.716043]\n",
      "epoch:4, iter:54,  [D loss: 0.637673, acc.: 67.00%] [G loss: 0.726474]\n",
      "epoch:4, iter:55,  [D loss: 0.644300, acc.: 68.00%] [G loss: 0.737361]\n",
      "epoch:4, iter:56,  [D loss: 0.629942, acc.: 70.00%] [G loss: 0.734347]\n",
      "epoch:4, iter:57,  [D loss: 0.637952, acc.: 64.00%] [G loss: 0.728302]\n",
      "epoch:4, iter:58,  [D loss: 0.636763, acc.: 74.00%] [G loss: 0.754348]\n",
      "epoch:4, iter:59,  [D loss: 0.643787, acc.: 70.00%] [G loss: 0.764096]\n",
      "epoch:4, iter:60,  [D loss: 0.617049, acc.: 75.00%] [G loss: 0.750466]\n",
      "epoch:4, iter:61,  [D loss: 0.630721, acc.: 68.00%] [G loss: 0.732767]\n",
      "epoch:4, iter:62,  [D loss: 0.610898, acc.: 76.00%] [G loss: 0.721683]\n",
      "epoch:4, iter:63,  [D loss: 0.617948, acc.: 74.00%] [G loss: 0.724515]\n",
      "epoch:4, iter:64,  [D loss: 0.615071, acc.: 76.00%] [G loss: 0.724484]\n",
      "epoch:4, iter:65,  [D loss: 0.645798, acc.: 76.00%] [G loss: 0.707577]\n",
      "epoch:4, iter:66,  [D loss: 0.619540, acc.: 68.00%] [G loss: 0.731058]\n",
      "epoch:4, iter:67,  [D loss: 0.664781, acc.: 66.00%] [G loss: 0.771939]\n",
      "epoch:4, iter:68,  [D loss: 0.632138, acc.: 66.00%] [G loss: 0.793437]\n",
      "epoch:4, iter:69,  [D loss: 0.634685, acc.: 70.00%] [G loss: 0.761114]\n",
      "epoch:4, iter:70,  [D loss: 0.655289, acc.: 57.00%] [G loss: 0.756490]\n",
      "epoch:4, iter:71,  [D loss: 0.629210, acc.: 63.00%] [G loss: 0.757966]\n",
      "epoch:4, iter:72,  [D loss: 0.637607, acc.: 61.00%] [G loss: 0.761020]\n",
      "epoch:4, iter:73,  [D loss: 0.622135, acc.: 65.00%] [G loss: 0.756536]\n",
      "epoch:4, iter:74,  [D loss: 0.639467, acc.: 57.00%] [G loss: 0.737169]\n",
      "epoch:4, iter:75,  [D loss: 0.622675, acc.: 61.00%] [G loss: 0.721115]\n",
      "epoch:4, iter:76,  [D loss: 0.609308, acc.: 69.00%] [G loss: 0.715078]\n",
      "epoch:4, iter:77,  [D loss: 0.642263, acc.: 63.00%] [G loss: 0.719636]\n",
      "epoch:4, iter:78,  [D loss: 0.628778, acc.: 61.00%] [G loss: 0.730393]\n",
      "epoch:4, iter:79,  [D loss: 0.664498, acc.: 56.00%] [G loss: 0.748370]\n",
      "epoch:4, iter:80,  [D loss: 0.640362, acc.: 59.00%] [G loss: 0.755596]\n",
      "epoch:4, iter:81,  [D loss: 0.640793, acc.: 58.00%] [G loss: 0.754947]\n",
      "epoch:4, iter:82,  [D loss: 0.646899, acc.: 58.00%] [G loss: 0.756351]\n",
      "epoch:4, iter:83,  [D loss: 0.663197, acc.: 53.00%] [G loss: 0.744289]\n",
      "epoch:4, iter:84,  [D loss: 0.640268, acc.: 61.00%] [G loss: 0.750494]\n",
      "epoch:4, iter:85,  [D loss: 0.664272, acc.: 51.00%] [G loss: 0.744516]\n",
      "epoch:4, iter:86,  [D loss: 0.681057, acc.: 60.00%] [G loss: 0.755751]\n",
      "epoch:4, iter:87,  [D loss: 0.649987, acc.: 69.00%] [G loss: 0.769789]\n",
      "epoch:4, iter:88,  [D loss: 0.643299, acc.: 64.00%] [G loss: 0.760415]\n",
      "epoch:4, iter:89,  [D loss: 0.650013, acc.: 50.00%] [G loss: 0.751050]\n",
      "epoch:4, iter:90,  [D loss: 0.651380, acc.: 63.00%] [G loss: 0.726630]\n",
      "epoch:4, iter:91,  [D loss: 0.633135, acc.: 67.00%] [G loss: 0.727133]\n",
      "epoch:4, iter:92,  [D loss: 0.646667, acc.: 62.00%] [G loss: 0.727460]\n",
      "epoch:4, iter:93,  [D loss: 0.676360, acc.: 60.00%] [G loss: 0.751244]\n",
      "epoch:4, iter:94,  [D loss: 0.647517, acc.: 62.00%] [G loss: 0.749004]\n",
      "epoch:4, iter:95,  [D loss: 0.645214, acc.: 61.00%] [G loss: 0.766650]\n",
      "epoch:4, iter:96,  [D loss: 0.630247, acc.: 62.00%] [G loss: 0.758416]\n",
      "epoch:4, iter:97,  [D loss: 0.654537, acc.: 57.00%] [G loss: 0.775228]\n",
      "epoch:4, iter:98,  [D loss: 0.655203, acc.: 55.00%] [G loss: 0.802338]\n",
      "epoch:4, iter:99,  [D loss: 0.627224, acc.: 62.00%] [G loss: 0.791306]\n",
      "epoch:5, iter:0,  [D loss: 0.647381, acc.: 58.00%] [G loss: 0.757937]\n",
      "epoch:5, iter:1,  [D loss: 0.651571, acc.: 52.00%] [G loss: 0.740468]\n",
      "epoch:5, iter:2,  [D loss: 0.652821, acc.: 53.00%] [G loss: 0.737202]\n",
      "epoch:5, iter:3,  [D loss: 0.663834, acc.: 51.00%] [G loss: 0.735050]\n",
      "epoch:5, iter:4,  [D loss: 0.653227, acc.: 55.00%] [G loss: 0.738218]\n",
      "epoch:5, iter:5,  [D loss: 0.657179, acc.: 52.00%] [G loss: 0.747792]\n",
      "epoch:5, iter:6,  [D loss: 0.647016, acc.: 52.00%] [G loss: 0.750626]\n",
      "epoch:5, iter:7,  [D loss: 0.641585, acc.: 55.00%] [G loss: 0.731834]\n",
      "epoch:5, iter:8,  [D loss: 0.664578, acc.: 50.00%] [G loss: 0.746318]\n",
      "epoch:5, iter:9,  [D loss: 0.648646, acc.: 56.00%] [G loss: 0.745870]\n",
      "epoch:5, iter:10,  [D loss: 0.636859, acc.: 60.00%] [G loss: 0.729808]\n",
      "epoch:5, iter:11,  [D loss: 0.662242, acc.: 57.00%] [G loss: 0.720530]\n",
      "epoch:5, iter:12,  [D loss: 0.653671, acc.: 52.00%] [G loss: 0.719442]\n",
      "epoch:5, iter:13,  [D loss: 0.626409, acc.: 63.00%] [G loss: 0.716330]\n",
      "epoch:5, iter:14,  [D loss: 0.643815, acc.: 57.00%] [G loss: 0.718947]\n",
      "epoch:5, iter:15,  [D loss: 0.653088, acc.: 54.00%] [G loss: 0.722526]\n",
      "epoch:5, iter:16,  [D loss: 0.649714, acc.: 61.00%] [G loss: 0.738544]\n",
      "epoch:5, iter:17,  [D loss: 0.648552, acc.: 57.00%] [G loss: 0.757670]\n",
      "epoch:5, iter:18,  [D loss: 0.650870, acc.: 54.00%] [G loss: 0.760771]\n",
      "epoch:5, iter:19,  [D loss: 0.646233, acc.: 64.00%] [G loss: 0.752565]\n",
      "epoch:5, iter:20,  [D loss: 0.631194, acc.: 66.00%] [G loss: 0.740226]\n",
      "epoch:5, iter:21,  [D loss: 0.631017, acc.: 63.00%] [G loss: 0.748992]\n",
      "epoch:5, iter:22,  [D loss: 0.647769, acc.: 61.00%] [G loss: 0.748898]\n",
      "epoch:5, iter:23,  [D loss: 0.615684, acc.: 71.00%] [G loss: 0.735400]\n",
      "epoch:5, iter:24,  [D loss: 0.631376, acc.: 65.00%] [G loss: 0.730593]\n",
      "epoch:5, iter:25,  [D loss: 0.617385, acc.: 63.00%] [G loss: 0.753564]\n",
      "epoch:5, iter:26,  [D loss: 0.636746, acc.: 61.00%] [G loss: 0.769984]\n",
      "epoch:5, iter:27,  [D loss: 0.650028, acc.: 69.00%] [G loss: 0.788913]\n",
      "epoch:5, iter:28,  [D loss: 0.643574, acc.: 58.00%] [G loss: 0.785370]\n",
      "epoch:5, iter:29,  [D loss: 0.620330, acc.: 64.00%] [G loss: 0.769720]\n",
      "epoch:5, iter:30,  [D loss: 0.622047, acc.: 54.00%] [G loss: 0.775531]\n",
      "epoch:5, iter:31,  [D loss: 0.647099, acc.: 58.00%] [G loss: 0.776335]\n",
      "epoch:5, iter:32,  [D loss: 0.633714, acc.: 67.00%] [G loss: 0.769624]\n",
      "epoch:5, iter:33,  [D loss: 0.649251, acc.: 61.00%] [G loss: 0.751040]\n",
      "epoch:5, iter:34,  [D loss: 0.639536, acc.: 61.00%] [G loss: 0.757660]\n",
      "epoch:5, iter:35,  [D loss: 0.625925, acc.: 56.00%] [G loss: 0.759081]\n",
      "epoch:5, iter:36,  [D loss: 0.646008, acc.: 57.00%] [G loss: 0.768095]\n",
      "epoch:5, iter:37,  [D loss: 0.630668, acc.: 68.00%] [G loss: 0.734563]\n",
      "epoch:5, iter:38,  [D loss: 0.642641, acc.: 71.00%] [G loss: 0.722297]\n",
      "epoch:5, iter:39,  [D loss: 0.682311, acc.: 63.00%] [G loss: 0.736002]\n",
      "epoch:5, iter:40,  [D loss: 0.651563, acc.: 67.00%] [G loss: 0.775753]\n",
      "epoch:5, iter:41,  [D loss: 0.648373, acc.: 58.00%] [G loss: 0.792585]\n",
      "epoch:5, iter:42,  [D loss: 0.668946, acc.: 52.00%] [G loss: 0.767281]\n",
      "epoch:5, iter:43,  [D loss: 0.655092, acc.: 51.00%] [G loss: 0.761212]\n",
      "epoch:5, iter:44,  [D loss: 0.670075, acc.: 64.00%] [G loss: 0.783216]\n",
      "epoch:5, iter:45,  [D loss: 0.658137, acc.: 65.00%] [G loss: 0.794237]\n",
      "epoch:5, iter:46,  [D loss: 0.643785, acc.: 63.00%] [G loss: 0.773329]\n",
      "epoch:5, iter:47,  [D loss: 0.645515, acc.: 60.00%] [G loss: 0.787399]\n",
      "epoch:5, iter:48,  [D loss: 0.655474, acc.: 60.00%] [G loss: 0.816741]\n",
      "epoch:5, iter:49,  [D loss: 0.646222, acc.: 60.00%] [G loss: 0.809380]\n",
      "epoch:5, iter:50,  [D loss: 0.634676, acc.: 57.00%] [G loss: 0.799223]\n",
      "epoch:5, iter:51,  [D loss: 0.653316, acc.: 56.00%] [G loss: 0.767293]\n",
      "epoch:5, iter:52,  [D loss: 0.638611, acc.: 62.00%] [G loss: 0.770539]\n",
      "epoch:5, iter:53,  [D loss: 0.649563, acc.: 56.00%] [G loss: 0.809604]\n",
      "epoch:5, iter:54,  [D loss: 0.643804, acc.: 58.00%] [G loss: 0.798327]\n",
      "epoch:5, iter:55,  [D loss: 0.653698, acc.: 56.00%] [G loss: 0.777583]\n",
      "epoch:5, iter:56,  [D loss: 0.669771, acc.: 53.00%] [G loss: 0.773903]\n",
      "epoch:5, iter:57,  [D loss: 0.654818, acc.: 49.00%] [G loss: 0.788639]\n",
      "epoch:5, iter:58,  [D loss: 0.640061, acc.: 57.00%] [G loss: 0.829144]\n",
      "epoch:5, iter:59,  [D loss: 0.668670, acc.: 57.00%] [G loss: 0.797685]\n",
      "epoch:5, iter:60,  [D loss: 0.670155, acc.: 48.00%] [G loss: 0.755924]\n",
      "epoch:5, iter:61,  [D loss: 0.660092, acc.: 51.00%] [G loss: 0.729001]\n",
      "epoch:5, iter:62,  [D loss: 0.648494, acc.: 57.00%] [G loss: 0.741813]\n",
      "epoch:5, iter:63,  [D loss: 0.669153, acc.: 63.00%] [G loss: 0.758250]\n",
      "epoch:5, iter:64,  [D loss: 0.663178, acc.: 58.00%] [G loss: 0.772291]\n",
      "epoch:5, iter:65,  [D loss: 0.647480, acc.: 61.00%] [G loss: 0.772032]\n",
      "epoch:5, iter:66,  [D loss: 0.676105, acc.: 52.00%] [G loss: 0.800800]\n",
      "epoch:5, iter:67,  [D loss: 0.671722, acc.: 58.00%] [G loss: 0.791909]\n",
      "epoch:5, iter:68,  [D loss: 0.655547, acc.: 62.00%] [G loss: 0.757967]\n",
      "epoch:5, iter:69,  [D loss: 0.665928, acc.: 57.00%] [G loss: 0.752088]\n",
      "epoch:5, iter:70,  [D loss: 0.657763, acc.: 57.00%] [G loss: 0.735059]\n",
      "epoch:5, iter:71,  [D loss: 0.655964, acc.: 55.00%] [G loss: 0.737321]\n",
      "epoch:5, iter:72,  [D loss: 0.652561, acc.: 56.00%] [G loss: 0.741588]\n",
      "epoch:5, iter:73,  [D loss: 0.643037, acc.: 56.00%] [G loss: 0.717948]\n",
      "epoch:5, iter:74,  [D loss: 0.681363, acc.: 57.00%] [G loss: 0.752863]\n",
      "epoch:5, iter:75,  [D loss: 0.621081, acc.: 62.00%] [G loss: 0.776621]\n",
      "epoch:5, iter:76,  [D loss: 0.632113, acc.: 70.00%] [G loss: 0.779513]\n",
      "epoch:5, iter:77,  [D loss: 0.622459, acc.: 62.00%] [G loss: 0.781165]\n",
      "epoch:5, iter:78,  [D loss: 0.645303, acc.: 68.00%] [G loss: 0.788414]\n",
      "epoch:5, iter:79,  [D loss: 0.664483, acc.: 61.00%] [G loss: 0.776592]\n",
      "epoch:5, iter:80,  [D loss: 0.655792, acc.: 57.00%] [G loss: 0.776147]\n",
      "epoch:5, iter:81,  [D loss: 0.648623, acc.: 56.00%] [G loss: 0.774569]\n",
      "epoch:5, iter:82,  [D loss: 0.647050, acc.: 57.00%] [G loss: 0.770632]\n",
      "epoch:5, iter:83,  [D loss: 0.655904, acc.: 58.00%] [G loss: 0.760637]\n",
      "epoch:5, iter:84,  [D loss: 0.655679, acc.: 51.00%] [G loss: 0.770460]\n",
      "epoch:5, iter:85,  [D loss: 0.648550, acc.: 58.00%] [G loss: 0.787098]\n",
      "epoch:5, iter:86,  [D loss: 0.655686, acc.: 56.00%] [G loss: 0.770232]\n",
      "epoch:5, iter:87,  [D loss: 0.659577, acc.: 57.00%] [G loss: 0.765298]\n",
      "epoch:5, iter:88,  [D loss: 0.637679, acc.: 56.00%] [G loss: 0.760560]\n",
      "epoch:5, iter:89,  [D loss: 0.634525, acc.: 63.00%] [G loss: 0.741834]\n",
      "epoch:5, iter:90,  [D loss: 0.667261, acc.: 56.00%] [G loss: 0.741967]\n",
      "epoch:5, iter:91,  [D loss: 0.628706, acc.: 58.00%] [G loss: 0.740401]\n",
      "epoch:5, iter:92,  [D loss: 0.635432, acc.: 59.00%] [G loss: 0.734767]\n",
      "epoch:5, iter:93,  [D loss: 0.638462, acc.: 72.00%] [G loss: 0.741305]\n",
      "epoch:5, iter:94,  [D loss: 0.665189, acc.: 74.00%] [G loss: 0.778796]\n",
      "epoch:5, iter:95,  [D loss: 0.639680, acc.: 71.00%] [G loss: 0.780731]\n",
      "epoch:5, iter:96,  [D loss: 0.635579, acc.: 66.00%] [G loss: 0.785015]\n",
      "epoch:5, iter:97,  [D loss: 0.631290, acc.: 65.00%] [G loss: 0.792951]\n",
      "epoch:5, iter:98,  [D loss: 0.637105, acc.: 68.00%] [G loss: 0.784992]\n",
      "epoch:5, iter:99,  [D loss: 0.632662, acc.: 58.00%] [G loss: 0.830807]\n",
      "epoch:6, iter:0,  [D loss: 0.624094, acc.: 71.00%] [G loss: 0.830742]\n",
      "epoch:6, iter:1,  [D loss: 0.602369, acc.: 73.00%] [G loss: 0.818904]\n",
      "epoch:6, iter:2,  [D loss: 0.637693, acc.: 57.00%] [G loss: 0.813456]\n",
      "epoch:6, iter:3,  [D loss: 0.614835, acc.: 66.00%] [G loss: 0.804528]\n",
      "epoch:6, iter:4,  [D loss: 0.605033, acc.: 67.00%] [G loss: 0.809637]\n",
      "epoch:6, iter:5,  [D loss: 0.625749, acc.: 58.00%] [G loss: 0.782105]\n",
      "epoch:6, iter:6,  [D loss: 0.668194, acc.: 45.00%] [G loss: 0.747093]\n",
      "epoch:6, iter:7,  [D loss: 0.637554, acc.: 60.00%] [G loss: 0.732430]\n",
      "epoch:6, iter:8,  [D loss: 0.658256, acc.: 52.00%] [G loss: 0.733362]\n",
      "epoch:6, iter:9,  [D loss: 0.645663, acc.: 53.00%] [G loss: 0.753219]\n",
      "epoch:6, iter:10,  [D loss: 0.655304, acc.: 64.00%] [G loss: 0.766532]\n",
      "epoch:6, iter:11,  [D loss: 0.642151, acc.: 64.00%] [G loss: 0.766665]\n",
      "epoch:6, iter:12,  [D loss: 0.653694, acc.: 63.00%] [G loss: 0.782159]\n",
      "epoch:6, iter:13,  [D loss: 0.673426, acc.: 54.00%] [G loss: 0.800922]\n",
      "epoch:6, iter:14,  [D loss: 0.651708, acc.: 52.00%] [G loss: 0.810592]\n",
      "epoch:6, iter:15,  [D loss: 0.636534, acc.: 63.00%] [G loss: 0.838530]\n",
      "epoch:6, iter:16,  [D loss: 0.632548, acc.: 62.00%] [G loss: 0.832422]\n",
      "epoch:6, iter:17,  [D loss: 0.658260, acc.: 55.00%] [G loss: 0.809729]\n",
      "epoch:6, iter:18,  [D loss: 0.652358, acc.: 60.00%] [G loss: 0.816253]\n",
      "epoch:6, iter:19,  [D loss: 0.673185, acc.: 53.00%] [G loss: 0.808231]\n",
      "epoch:6, iter:20,  [D loss: 0.650328, acc.: 59.00%] [G loss: 0.790040]\n",
      "epoch:6, iter:21,  [D loss: 0.640862, acc.: 59.00%] [G loss: 0.787743]\n",
      "epoch:6, iter:22,  [D loss: 0.650596, acc.: 60.00%] [G loss: 0.778682]\n",
      "epoch:6, iter:23,  [D loss: 0.656008, acc.: 58.00%] [G loss: 0.805340]\n",
      "epoch:6, iter:24,  [D loss: 0.634186, acc.: 61.00%] [G loss: 0.844504]\n",
      "epoch:6, iter:25,  [D loss: 0.632195, acc.: 64.00%] [G loss: 0.833814]\n",
      "epoch:6, iter:26,  [D loss: 0.652112, acc.: 62.00%] [G loss: 0.804207]\n",
      "epoch:6, iter:27,  [D loss: 0.644487, acc.: 66.00%] [G loss: 0.788942]\n",
      "epoch:6, iter:28,  [D loss: 0.623329, acc.: 65.00%] [G loss: 0.793808]\n",
      "epoch:6, iter:29,  [D loss: 0.626518, acc.: 66.00%] [G loss: 0.798663]\n",
      "epoch:6, iter:30,  [D loss: 0.656454, acc.: 66.00%] [G loss: 0.808720]\n",
      "epoch:6, iter:31,  [D loss: 0.642844, acc.: 61.00%] [G loss: 0.789927]\n",
      "epoch:6, iter:32,  [D loss: 0.644657, acc.: 58.00%] [G loss: 0.764326]\n",
      "epoch:6, iter:33,  [D loss: 0.654638, acc.: 57.00%] [G loss: 0.770669]\n",
      "epoch:6, iter:34,  [D loss: 0.631536, acc.: 66.00%] [G loss: 0.780791]\n",
      "epoch:6, iter:35,  [D loss: 0.644808, acc.: 67.00%] [G loss: 0.796814]\n",
      "epoch:6, iter:36,  [D loss: 0.653037, acc.: 60.00%] [G loss: 0.809165]\n",
      "epoch:6, iter:37,  [D loss: 0.612239, acc.: 70.00%] [G loss: 0.822763]\n",
      "epoch:6, iter:38,  [D loss: 0.670159, acc.: 62.00%] [G loss: 0.797303]\n",
      "epoch:6, iter:39,  [D loss: 0.662542, acc.: 55.00%] [G loss: 0.788875]\n",
      "epoch:6, iter:40,  [D loss: 0.660147, acc.: 55.00%] [G loss: 0.817119]\n",
      "epoch:6, iter:41,  [D loss: 0.658571, acc.: 61.00%] [G loss: 0.784116]\n",
      "epoch:6, iter:42,  [D loss: 0.650231, acc.: 65.00%] [G loss: 0.775459]\n",
      "epoch:6, iter:43,  [D loss: 0.674601, acc.: 62.00%] [G loss: 0.812199]\n",
      "epoch:6, iter:44,  [D loss: 0.637278, acc.: 68.00%] [G loss: 0.809937]\n",
      "epoch:6, iter:45,  [D loss: 0.685962, acc.: 54.00%] [G loss: 0.843805]\n",
      "epoch:6, iter:46,  [D loss: 0.661256, acc.: 59.00%] [G loss: 0.854279]\n",
      "epoch:6, iter:47,  [D loss: 0.641110, acc.: 61.00%] [G loss: 0.820951]\n",
      "epoch:6, iter:48,  [D loss: 0.651852, acc.: 55.00%] [G loss: 0.782087]\n",
      "epoch:6, iter:49,  [D loss: 0.646373, acc.: 64.00%] [G loss: 0.773907]\n",
      "epoch:6, iter:50,  [D loss: 0.641353, acc.: 63.00%] [G loss: 0.785154]\n",
      "epoch:6, iter:51,  [D loss: 0.642275, acc.: 61.00%] [G loss: 0.782385]\n",
      "epoch:6, iter:52,  [D loss: 0.639782, acc.: 63.00%] [G loss: 0.785423]\n",
      "epoch:6, iter:53,  [D loss: 0.636652, acc.: 64.00%] [G loss: 0.799563]\n",
      "epoch:6, iter:54,  [D loss: 0.620036, acc.: 66.00%] [G loss: 0.811508]\n",
      "epoch:6, iter:55,  [D loss: 0.611286, acc.: 68.00%] [G loss: 0.845944]\n",
      "epoch:6, iter:56,  [D loss: 0.637361, acc.: 65.00%] [G loss: 0.836195]\n",
      "epoch:6, iter:57,  [D loss: 0.637564, acc.: 60.00%] [G loss: 0.844908]\n",
      "epoch:6, iter:58,  [D loss: 0.639817, acc.: 62.00%] [G loss: 0.833116]\n",
      "epoch:6, iter:59,  [D loss: 0.631881, acc.: 62.00%] [G loss: 0.794753]\n",
      "epoch:6, iter:60,  [D loss: 0.622314, acc.: 66.00%] [G loss: 0.794890]\n",
      "epoch:6, iter:61,  [D loss: 0.639861, acc.: 58.00%] [G loss: 0.785767]\n",
      "epoch:6, iter:62,  [D loss: 0.635067, acc.: 61.00%] [G loss: 0.791395]\n",
      "epoch:6, iter:63,  [D loss: 0.616007, acc.: 64.00%] [G loss: 0.809189]\n",
      "epoch:6, iter:64,  [D loss: 0.609227, acc.: 81.00%] [G loss: 0.804320]\n",
      "epoch:6, iter:65,  [D loss: 0.641084, acc.: 70.00%] [G loss: 0.847503]\n",
      "epoch:6, iter:66,  [D loss: 0.590812, acc.: 76.00%] [G loss: 0.894102]\n",
      "epoch:6, iter:67,  [D loss: 0.619436, acc.: 71.00%] [G loss: 0.844030]\n",
      "epoch:6, iter:68,  [D loss: 0.589590, acc.: 71.00%] [G loss: 0.854246]\n",
      "epoch:6, iter:69,  [D loss: 0.592457, acc.: 73.00%] [G loss: 0.860307]\n",
      "epoch:6, iter:70,  [D loss: 0.578753, acc.: 69.00%] [G loss: 0.888164]\n",
      "epoch:6, iter:71,  [D loss: 0.595953, acc.: 62.00%] [G loss: 0.941520]\n",
      "epoch:6, iter:72,  [D loss: 0.591555, acc.: 77.00%] [G loss: 0.912789]\n",
      "epoch:6, iter:73,  [D loss: 0.636821, acc.: 68.00%] [G loss: 0.859568]\n",
      "epoch:6, iter:74,  [D loss: 0.612286, acc.: 66.00%] [G loss: 0.842210]\n",
      "epoch:6, iter:75,  [D loss: 0.638585, acc.: 73.00%] [G loss: 0.887431]\n",
      "epoch:6, iter:76,  [D loss: 0.611340, acc.: 79.00%] [G loss: 0.890493]\n",
      "epoch:6, iter:77,  [D loss: 0.615525, acc.: 72.00%] [G loss: 0.859473]\n",
      "epoch:6, iter:78,  [D loss: 0.639019, acc.: 65.00%] [G loss: 0.830930]\n",
      "epoch:6, iter:79,  [D loss: 0.619485, acc.: 66.00%] [G loss: 0.825040]\n",
      "epoch:6, iter:80,  [D loss: 0.642264, acc.: 63.00%] [G loss: 0.822054]\n",
      "epoch:6, iter:81,  [D loss: 0.613528, acc.: 70.00%] [G loss: 0.839171]\n",
      "epoch:6, iter:82,  [D loss: 0.630062, acc.: 63.00%] [G loss: 0.803093]\n",
      "epoch:6, iter:83,  [D loss: 0.635884, acc.: 67.00%] [G loss: 0.808927]\n",
      "epoch:6, iter:84,  [D loss: 0.618216, acc.: 69.00%] [G loss: 0.791572]\n",
      "epoch:6, iter:85,  [D loss: 0.630181, acc.: 67.00%] [G loss: 0.797277]\n",
      "epoch:6, iter:86,  [D loss: 0.648160, acc.: 71.00%] [G loss: 0.834212]\n",
      "epoch:6, iter:87,  [D loss: 0.647874, acc.: 60.00%] [G loss: 0.842803]\n",
      "epoch:6, iter:88,  [D loss: 0.626078, acc.: 68.00%] [G loss: 0.894370]\n",
      "epoch:6, iter:89,  [D loss: 0.614887, acc.: 74.00%] [G loss: 0.862993]\n",
      "epoch:6, iter:90,  [D loss: 0.652563, acc.: 52.00%] [G loss: 0.812981]\n",
      "epoch:6, iter:91,  [D loss: 0.630022, acc.: 60.00%] [G loss: 0.826183]\n",
      "epoch:6, iter:92,  [D loss: 0.612881, acc.: 67.00%] [G loss: 0.819283]\n",
      "epoch:6, iter:93,  [D loss: 0.634388, acc.: 65.00%] [G loss: 0.802226]\n",
      "epoch:6, iter:94,  [D loss: 0.639375, acc.: 63.00%] [G loss: 0.822554]\n",
      "epoch:6, iter:95,  [D loss: 0.598596, acc.: 79.00%] [G loss: 0.843379]\n",
      "epoch:6, iter:96,  [D loss: 0.623727, acc.: 72.00%] [G loss: 0.854142]\n",
      "epoch:6, iter:97,  [D loss: 0.657031, acc.: 68.00%] [G loss: 0.857215]\n",
      "epoch:6, iter:98,  [D loss: 0.615672, acc.: 74.00%] [G loss: 0.843061]\n",
      "epoch:6, iter:99,  [D loss: 0.621384, acc.: 73.00%] [G loss: 0.822758]\n",
      "epoch:7, iter:0,  [D loss: 0.658657, acc.: 66.00%] [G loss: 0.847077]\n",
      "epoch:7, iter:1,  [D loss: 0.640004, acc.: 67.00%] [G loss: 0.855835]\n",
      "epoch:7, iter:2,  [D loss: 0.627010, acc.: 69.00%] [G loss: 0.855110]\n",
      "epoch:7, iter:3,  [D loss: 0.627849, acc.: 65.00%] [G loss: 0.903365]\n",
      "epoch:7, iter:4,  [D loss: 0.607506, acc.: 71.00%] [G loss: 0.914009]\n",
      "epoch:7, iter:5,  [D loss: 0.601799, acc.: 73.00%] [G loss: 0.885604]\n",
      "epoch:7, iter:6,  [D loss: 0.597242, acc.: 70.00%] [G loss: 0.901276]\n",
      "epoch:7, iter:7,  [D loss: 0.608802, acc.: 70.00%] [G loss: 0.906838]\n",
      "epoch:7, iter:8,  [D loss: 0.642717, acc.: 62.00%] [G loss: 0.858265]\n",
      "epoch:7, iter:9,  [D loss: 0.586908, acc.: 79.00%] [G loss: 0.856091]\n",
      "epoch:7, iter:10,  [D loss: 0.624143, acc.: 71.00%] [G loss: 0.833375]\n",
      "epoch:7, iter:11,  [D loss: 0.602161, acc.: 76.00%] [G loss: 0.843839]\n",
      "epoch:7, iter:12,  [D loss: 0.563013, acc.: 77.00%] [G loss: 0.860372]\n",
      "epoch:7, iter:13,  [D loss: 0.613675, acc.: 67.00%] [G loss: 0.837431]\n",
      "epoch:7, iter:14,  [D loss: 0.585516, acc.: 71.00%] [G loss: 0.848183]\n",
      "epoch:7, iter:15,  [D loss: 0.595444, acc.: 74.00%] [G loss: 0.848642]\n",
      "epoch:7, iter:16,  [D loss: 0.618929, acc.: 65.00%] [G loss: 0.829052]\n",
      "epoch:7, iter:17,  [D loss: 0.592423, acc.: 75.00%] [G loss: 0.838485]\n",
      "epoch:7, iter:18,  [D loss: 0.593923, acc.: 72.00%] [G loss: 0.857956]\n",
      "epoch:7, iter:19,  [D loss: 0.598078, acc.: 72.00%] [G loss: 0.878055]\n",
      "epoch:7, iter:20,  [D loss: 0.591709, acc.: 69.00%] [G loss: 0.924570]\n",
      "epoch:7, iter:21,  [D loss: 0.605416, acc.: 72.00%] [G loss: 0.868729]\n",
      "epoch:7, iter:22,  [D loss: 0.624792, acc.: 63.00%] [G loss: 0.816895]\n",
      "epoch:7, iter:23,  [D loss: 0.593677, acc.: 70.00%] [G loss: 0.857581]\n",
      "epoch:7, iter:24,  [D loss: 0.623504, acc.: 67.00%] [G loss: 0.886253]\n",
      "epoch:7, iter:25,  [D loss: 0.615929, acc.: 70.00%] [G loss: 0.870620]\n",
      "epoch:7, iter:26,  [D loss: 0.614093, acc.: 71.00%] [G loss: 0.864316]\n",
      "epoch:7, iter:27,  [D loss: 0.639323, acc.: 55.00%] [G loss: 0.900341]\n",
      "epoch:7, iter:28,  [D loss: 0.594440, acc.: 80.00%] [G loss: 0.902740]\n",
      "epoch:7, iter:29,  [D loss: 0.594686, acc.: 77.00%] [G loss: 0.871601]\n",
      "epoch:7, iter:30,  [D loss: 0.617530, acc.: 66.00%] [G loss: 0.867757]\n",
      "epoch:7, iter:31,  [D loss: 0.614463, acc.: 69.00%] [G loss: 0.862122]\n",
      "epoch:7, iter:32,  [D loss: 0.585136, acc.: 80.00%] [G loss: 0.869799]\n",
      "epoch:7, iter:33,  [D loss: 0.572590, acc.: 84.00%] [G loss: 0.861415]\n",
      "epoch:7, iter:34,  [D loss: 0.631768, acc.: 74.00%] [G loss: 0.869064]\n",
      "epoch:7, iter:35,  [D loss: 0.589390, acc.: 79.00%] [G loss: 0.850365]\n",
      "epoch:7, iter:36,  [D loss: 0.612723, acc.: 69.00%] [G loss: 0.855582]\n",
      "epoch:7, iter:37,  [D loss: 0.638539, acc.: 60.00%] [G loss: 0.883404]\n",
      "epoch:7, iter:38,  [D loss: 0.616945, acc.: 66.00%] [G loss: 0.918140]\n",
      "epoch:7, iter:39,  [D loss: 0.628685, acc.: 65.00%] [G loss: 0.879523]\n",
      "epoch:7, iter:40,  [D loss: 0.655842, acc.: 59.00%] [G loss: 0.839194]\n",
      "epoch:7, iter:41,  [D loss: 0.645703, acc.: 57.00%] [G loss: 0.867913]\n",
      "epoch:7, iter:42,  [D loss: 0.627563, acc.: 62.00%] [G loss: 0.898775]\n",
      "epoch:7, iter:43,  [D loss: 0.586513, acc.: 77.00%] [G loss: 0.969954]\n",
      "epoch:7, iter:44,  [D loss: 0.645302, acc.: 66.00%] [G loss: 0.898732]\n",
      "epoch:7, iter:45,  [D loss: 0.657948, acc.: 61.00%] [G loss: 0.852012]\n",
      "epoch:7, iter:46,  [D loss: 0.631737, acc.: 65.00%] [G loss: 0.864972]\n",
      "epoch:7, iter:47,  [D loss: 0.634640, acc.: 67.00%] [G loss: 0.891253]\n",
      "epoch:7, iter:48,  [D loss: 0.649172, acc.: 66.00%] [G loss: 0.869174]\n",
      "epoch:7, iter:49,  [D loss: 0.673282, acc.: 56.00%] [G loss: 0.832545]\n",
      "epoch:7, iter:50,  [D loss: 0.631761, acc.: 65.00%] [G loss: 0.812774]\n",
      "epoch:7, iter:51,  [D loss: 0.652309, acc.: 57.00%] [G loss: 0.851633]\n",
      "epoch:7, iter:52,  [D loss: 0.625247, acc.: 64.00%] [G loss: 0.847015]\n",
      "epoch:7, iter:53,  [D loss: 0.628095, acc.: 65.00%] [G loss: 0.840884]\n",
      "epoch:7, iter:54,  [D loss: 0.637950, acc.: 60.00%] [G loss: 0.808388]\n",
      "epoch:7, iter:55,  [D loss: 0.619860, acc.: 66.00%] [G loss: 0.845794]\n",
      "epoch:7, iter:56,  [D loss: 0.614355, acc.: 67.00%] [G loss: 0.868037]\n",
      "epoch:7, iter:57,  [D loss: 0.647140, acc.: 53.00%] [G loss: 0.888183]\n",
      "epoch:7, iter:58,  [D loss: 0.625484, acc.: 63.00%] [G loss: 0.847741]\n",
      "epoch:7, iter:59,  [D loss: 0.646898, acc.: 60.00%] [G loss: 0.828827]\n",
      "epoch:7, iter:60,  [D loss: 0.661032, acc.: 59.00%] [G loss: 0.831323]\n",
      "epoch:7, iter:61,  [D loss: 0.635127, acc.: 67.00%] [G loss: 0.829310]\n",
      "epoch:7, iter:62,  [D loss: 0.686047, acc.: 53.00%] [G loss: 0.861193]\n",
      "epoch:7, iter:63,  [D loss: 0.636399, acc.: 68.00%] [G loss: 0.852480]\n",
      "epoch:7, iter:64,  [D loss: 0.611678, acc.: 69.00%] [G loss: 0.889133]\n",
      "epoch:7, iter:65,  [D loss: 0.641917, acc.: 62.00%] [G loss: 0.831108]\n",
      "epoch:7, iter:66,  [D loss: 0.611603, acc.: 59.00%] [G loss: 0.834086]\n",
      "epoch:7, iter:67,  [D loss: 0.610846, acc.: 65.00%] [G loss: 0.820845]\n",
      "epoch:7, iter:68,  [D loss: 0.674904, acc.: 56.00%] [G loss: 0.817594]\n",
      "epoch:7, iter:69,  [D loss: 0.627950, acc.: 73.00%] [G loss: 0.803282]\n",
      "epoch:7, iter:70,  [D loss: 0.621723, acc.: 65.00%] [G loss: 0.774706]\n",
      "epoch:7, iter:71,  [D loss: 0.635400, acc.: 59.00%] [G loss: 0.818998]\n",
      "epoch:7, iter:72,  [D loss: 0.645811, acc.: 63.00%] [G loss: 0.853600]\n",
      "epoch:7, iter:73,  [D loss: 0.636209, acc.: 61.00%] [G loss: 0.902699]\n",
      "epoch:7, iter:74,  [D loss: 0.644308, acc.: 66.00%] [G loss: 0.927565]\n",
      "epoch:7, iter:75,  [D loss: 0.659195, acc.: 58.00%] [G loss: 0.891584]\n",
      "epoch:7, iter:76,  [D loss: 0.681387, acc.: 49.00%] [G loss: 0.838938]\n",
      "epoch:7, iter:77,  [D loss: 0.639048, acc.: 57.00%] [G loss: 0.864368]\n",
      "epoch:7, iter:78,  [D loss: 0.631920, acc.: 68.00%] [G loss: 0.840649]\n",
      "epoch:7, iter:79,  [D loss: 0.631417, acc.: 62.00%] [G loss: 0.821463]\n",
      "epoch:7, iter:80,  [D loss: 0.646514, acc.: 60.00%] [G loss: 0.857124]\n",
      "epoch:7, iter:81,  [D loss: 0.628699, acc.: 68.00%] [G loss: 0.880380]\n",
      "epoch:7, iter:82,  [D loss: 0.659074, acc.: 60.00%] [G loss: 0.866710]\n",
      "epoch:7, iter:83,  [D loss: 0.668332, acc.: 55.00%] [G loss: 0.874925]\n",
      "epoch:7, iter:84,  [D loss: 0.635600, acc.: 62.00%] [G loss: 0.859508]\n",
      "epoch:7, iter:85,  [D loss: 0.683935, acc.: 56.00%] [G loss: 0.852534]\n",
      "epoch:7, iter:86,  [D loss: 0.629066, acc.: 65.00%] [G loss: 0.897535]\n",
      "epoch:7, iter:87,  [D loss: 0.655795, acc.: 63.00%] [G loss: 0.845257]\n",
      "epoch:7, iter:88,  [D loss: 0.666485, acc.: 53.00%] [G loss: 0.830675]\n",
      "epoch:7, iter:89,  [D loss: 0.631013, acc.: 63.00%] [G loss: 0.835989]\n",
      "epoch:7, iter:90,  [D loss: 0.632166, acc.: 65.00%] [G loss: 0.821530]\n",
      "epoch:7, iter:91,  [D loss: 0.657523, acc.: 58.00%] [G loss: 0.777398]\n",
      "epoch:7, iter:92,  [D loss: 0.636680, acc.: 60.00%] [G loss: 0.775883]\n",
      "epoch:7, iter:93,  [D loss: 0.640495, acc.: 62.00%] [G loss: 0.792349]\n",
      "epoch:7, iter:94,  [D loss: 0.620562, acc.: 65.00%] [G loss: 0.790530]\n",
      "epoch:7, iter:95,  [D loss: 0.660263, acc.: 63.00%] [G loss: 0.808231]\n",
      "epoch:7, iter:96,  [D loss: 0.627217, acc.: 60.00%] [G loss: 0.846145]\n",
      "epoch:7, iter:97,  [D loss: 0.649362, acc.: 58.00%] [G loss: 0.844468]\n",
      "epoch:7, iter:98,  [D loss: 0.653473, acc.: 55.00%] [G loss: 0.816819]\n",
      "epoch:7, iter:99,  [D loss: 0.651230, acc.: 53.00%] [G loss: 0.809637]\n",
      "epoch:8, iter:0,  [D loss: 0.619978, acc.: 71.00%] [G loss: 0.807488]\n",
      "epoch:8, iter:1,  [D loss: 0.648111, acc.: 66.00%] [G loss: 0.797989]\n",
      "epoch:8, iter:2,  [D loss: 0.667869, acc.: 63.00%] [G loss: 0.822745]\n",
      "epoch:8, iter:3,  [D loss: 0.620232, acc.: 71.00%] [G loss: 0.846900]\n",
      "epoch:8, iter:4,  [D loss: 0.635818, acc.: 72.00%] [G loss: 0.855556]\n",
      "epoch:8, iter:5,  [D loss: 0.635903, acc.: 62.00%] [G loss: 0.858608]\n",
      "epoch:8, iter:6,  [D loss: 0.619376, acc.: 65.00%] [G loss: 0.879305]\n",
      "epoch:8, iter:7,  [D loss: 0.630845, acc.: 68.00%] [G loss: 0.905636]\n",
      "epoch:8, iter:8,  [D loss: 0.636655, acc.: 66.00%] [G loss: 0.868472]\n",
      "epoch:8, iter:9,  [D loss: 0.649840, acc.: 64.00%] [G loss: 0.852169]\n",
      "epoch:8, iter:10,  [D loss: 0.647316, acc.: 63.00%] [G loss: 0.822576]\n",
      "epoch:8, iter:11,  [D loss: 0.636310, acc.: 67.00%] [G loss: 0.841327]\n",
      "epoch:8, iter:12,  [D loss: 0.597338, acc.: 69.00%] [G loss: 0.886571]\n",
      "epoch:8, iter:13,  [D loss: 0.586584, acc.: 77.00%] [G loss: 0.889521]\n",
      "epoch:8, iter:14,  [D loss: 0.631671, acc.: 70.00%] [G loss: 0.868612]\n",
      "epoch:8, iter:15,  [D loss: 0.601314, acc.: 71.00%] [G loss: 0.853495]\n",
      "epoch:8, iter:16,  [D loss: 0.639588, acc.: 60.00%] [G loss: 0.833314]\n",
      "epoch:8, iter:17,  [D loss: 0.616379, acc.: 66.00%] [G loss: 0.861420]\n",
      "epoch:8, iter:18,  [D loss: 0.605404, acc.: 70.00%] [G loss: 0.862859]\n",
      "epoch:8, iter:19,  [D loss: 0.615192, acc.: 74.00%] [G loss: 0.841676]\n",
      "epoch:8, iter:20,  [D loss: 0.601671, acc.: 63.00%] [G loss: 0.861067]\n",
      "epoch:8, iter:21,  [D loss: 0.592334, acc.: 72.00%] [G loss: 0.854033]\n",
      "epoch:8, iter:22,  [D loss: 0.618515, acc.: 67.00%] [G loss: 0.881343]\n",
      "epoch:8, iter:23,  [D loss: 0.607080, acc.: 74.00%] [G loss: 0.875120]\n",
      "epoch:8, iter:24,  [D loss: 0.629225, acc.: 59.00%] [G loss: 0.857936]\n",
      "epoch:8, iter:25,  [D loss: 0.592601, acc.: 65.00%] [G loss: 0.870599]\n",
      "epoch:8, iter:26,  [D loss: 0.617176, acc.: 65.00%] [G loss: 0.839181]\n",
      "epoch:8, iter:27,  [D loss: 0.592386, acc.: 67.00%] [G loss: 0.832006]\n",
      "epoch:8, iter:28,  [D loss: 0.619627, acc.: 67.00%] [G loss: 0.802378]\n",
      "epoch:8, iter:29,  [D loss: 0.652196, acc.: 67.00%] [G loss: 0.833843]\n",
      "epoch:8, iter:30,  [D loss: 0.625667, acc.: 71.00%] [G loss: 0.858157]\n",
      "epoch:8, iter:31,  [D loss: 0.586766, acc.: 72.00%] [G loss: 0.869991]\n",
      "epoch:8, iter:32,  [D loss: 0.655920, acc.: 57.00%] [G loss: 0.867523]\n",
      "epoch:8, iter:33,  [D loss: 0.627357, acc.: 69.00%] [G loss: 0.873258]\n",
      "epoch:8, iter:34,  [D loss: 0.645304, acc.: 60.00%] [G loss: 0.822911]\n",
      "epoch:8, iter:35,  [D loss: 0.618920, acc.: 61.00%] [G loss: 0.852357]\n",
      "epoch:8, iter:36,  [D loss: 0.607676, acc.: 74.00%] [G loss: 0.838899]\n",
      "epoch:8, iter:37,  [D loss: 0.606063, acc.: 70.00%] [G loss: 0.819654]\n",
      "epoch:8, iter:38,  [D loss: 0.639184, acc.: 59.00%] [G loss: 0.824698]\n",
      "epoch:8, iter:39,  [D loss: 0.652422, acc.: 56.00%] [G loss: 0.821879]\n",
      "epoch:8, iter:40,  [D loss: 0.666822, acc.: 50.00%] [G loss: 0.822262]\n",
      "epoch:8, iter:41,  [D loss: 0.655461, acc.: 54.00%] [G loss: 0.794134]\n",
      "epoch:8, iter:42,  [D loss: 0.647614, acc.: 55.00%] [G loss: 0.818793]\n",
      "epoch:8, iter:43,  [D loss: 0.632548, acc.: 62.00%] [G loss: 0.801041]\n",
      "epoch:8, iter:44,  [D loss: 0.622782, acc.: 60.00%] [G loss: 0.803107]\n",
      "epoch:8, iter:45,  [D loss: 0.630422, acc.: 64.00%] [G loss: 0.798896]\n",
      "epoch:8, iter:46,  [D loss: 0.622952, acc.: 71.00%] [G loss: 0.797990]\n",
      "epoch:8, iter:47,  [D loss: 0.631821, acc.: 67.00%] [G loss: 0.806342]\n",
      "epoch:8, iter:48,  [D loss: 0.640298, acc.: 63.00%] [G loss: 0.831551]\n",
      "epoch:8, iter:49,  [D loss: 0.612238, acc.: 69.00%] [G loss: 0.866161]\n",
      "epoch:8, iter:50,  [D loss: 0.640371, acc.: 64.00%] [G loss: 0.861419]\n",
      "epoch:8, iter:51,  [D loss: 0.613045, acc.: 69.00%] [G loss: 0.820289]\n",
      "epoch:8, iter:52,  [D loss: 0.616779, acc.: 68.00%] [G loss: 0.800436]\n",
      "epoch:8, iter:53,  [D loss: 0.647377, acc.: 62.00%] [G loss: 0.790531]\n",
      "epoch:8, iter:54,  [D loss: 0.670544, acc.: 55.00%] [G loss: 0.788074]\n",
      "epoch:8, iter:55,  [D loss: 0.614868, acc.: 61.00%] [G loss: 0.842486]\n",
      "epoch:8, iter:56,  [D loss: 0.645461, acc.: 63.00%] [G loss: 0.848545]\n",
      "epoch:8, iter:57,  [D loss: 0.619458, acc.: 67.00%] [G loss: 0.837975]\n",
      "epoch:8, iter:58,  [D loss: 0.604032, acc.: 73.00%] [G loss: 0.838669]\n",
      "epoch:8, iter:59,  [D loss: 0.605619, acc.: 69.00%] [G loss: 0.823886]\n",
      "epoch:8, iter:60,  [D loss: 0.609501, acc.: 68.00%] [G loss: 0.854085]\n",
      "epoch:8, iter:61,  [D loss: 0.632209, acc.: 64.00%] [G loss: 0.814610]\n",
      "epoch:8, iter:62,  [D loss: 0.642193, acc.: 64.00%] [G loss: 0.804432]\n",
      "epoch:8, iter:63,  [D loss: 0.597595, acc.: 70.00%] [G loss: 0.819909]\n",
      "epoch:8, iter:64,  [D loss: 0.619578, acc.: 65.00%] [G loss: 0.837893]\n",
      "epoch:8, iter:65,  [D loss: 0.631479, acc.: 60.00%] [G loss: 0.827480]\n",
      "epoch:8, iter:66,  [D loss: 0.613774, acc.: 69.00%] [G loss: 0.851225]\n",
      "epoch:8, iter:67,  [D loss: 0.620787, acc.: 63.00%] [G loss: 0.839505]\n",
      "epoch:8, iter:68,  [D loss: 0.625838, acc.: 66.00%] [G loss: 0.856614]\n",
      "epoch:8, iter:69,  [D loss: 0.612004, acc.: 74.00%] [G loss: 0.855303]\n",
      "epoch:8, iter:70,  [D loss: 0.589163, acc.: 77.00%] [G loss: 0.843282]\n",
      "epoch:8, iter:71,  [D loss: 0.651682, acc.: 58.00%] [G loss: 0.827733]\n",
      "epoch:8, iter:72,  [D loss: 0.624027, acc.: 64.00%] [G loss: 0.838900]\n",
      "epoch:8, iter:73,  [D loss: 0.632777, acc.: 62.00%] [G loss: 0.860577]\n",
      "epoch:8, iter:74,  [D loss: 0.620760, acc.: 64.00%] [G loss: 0.882526]\n",
      "epoch:8, iter:75,  [D loss: 0.611682, acc.: 72.00%] [G loss: 0.864896]\n",
      "epoch:8, iter:76,  [D loss: 0.669647, acc.: 61.00%] [G loss: 0.791999]\n",
      "epoch:8, iter:77,  [D loss: 0.638338, acc.: 61.00%] [G loss: 0.777654]\n",
      "epoch:8, iter:78,  [D loss: 0.649179, acc.: 64.00%] [G loss: 0.796431]\n",
      "epoch:8, iter:79,  [D loss: 0.666578, acc.: 65.00%] [G loss: 0.812650]\n",
      "epoch:8, iter:80,  [D loss: 0.685098, acc.: 57.00%] [G loss: 0.813289]\n",
      "epoch:8, iter:81,  [D loss: 0.652457, acc.: 65.00%] [G loss: 0.823192]\n",
      "epoch:8, iter:82,  [D loss: 0.670788, acc.: 60.00%] [G loss: 0.805516]\n",
      "epoch:8, iter:83,  [D loss: 0.641760, acc.: 60.00%] [G loss: 0.829317]\n",
      "epoch:8, iter:84,  [D loss: 0.647870, acc.: 59.00%] [G loss: 0.835730]\n",
      "epoch:8, iter:85,  [D loss: 0.694589, acc.: 54.00%] [G loss: 0.793527]\n",
      "epoch:8, iter:86,  [D loss: 0.651330, acc.: 69.00%] [G loss: 0.786871]\n",
      "epoch:8, iter:87,  [D loss: 0.651541, acc.: 73.00%] [G loss: 0.843940]\n",
      "epoch:8, iter:88,  [D loss: 0.667317, acc.: 60.00%] [G loss: 0.865016]\n",
      "epoch:8, iter:89,  [D loss: 0.662416, acc.: 63.00%] [G loss: 0.860211]\n",
      "epoch:8, iter:90,  [D loss: 0.638827, acc.: 66.00%] [G loss: 0.844870]\n",
      "epoch:8, iter:91,  [D loss: 0.645519, acc.: 67.00%] [G loss: 0.801089]\n",
      "epoch:8, iter:92,  [D loss: 0.632882, acc.: 69.00%] [G loss: 0.842231]\n",
      "epoch:8, iter:93,  [D loss: 0.637930, acc.: 61.00%] [G loss: 0.885957]\n",
      "epoch:8, iter:94,  [D loss: 0.618870, acc.: 72.00%] [G loss: 0.917541]\n",
      "epoch:8, iter:95,  [D loss: 0.646776, acc.: 68.00%] [G loss: 0.874198]\n",
      "epoch:8, iter:96,  [D loss: 0.632916, acc.: 66.00%] [G loss: 0.834549]\n",
      "epoch:8, iter:97,  [D loss: 0.626597, acc.: 61.00%] [G loss: 0.865937]\n",
      "epoch:8, iter:98,  [D loss: 0.600553, acc.: 75.00%] [G loss: 0.856821]\n",
      "epoch:8, iter:99,  [D loss: 0.624739, acc.: 64.00%] [G loss: 0.842486]\n",
      "epoch:9, iter:0,  [D loss: 0.624840, acc.: 67.00%] [G loss: 0.846903]\n",
      "epoch:9, iter:1,  [D loss: 0.612003, acc.: 69.00%] [G loss: 0.831848]\n",
      "epoch:9, iter:2,  [D loss: 0.620019, acc.: 66.00%] [G loss: 0.819456]\n",
      "epoch:9, iter:3,  [D loss: 0.622392, acc.: 70.00%] [G loss: 0.828025]\n",
      "epoch:9, iter:4,  [D loss: 0.613693, acc.: 64.00%] [G loss: 0.857305]\n",
      "epoch:9, iter:5,  [D loss: 0.607379, acc.: 73.00%] [G loss: 0.901499]\n",
      "epoch:9, iter:6,  [D loss: 0.594899, acc.: 77.00%] [G loss: 0.886298]\n",
      "epoch:9, iter:7,  [D loss: 0.606365, acc.: 71.00%] [G loss: 0.889571]\n",
      "epoch:9, iter:8,  [D loss: 0.595226, acc.: 73.00%] [G loss: 0.855282]\n",
      "epoch:9, iter:9,  [D loss: 0.614053, acc.: 65.00%] [G loss: 0.827780]\n",
      "epoch:9, iter:10,  [D loss: 0.626636, acc.: 63.00%] [G loss: 0.837869]\n",
      "epoch:9, iter:11,  [D loss: 0.618426, acc.: 69.00%] [G loss: 0.833960]\n",
      "epoch:9, iter:12,  [D loss: 0.619747, acc.: 69.00%] [G loss: 0.829678]\n",
      "epoch:9, iter:13,  [D loss: 0.636697, acc.: 64.00%] [G loss: 0.804799]\n",
      "epoch:9, iter:14,  [D loss: 0.595693, acc.: 69.00%] [G loss: 0.833151]\n",
      "epoch:9, iter:15,  [D loss: 0.603183, acc.: 68.00%] [G loss: 0.840426]\n",
      "epoch:9, iter:16,  [D loss: 0.620868, acc.: 65.00%] [G loss: 0.839217]\n",
      "epoch:9, iter:17,  [D loss: 0.616570, acc.: 69.00%] [G loss: 0.832488]\n",
      "epoch:9, iter:18,  [D loss: 0.601710, acc.: 69.00%] [G loss: 0.836899]\n",
      "epoch:9, iter:19,  [D loss: 0.595641, acc.: 72.00%] [G loss: 0.891918]\n",
      "epoch:9, iter:20,  [D loss: 0.629105, acc.: 69.00%] [G loss: 0.889935]\n",
      "epoch:9, iter:21,  [D loss: 0.607791, acc.: 73.00%] [G loss: 0.918035]\n",
      "epoch:9, iter:22,  [D loss: 0.630986, acc.: 65.00%] [G loss: 0.858761]\n",
      "epoch:9, iter:23,  [D loss: 0.632900, acc.: 60.00%] [G loss: 0.850985]\n",
      "epoch:9, iter:24,  [D loss: 0.651484, acc.: 59.00%] [G loss: 0.823665]\n",
      "epoch:9, iter:25,  [D loss: 0.650066, acc.: 60.00%] [G loss: 0.853114]\n",
      "epoch:9, iter:26,  [D loss: 0.616028, acc.: 65.00%] [G loss: 0.876507]\n",
      "epoch:9, iter:27,  [D loss: 0.661286, acc.: 58.00%] [G loss: 0.874729]\n",
      "epoch:9, iter:28,  [D loss: 0.626907, acc.: 67.00%] [G loss: 0.826756]\n",
      "epoch:9, iter:29,  [D loss: 0.654538, acc.: 55.00%] [G loss: 0.811502]\n",
      "epoch:9, iter:30,  [D loss: 0.639067, acc.: 62.00%] [G loss: 0.857843]\n",
      "epoch:9, iter:31,  [D loss: 0.653030, acc.: 61.00%] [G loss: 0.877906]\n",
      "epoch:9, iter:32,  [D loss: 0.629953, acc.: 69.00%] [G loss: 0.872985]\n",
      "epoch:9, iter:33,  [D loss: 0.648581, acc.: 60.00%] [G loss: 0.871608]\n",
      "epoch:9, iter:34,  [D loss: 0.682604, acc.: 56.00%] [G loss: 0.836941]\n",
      "epoch:9, iter:35,  [D loss: 0.654419, acc.: 61.00%] [G loss: 0.859324]\n",
      "epoch:9, iter:36,  [D loss: 0.650721, acc.: 61.00%] [G loss: 0.892807]\n",
      "epoch:9, iter:37,  [D loss: 0.704869, acc.: 50.00%] [G loss: 0.858008]\n",
      "epoch:9, iter:38,  [D loss: 0.624885, acc.: 71.00%] [G loss: 0.909336]\n",
      "epoch:9, iter:39,  [D loss: 0.662393, acc.: 57.00%] [G loss: 0.831485]\n",
      "epoch:9, iter:40,  [D loss: 0.664513, acc.: 53.00%] [G loss: 0.797732]\n",
      "epoch:9, iter:41,  [D loss: 0.659381, acc.: 59.00%] [G loss: 0.808467]\n",
      "epoch:9, iter:42,  [D loss: 0.675548, acc.: 47.00%] [G loss: 0.850057]\n",
      "epoch:9, iter:43,  [D loss: 0.637588, acc.: 63.00%] [G loss: 0.871125]\n",
      "epoch:9, iter:44,  [D loss: 0.657924, acc.: 62.00%] [G loss: 0.856066]\n",
      "epoch:9, iter:45,  [D loss: 0.660464, acc.: 57.00%] [G loss: 0.853148]\n",
      "epoch:9, iter:46,  [D loss: 0.633183, acc.: 68.00%] [G loss: 0.868465]\n",
      "epoch:9, iter:47,  [D loss: 0.619841, acc.: 71.00%] [G loss: 0.892033]\n",
      "epoch:9, iter:48,  [D loss: 0.632451, acc.: 66.00%] [G loss: 0.889065]\n",
      "epoch:9, iter:49,  [D loss: 0.617522, acc.: 70.00%] [G loss: 0.862321]\n",
      "epoch:9, iter:50,  [D loss: 0.631788, acc.: 66.00%] [G loss: 0.840289]\n",
      "epoch:9, iter:51,  [D loss: 0.663542, acc.: 55.00%] [G loss: 0.845897]\n",
      "epoch:9, iter:52,  [D loss: 0.613381, acc.: 74.00%] [G loss: 0.867615]\n",
      "epoch:9, iter:53,  [D loss: 0.639747, acc.: 73.00%] [G loss: 0.941440]\n",
      "epoch:9, iter:54,  [D loss: 0.597919, acc.: 78.00%] [G loss: 0.926213]\n",
      "epoch:9, iter:55,  [D loss: 0.611303, acc.: 72.00%] [G loss: 0.880310]\n",
      "epoch:9, iter:56,  [D loss: 0.624141, acc.: 68.00%] [G loss: 0.856916]\n",
      "epoch:9, iter:57,  [D loss: 0.622268, acc.: 75.00%] [G loss: 0.866110]\n",
      "epoch:9, iter:58,  [D loss: 0.606268, acc.: 73.00%] [G loss: 0.901086]\n",
      "epoch:9, iter:59,  [D loss: 0.582704, acc.: 80.00%] [G loss: 0.943277]\n",
      "epoch:9, iter:60,  [D loss: 0.595617, acc.: 80.00%] [G loss: 0.926331]\n",
      "epoch:9, iter:61,  [D loss: 0.589098, acc.: 80.00%] [G loss: 0.946851]\n",
      "epoch:9, iter:62,  [D loss: 0.636100, acc.: 66.00%] [G loss: 0.835891]\n",
      "epoch:9, iter:63,  [D loss: 0.630067, acc.: 75.00%] [G loss: 0.822387]\n",
      "epoch:9, iter:64,  [D loss: 0.637027, acc.: 65.00%] [G loss: 0.827958]\n",
      "epoch:9, iter:65,  [D loss: 0.608148, acc.: 70.00%] [G loss: 0.865401]\n",
      "epoch:9, iter:66,  [D loss: 0.608357, acc.: 68.00%] [G loss: 0.890337]\n",
      "epoch:9, iter:67,  [D loss: 0.628532, acc.: 67.00%] [G loss: 0.860930]\n",
      "epoch:9, iter:68,  [D loss: 0.635852, acc.: 64.00%] [G loss: 0.845853]\n",
      "epoch:9, iter:69,  [D loss: 0.636917, acc.: 57.00%] [G loss: 0.829856]\n",
      "epoch:9, iter:70,  [D loss: 0.624344, acc.: 68.00%] [G loss: 0.832111]\n",
      "epoch:9, iter:71,  [D loss: 0.605626, acc.: 67.00%] [G loss: 0.847235]\n",
      "epoch:9, iter:72,  [D loss: 0.621631, acc.: 65.00%] [G loss: 0.829301]\n",
      "epoch:9, iter:73,  [D loss: 0.623861, acc.: 67.00%] [G loss: 0.814943]\n",
      "epoch:9, iter:74,  [D loss: 0.642744, acc.: 57.00%] [G loss: 0.830279]\n",
      "epoch:9, iter:75,  [D loss: 0.621461, acc.: 63.00%] [G loss: 0.846069]\n",
      "epoch:9, iter:76,  [D loss: 0.639663, acc.: 64.00%] [G loss: 0.834361]\n",
      "epoch:9, iter:77,  [D loss: 0.627274, acc.: 68.00%] [G loss: 0.866603]\n",
      "epoch:9, iter:78,  [D loss: 0.640975, acc.: 65.00%] [G loss: 0.864473]\n",
      "epoch:9, iter:79,  [D loss: 0.604612, acc.: 72.00%] [G loss: 0.855653]\n",
      "epoch:9, iter:80,  [D loss: 0.601397, acc.: 73.00%] [G loss: 0.848243]\n",
      "epoch:9, iter:81,  [D loss: 0.614088, acc.: 68.00%] [G loss: 0.831143]\n",
      "epoch:9, iter:82,  [D loss: 0.627806, acc.: 64.00%] [G loss: 0.844007]\n",
      "epoch:9, iter:83,  [D loss: 0.632570, acc.: 67.00%] [G loss: 0.842259]\n",
      "epoch:9, iter:84,  [D loss: 0.626293, acc.: 62.00%] [G loss: 0.863597]\n",
      "epoch:9, iter:85,  [D loss: 0.576012, acc.: 80.00%] [G loss: 0.850729]\n",
      "epoch:9, iter:86,  [D loss: 0.600102, acc.: 70.00%] [G loss: 0.876009]\n",
      "epoch:9, iter:87,  [D loss: 0.615076, acc.: 76.00%] [G loss: 0.882670]\n",
      "epoch:9, iter:88,  [D loss: 0.592333, acc.: 78.00%] [G loss: 0.870517]\n",
      "epoch:9, iter:89,  [D loss: 0.599815, acc.: 71.00%] [G loss: 0.849106]\n",
      "epoch:9, iter:90,  [D loss: 0.597144, acc.: 70.00%] [G loss: 0.863515]\n",
      "epoch:9, iter:91,  [D loss: 0.608308, acc.: 67.00%] [G loss: 0.895848]\n",
      "epoch:9, iter:92,  [D loss: 0.585044, acc.: 77.00%] [G loss: 0.917920]\n",
      "epoch:9, iter:93,  [D loss: 0.599345, acc.: 73.00%] [G loss: 0.857787]\n",
      "epoch:9, iter:94,  [D loss: 0.627171, acc.: 65.00%] [G loss: 0.857470]\n",
      "epoch:9, iter:95,  [D loss: 0.554437, acc.: 82.00%] [G loss: 0.902051]\n",
      "epoch:9, iter:96,  [D loss: 0.581452, acc.: 73.00%] [G loss: 0.913760]\n",
      "epoch:9, iter:97,  [D loss: 0.615131, acc.: 63.00%] [G loss: 0.944257]\n",
      "epoch:9, iter:98,  [D loss: 0.608927, acc.: 70.00%] [G loss: 0.900795]\n",
      "epoch:9, iter:99,  [D loss: 0.587144, acc.: 65.00%] [G loss: 0.900504]\n",
      "経過時間：559.8688862323761\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t1 = time.time() \n",
    "train(epochs=10, batch_size=100, save_interval=1)　#　エポック数、バッチサイズ、画像保存頻度\n",
    "t2 = time.time()\n",
    "\n",
    "# 経過時間を表示\n",
    "elapsed_time = t2-t1\n",
    "print(f\"経過時間：{elapsed_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0kvRNEFWneVf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Original_GAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
